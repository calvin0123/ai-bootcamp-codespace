{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973cfaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "import search_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af423f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    chunk_size: int = 2000\n",
    "    chunk_step: int = 1000\n",
    "    top_k: int = 5\n",
    "\n",
    "    model: str = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0194c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "You are a search assistant for the Evidently documentation.\n",
    "\n",
    "Evidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\n",
    "It provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n",
    "\n",
    "Your task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "- search — Use this to explore the topic and retrieve relevant snippets or documentation.\n",
    "- read_file — Use this to retrieve or verify the complete content of a file when:\n",
    "    * A code snippet is incomplete, truncated, or missing definitions.\n",
    "    * You need to check that all variables, imports, and functions referenced in code are defined.\n",
    "    * You must ensure the code example is syntactically correct and runnable.\n",
    "\n",
    "If `read_file` cannot be used or the file content is unavailable, clearly state:\n",
    "> \"Unable to verify with read_file.\"\n",
    "\n",
    "Search Strategy\n",
    "\n",
    "- For every user query:\n",
    "    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n",
    "    * Each search must use a different phrasing or keyword variation of the user's question.\n",
    "    * Make sure that the search requests are relevant to evidently, testing, evaluating and monitoring AI systems.\n",
    "    * No need to include \"Evidently\" in the search text.\n",
    "\n",
    "- After collecting search results:\n",
    "    1. Synthesize the information into a concise, accurate answer.\n",
    "    2. If your answer includes code, always validate it with `read_file` before finalizing.\n",
    "    3. If a code snippet or reference is incomplete, explicitly mention it.\n",
    "\n",
    "Important:\n",
    "- The 6-search limit applies only to `search` calls.\n",
    "- You may call `read_file` at any time, even after the search limit is reached.\n",
    "- `read_file` calls are verification steps and do not count toward the 6-search limit.\n",
    "\n",
    "Code Verification and Completeness Rules\n",
    "\n",
    "- All variables, functions, and imports in your final code examples must be defined or imported.\n",
    "- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n",
    "- When something is missing or undefined in the search results:\n",
    "    * Call `read_file` with the likely filename to retrieve the complete file content.\n",
    "    * Replace any partial code with the full verified version.\n",
    "- If the file is not available or cannot be verified:\n",
    "    * Include a clear note: \"Unable to verify this code.\"\n",
    "- Do not reformat, rename variables, or omit lines from the verified code.\n",
    "\n",
    "Output Format\n",
    "\n",
    "- Write your answer clearly and accurately.\n",
    "- Include a \"References\" section listing the search queries or file names you used.\n",
    "- If you couldn't find a complete answer after 6 searches, set found_answer = False.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df8705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reference(BaseModel):\n",
    "    title: str\n",
    "    filename: str\n",
    "\n",
    "class Section(BaseModel):\n",
    "    heading: str\n",
    "    content: str\n",
    "    references: list[Reference]\n",
    "\n",
    "class SearchResultArticle(BaseModel):\n",
    "    found_answer: bool\n",
    "    title: str\n",
    "    sections: list[Section]\n",
    "    references: list[Reference]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef9a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AgentConfig()\n",
    "\n",
    "tools = search_tools.prepare_search_tools(\n",
    "    config.chunk_size,\n",
    "    config.chunk_step,\n",
    "    config.top_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee43e9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a52908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5532c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool\n",
    "\n",
    "agent_tools = [\n",
    "    function_tool(tools.search),\n",
    "    function_tool(tools.read_file)\n",
    "]\n",
    "\n",
    "search_agent = Agent(\n",
    "    name='search',\n",
    "    tools=agent_tools,\n",
    "    instructions=search_instructions,\n",
    "    model=config.model,\n",
    "    output_type=SearchResultArticle,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c7f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "\n",
    "input = 'llm as a judge'\n",
    "result = await Runner.run(search_agent, input=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a8a5f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\":\"using AI or LLMs as judges in legal systems\"})\n",
      "TOOL CALL (search): search({\"query\":\"AI judges ethical implications\"})\n",
      "TOOL CALL (search): search({\"query\":\"legal decisions by artificial intelligence\"})\n",
      "TOOL CALL (search): search({\"query\":\"AI in the courtroom\"})\n",
      "TOOL CALL (search): search({\"query\":\"effectiveness of AI in judicial roles\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"examples/LLM_judge.mdx\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"metrics/customize_llm_judge.mdx\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"quickstart_llm.mdx\"})\n",
      "{\"found_answer\":true,\"title\":\"Using LLMs as Judges\",\"sections\":[{\"heading\":\"Overview\",\"content\":\"Using Language Models (LLMs) as judges in various evaluations, such as for legal contexts or compliance checks, can help in assessing and interpreting complex data. LLMs can be employed to evaluate outputs based on predefined criteria, comparing new responses against reference inputs (reference-based evaluation) or assessing them based on custom standards (open-ended evaluation).\",\"references\":[]},{\"heading\":\"Implementation Steps\",\"content\":\"1. **Set up Environment**: Ensure you have Python installed and the Evidently library:\\n   ```python\\n   pip install evidently\\n   ```\\n\\n2. **Import Required Libraries**:\\n   ```python\\n   import pandas as pd\\n   from evidently import Dataset, DataDefinition, Report\\n   from evidently.llm.templates import BinaryClassificationPromptTemplate\\n   from evidently.descriptors import LLMEval\\n   import os\\n   os.environ[\\\"OPENAI_API_KEY\\\"] = \\\"YOUR_API_KEY\\\"\\n   ```\\n\\n3. **Create Dataset**:\\n   Prepare a dataset with the questions and corresponding target and new answers.\\n   ```python\\n   data = [\\n       [\\\"Q1?\\\", \\\"Target answer 1\\\", \\\"New answer 1\\\", \\\"correct\\\"],\\n       [\\\"Q2?\\\", \\\"Target answer 2\\\", \\\"New answer 2\\\", \\\"incorrect\\\"],\\n       ...\\n   ]\\n   df = pd.DataFrame(data, columns=[\\\"question\\\", \\\"target_response\\\", \\\"new_response\\\", \\\"label\\\"])\\n   eval_dataset = Dataset.from_pandas(df, data_definition=DataDefinition())\\n   ```\\n\\n4. **Define Evaluation Criteria**:\\n   Set up the criteria using the Binary Classification Prompt Template:\\n   ```python\\n   correctness = BinaryClassificationPromptTemplate(\\n       criteria=\\\"\\\"\\\"An ANSWER is correct when it matches the REFERENCE in facts and details.\\\"\\\"\\\",\\n       target_category=\\\"correct\\\",\\n       non_target_category=\\\"incorrect\\\",\\n       include_reasoning=True,\\n   )\\n   ```\\n\\n5. **Run Evaluation**:\\n   Integrate the LLM evaluation descriptor to assess the new responses:\\n   ```python\\n   eval_dataset.add_descriptors([\\n       LLMEval(\\\"new_response\\\", template=correctness, provider=\\\"openai\\\", model=\\\"gpt-4o-mini\\\")\\n   ])\\n   ```\\n\\n6. **Generate Report**:\\n   Generate and visualize the report:\\n   ```python\\n   report = Report([\\n       TextEvals()\\n   ])\\n   my_eval = report.run(eval_dataset)\\n   my_eval\\n   ```\\n   \\n   With this approach, you can effectively leverage LLMs to serve as judges in various tasks.\",\"references\":[{\"title\":\"LLM as a Judge Tutorial\",\"filename\":\"examples/LLM_judge.mdx\"},{\"title\":\"LLM Judge Documentation\",\"filename\":\"metrics/customize_llm_judge.mdx\"},{\"title\":\"Quickstart for LLM Evaluations\",\"filename\":\"quickstart_llm.mdx\"}]}],\"references\":[{\"title\":\"LLM as a Judge Tutorial\",\"filename\":\"examples/LLM_judge.mdx\"},{\"title\":\"LLM Judge Documentation\",\"filename\":\"metrics/customize_llm_judge.mdx\"},{\"title\":\"Quickstart for LLM Evaluations\",\"filename\":\"quickstart_llm.mdx\"}]}"
     ]
    }
   ],
   "source": [
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "\n",
    "result = Runner.run_streamed(\n",
    "    search_agent,\n",
    "    input=input,\n",
    ")\n",
    "\n",
    "async for event in result.stream_events():\n",
    "    if event.type == \"run_item_stream_event\":\n",
    "        if event.item.type == \"tool_call_item\":\n",
    "            tool_call = event.item.raw_item\n",
    "            f_name = tool_call.name\n",
    "            args = tool_call.arguments\n",
    "            print(f\"TOOL CALL ({event.item.agent.name}): {f_name}({args})\")\n",
    "    \n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3c0c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.exceptions import MaxTurnsExceeded\n",
    "\n",
    "class SearchResultHandler(JSONParserHandler):\n",
    "    def on_field_start(self, path: str, field_name: str):\n",
    "        if field_name == \"references\":\n",
    "            level = path.count(\"/\") + 2\n",
    "            print(f\"\\n{'#' * level} References\\n\")\n",
    "\n",
    "    def on_field_end(self, path, field_name, value, parsed_value=None):\n",
    "        if field_name == \"title\" and path == \"\":\n",
    "            print(f\"# {value}\")\n",
    "\n",
    "        elif field_name == \"heading\":\n",
    "            print(f\"\\n\\n## {value}\\n\")\n",
    "        elif field_name == \"content\":\n",
    "            print(\"\\n\") \n",
    "\n",
    "    def on_value_chunk(self, path, field_name, chunk):\n",
    "        if field_name == \"content\":\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    def on_array_item_end(self, path, field_name, item=None):\n",
    "        if field_name == \"references\":\n",
    "            title = item.get(\"title\", \"\")\n",
    "            filename = item.get(\"filename\", \"\")\n",
    "            print(f\"- [{title}]({filename})\")\n",
    "\n",
    "handler = SearchResultHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "974466da",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_stream(agent, input, handler, max_turns=3):\n",
    "    try:\n",
    "        result = Runner.run_streamed(\n",
    "            agent,\n",
    "            input=input,\n",
    "            max_turns=max_turns\n",
    "        )\n",
    "        \n",
    "        parser = StreamingJSONParser(handler)\n",
    "\n",
    "        async for event in result.stream_events():\n",
    "            if event.type == \"run_item_stream_event\":\n",
    "                if event.item.type == \"tool_call_item\":\n",
    "                    tool_call = event.item.raw_item\n",
    "                    f_name = tool_call.name\n",
    "                    args = tool_call.arguments\n",
    "                    print(f\"TOOL CALL ({event.item.agent.name}): {f_name}({args})\")\n",
    "            \n",
    "            if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "                parser.parse_incremental(event.data.delta)\n",
    "\n",
    "        return result\n",
    "    except MaxTurnsExceeded as e:\n",
    "        print('too many turns')\n",
    "        finish_prompt = 'System message: The number of searches has exceeded the limit. Proceed to finishing the writeup'\n",
    "        finish_message = [{'role': 'user', 'content': finish_prompt}]\n",
    "        messages = result.to_input_list() + finish_message\n",
    "        final_result = await run_stream(agent, input=messages, handler=handler, max_turns=1)\n",
    "        return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14785b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\":\"using LLM as judge in evaluations\"})\n",
      "TOOL CALL (search): search({\"query\":\"LLM for decision-making systems\"})\n",
      "TOOL CALL (search): search({\"query\":\"role of LLM in bias evaluation\"})\n",
      "TOOL CALL (search): search({\"query\":\"evaluating AI decisions using LLMs\"})\n",
      "TOOL CALL (search): search({\"query\":\"how LLMs can assist in legal judgments\"})\n",
      "TOOL CALL (search): search({\"query\":\"LLMs in compliance and ethical decision-making\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"examples/LLM_judge.mdx\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"examples/LLM_jury.mdx\"})\n",
      "too many turns\n",
      "# Using LLMs as Judges in Evaluations\n",
      "\n",
      "\n",
      "## Overview of LLMs as Judges\n",
      "\n",
      "Large Language Models (LLMs) can serve as evaluators in various contexts, helping to assess responses or outputs based on defined criteria. Their use can be classified into two primary types:\n",
      "\n",
      "1. **Reference-based Evaluation**: Newly generated responses are compared against a reference set of responses (the \"ground truth\"). This approach is beneficial for regression testing where prior accepted outputs exist.\n",
      "\n",
      "2. **Open-ended Evaluation**: LLMs evaluate responses based on custom criteria, which helps in situations where a direct reference isn't available. \n",
      "\n",
      "The advantage of using LLMs is their ability to quickly process large datasets, provide nuanced analysis, and offer consistency in evaluation, which is especially valuable in automated systems.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "- [LLM-as-a-jury](examples/LLM_jury.mdx)\n",
      "\n",
      "\n",
      "## Setting Up LLM Evaluators\n",
      "\n",
      "To set up an LLM as a judge, follow these steps using the Evidently framework:\n",
      "\n",
      "1. **Install Required Libraries**: Make sure that the Evidently Python library is installed:\n",
      "   ```bash\n",
      "   pip install evidently\n",
      "   ```\n",
      "\n",
      "2. **Import Necessary Modules**: You will commonly need: \n",
      "   ```python\n",
      "   import pandas as pd\n",
      "   from evidently import Dataset\n",
      "   from evidently import DataDefinition\n",
      "   from evidently import Report\n",
      "   from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "   ```\n",
      "\n",
      "3. **Define Evaluation Criteria**: Use `BinaryClassificationPromptTemplate` to set the criteria under which the LLM will evaluate responses. For example, you can define a prompt that instructs the model to judge if a response is appropriate based on established communication standards.\n",
      "\n",
      "4. **Create and Score the Evaluator**: Note the scoring process involves using LLMs from different providers (OpenAI, Anthropic) by integrating their API keys, and setting the conditions for pass/fail judgments based on your template prompts.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "- [LLM-as-a-jury](examples/LLM_jury.mdx)\n",
      "\n",
      "\n",
      "## Evaluator Quality Assessment\n",
      "\n",
      "Once you’ve set up your LLM judges, it’s crucial to evaluate their performance:\n",
      "\n",
      "- **Accuracy Check**: Compare the LLM-generated judgments with manual labels to determine accuracy.\n",
      "- **Confusion Matrix Analysis**: Use confusion matrices to visualize where the LLMs are making mistakes and understand patterns in the data.\n",
      "- **Feedback Loop**: Continuously refine the evaluation prompts based on feedback and the discrepancies detected during assessments.\n",
      "\n",
      "This ensures that the LLMs are not just outputting results, but are aligned with desired evaluation metrics that can evolve based on usage.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "- [LLM-as-a-jury](examples/LLM_jury.mdx)\n",
      "\n",
      "\n",
      "## Practical Applications\n",
      "\n",
      "LLMs as judges can be utilized in various domains:\n",
      "\n",
      "- **Customer Support Evaluations**: Assess the effectiveness of automated responses in customer service scenarios.\n",
      "- **Content Appropriateness**: Ensure that generated emails or texts maintain the desired tone and style appropriate for professional communication.\n",
      "- **Model Evaluation**: Use LLM evaluations to compare different models in terms of output quality in natural language generation tasks.\n",
      "\n",
      "Moreover, implementing multiple LLMs as judges can provide a more rounded perspective by aggregating their assessments (similar to a jury), helping to identify inconsistencies and reinforce reliable output.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "- [LLM-as-a-jury](examples/LLM_jury.mdx)\n",
      "\n",
      "## References\n",
      "\n",
      "- [LLM as a judge tutorial](examples/LLM_judge.mdx)\n",
      "- [LLM-as-a-jury example](examples/LLM_jury.mdx)\n"
     ]
    }
   ],
   "source": [
    "result = await run_stream(search_agent, 'llm as a judge', SearchResultHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b54d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a guradrail agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d5c744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_instructions = \"\"\"\n",
    "Make sure the user queries are related to the Evidently framework and its documentation.\n",
    "\n",
    "Evidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data,\n",
    "AI and LLM systems. It provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n",
    "\n",
    "Examples of relevant topics:\n",
    "\n",
    "- Create a custom LLM judge\n",
    "- Customize data drift detection\n",
    "- llm evaluations\n",
    "\n",
    "Output 'fail=True' if the query is not about Evidently documentation or related topics.\n",
    "Keep reasoning short (up to 10 words)\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba5f77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidentlyDocsGuardrail(BaseModel):\n",
    "    reasoning: str\n",
    "    fail: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cad16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_agent = Agent( \n",
    "    name=\"guardrail\",\n",
    "    instructions=guardrail_instructions,\n",
    "    output_type=EvidentlyDocsGuardrail,\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d972218",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(guardrail_agent, input='llm as a judge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62885b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvidentlyDocsGuardrail(reasoning='Relevant to LLM evaluations in Evidently.', fail=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a29f9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(guardrail_agent, 'whats sqrt(pi)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fba697d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvidentlyDocsGuardrail(reasoning='Not related to Evidently documentation.', fail=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37ccb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Input Guardrail Function\n",
    "\n",
    "from agents import GuardrailFunctionOutput, input_guardrail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@input_guardrail\n",
    "async def guardrail(ctx, agent, messages):\n",
    "    result = await Runner.run(guardrail_agent, input=messages)\n",
    "    decision = result.final_output\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=decision.reasoning,\n",
    "        tripwire_triggered=decision.fail\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f7dd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "@input_guardrail\n",
    "async def guardrail(ctx, agent, messages):\n",
    "    if type(messages) == list and len(messages) > 1:\n",
    "        return GuardrailFunctionOutput(\n",
    "            output_info='no need to trigger for continued conversations',\n",
    "            tripwire_triggered=False\n",
    "        )\n",
    "    result = await Runner.run(guardrail_agent, input=messages)\n",
    "    decision = result.final_output\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=decision.reasoning,\n",
    "        tripwire_triggered=decision.fail\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca14f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have input guardrail and output guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41e34fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@input_guardrail\n",
    "async def documentation_guardrail(ctx, agent, input):\n",
    "    result = await Runner.run(guardrail_agent, input)\n",
    "    final_output = result.final_output\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=final_output.reasoning, \n",
    "        tripwire_triggered=final_output.fail,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e2dffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent = Agent(\n",
    "    name='search',\n",
    "    instructions=search_instructions,\n",
    "    tools=agent_tools,\n",
    "    input_guardrails=[documentation_guardrail],\n",
    "    model=config.model,\n",
    "    output_type=SearchResultArticle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "535aa3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not related to Evidently documentation.\n"
     ]
    }
   ],
   "source": [
    "from agents.exceptions import InputGuardrailTripwireTriggered\n",
    "\n",
    "try:\n",
    "    result = await Runner.run(search_agent, 'whats sqrt(pi)')\n",
    "except InputGuardrailTripwireTriggered as e:\n",
    "    output = e.guardrail_result.output\n",
    "    if output.tripwire_triggered: \n",
    "        print(output.output_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ccb2c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_stream(agent, input, handler, max_turns=3):\n",
    "    try:\n",
    "        result = Runner.run_streamed(\n",
    "            agent,\n",
    "            input=input,\n",
    "            max_turns=max_turns\n",
    "        )\n",
    "        \n",
    "        parser = StreamingJSONParser(handler)\n",
    "\n",
    "        async for event in result.stream_events():\n",
    "            if event.type == \"run_item_stream_event\":\n",
    "                if event.item.type == \"tool_call_item\":\n",
    "                    tool_call = event.item.raw_item\n",
    "                    f_name = tool_call.name\n",
    "                    args = tool_call.arguments\n",
    "                    print(f\"TOOL CALL ({event.item.agent.name}): {f_name}({args})\")\n",
    "            \n",
    "            if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "                parser.parse_incremental(event.data.delta)\n",
    "\n",
    "        return result\n",
    "    except MaxTurnsExceeded as e:\n",
    "        print('too many turns')\n",
    "        finish_prompt = 'System message: The number of searches has exceeded the limit. Proceed to finishing the writeup'\n",
    "        finish_message = [{'role': 'user', 'content': finish_prompt}]\n",
    "        messages = result.to_input_list() + finish_message\n",
    "        final_result = await run_stream(agent, input=messages, handler=handler, max_turns=1)\n",
    "        return final_result\n",
    "    except InputGuardrailTripwireTriggered as e:\n",
    "        run_data = e.run_data\n",
    "        for input_guardrail in run_data.input_guardrail_results:\n",
    "            o = input_guardrail.output\n",
    "            if o.tripwire_triggered:\n",
    "                print(o.output_info)\n",
    "        return e.run_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d39a627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\":\"sqrt(pi)\"})\n",
      "Query not related to Evidently documentation.\n"
     ]
    }
   ],
   "source": [
    "result = await run_stream(search_agent, 'how much is sqrt(pi)', SearchResultHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd589724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp-codespace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
