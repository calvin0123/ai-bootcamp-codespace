{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96682f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import agents\n",
    "from dotenv import load_dotenv\n",
    "# from openai import OpenAI\n",
    "# openai_client = OpenAI()\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c18c003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f800cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "# Create the function\n",
    "def fetch_url(url):\n",
    "    reader_url_prefix = 'https://r.jina.ai/'\n",
    "    request_url = reader_url_prefix + url\n",
    "    response = requests.get(request_url)\n",
    "    return response.content.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02d656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_url('https://datatalks.club')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338bb997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# Make the function with docstring, and type -> Easily parsed by the agent \n",
    "def fetch_url(url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch the textual content of a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): The target URL to fetch content from.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The decoded HTML/text content of the fetched page if successful,\n",
    "        or None if an error occurred.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the provided URL is empty or invalid.\n",
    "    \"\"\"\n",
    "    if not url or not isinstance(url, str):\n",
    "        raise ValueError(\"The 'url' parameter must be a non-empty string.\")\n",
    "\n",
    "    jina_reader_base_url = \"https://r.jina.ai/\"\n",
    "    jina_reader_url = jina_reader_base_url + url.lstrip(\"/\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(jina_reader_url, timeout=10)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad status codes\n",
    "        return response.content.decode(\"utf-8\")\n",
    "    except RequestException as e:\n",
    "        # Catch all network-related errors (e.g., ConnectionError, Timeout, HTTPError)\n",
    "        print(f\"Error fetching URL '{jina_reader_url}': {e}\")\n",
    "        return None\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error decoding response from '{jina_reader_url}'.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc902d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3add24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the agent \n",
    "# get the runner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d56d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the agent with the tool, system instruction and models\n",
    "assitant_instructions = \"\"\"\n",
    "You're a helpful assistant that helps answer user questions.\n",
    "\"\"\"\n",
    "\n",
    "web_agent = Agent(\n",
    "    name='web_agent',\n",
    "    tools=[function_tool(fetch_url)],\n",
    "    instructions=assitant_instructions,\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c9ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20bc32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "\n",
    "# Start running ..     with user prompt\n",
    "runner = Runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5534b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Summarize the content of https://openai.github.io/openai-agents-python/\"\n",
    "\n",
    "results = await runner.run(web_agent, input=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e952b95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunResult(input='Summarize the content of https://openai.github.io/openai-agents-python/', new_items=[ToolCallItem(agent=Agent(name='web_agent', handoff_description=None, tools=[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)], mcp_servers=[], mcp_config={}, instructions=\"\\nYou're a helpful assistant that helps answer user questions.\\n\", prompt=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{\"url\":\"https://openai.github.io/openai-agents-python/\"}', call_id='call_omvYjRh7FkwVE54GzArC0kt9', name='fetch_url', type='function_call', id='fc_0054eb01405294e40068ef25c4d08c8196bda5641a7831e31d', status='completed'), type='tool_call_item'), ToolCallOutputItem(agent=Agent(name='web_agent', handoff_description=None, tools=[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)], mcp_servers=[], mcp_config={}, instructions=\"\\nYou're a helpful assistant that helps answer user questions.\\n\", prompt=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': 'call_omvYjRh7FkwVE54GzArC0kt9', 'output': 'Title: OpenAI Agents SDK\\n\\nURL Source: https://openai.github.io/openai-agents-python/\\n\\nPublished Time: Wed, 15 Oct 2025 00:12:28 GMT\\n\\nMarkdown Content:\\nOpenAI Agents SDK\\n\\n===============\\n- [x] - [x] \\n\\n[Skip to content](https://openai.github.io/openai-agents-python/#openai-agents-sdk)\\n\\n[![Image 1: logo](https://openai.github.io/openai-agents-python/assets/logo.svg)](https://openai.github.io/openai-agents-python/ \"OpenAI Agents SDK\")\\n\\n OpenAI Agents SDK \\n\\n Intro \\n\\n*   [English](https://openai.github.io/openai-agents-python/)\\n*   [日本語](https://openai.github.io/openai-agents-python/ja/)\\n*   [한국어](https://openai.github.io/openai-agents-python/ko/)\\n*   [简体中文](https://openai.github.io/openai-agents-python/zh/)\\n\\nType to start searching\\n\\n[openai-agents-python * v0.3.3 * 16.5k * 2.7k](https://github.com/openai/openai-agents-python \"Go to repository\")\\n\\n[![Image 2: logo](https://openai.github.io/openai-agents-python/assets/logo.svg)](https://openai.github.io/openai-agents-python/ \"OpenAI Agents SDK\") OpenAI Agents SDK  \\n\\n[openai-agents-python * v0.3.3 * 16.5k * 2.7k](https://github.com/openai/openai-agents-python \"Go to repository\")\\n\\n*   - [x]  Intro  [Intro](https://openai.github.io/openai-agents-python/) Table of contents  \\n    *   [Why use the Agents SDK](https://openai.github.io/openai-agents-python/#why-use-the-agents-sdk)\\n    *   [Installation](https://openai.github.io/openai-agents-python/#installation)\\n    *   [Hello world example](https://openai.github.io/openai-agents-python/#hello-world-example)\\n\\n*   [Quickstart](https://openai.github.io/openai-agents-python/quickstart/)\\n*   [Examples](https://openai.github.io/openai-agents-python/examples/)\\n*   - [x]  Documentation   Documentation  \\n    *   [Agents](https://openai.github.io/openai-agents-python/agents/)\\n    *   [Running agents](https://openai.github.io/openai-agents-python/running_agents/)\\n    *   [Sessions](https://openai.github.io/openai-agents-python/sessions/)\\n    *   [Results](https://openai.github.io/openai-agents-python/results/)\\n    *   [Streaming](https://openai.github.io/openai-agents-python/streaming/)\\n    *   [REPL utility](https://openai.github.io/openai-agents-python/repl/)\\n    *   [Tools](https://openai.github.io/openai-agents-python/tools/)\\n    *   [Model context protocol (MCP)](https://openai.github.io/openai-agents-python/mcp/)\\n    *   [Handoffs](https://openai.github.io/openai-agents-python/handoffs/)\\n    *   [Tracing](https://openai.github.io/openai-agents-python/tracing/)\\n    *   [Context management](https://openai.github.io/openai-agents-python/context/)\\n    *   [Guardrails](https://openai.github.io/openai-agents-python/guardrails/)\\n    *   [Orchestrating multiple agents](https://openai.github.io/openai-agents-python/multi_agent/)\\n    *   [Usage](https://openai.github.io/openai-agents-python/usage/)\\n    *   - [x]  Models   Models  \\n        *   [Models](https://openai.github.io/openai-agents-python/models/)\\n        *   [Using any model via LiteLLM](https://openai.github.io/openai-agents-python/models/litellm/)\\n\\n    *   [Configuring the SDK](https://openai.github.io/openai-agents-python/config/)\\n    *   [Agent Visualization](https://openai.github.io/openai-agents-python/visualization/)\\n    *   [Release process/changelog](https://openai.github.io/openai-agents-python/release/)\\n    *   - [x]  Voice agents   Voice agents  \\n        *   [Quickstart](https://openai.github.io/openai-agents-python/voice/quickstart/)\\n        *   [Pipelines and workflows](https://openai.github.io/openai-agents-python/voice/pipeline/)\\n        *   [Tracing](https://openai.github.io/openai-agents-python/voice/tracing/)\\n\\n    *   - [x]  Realtime agents   Realtime agents  \\n        *   [Quickstart](https://openai.github.io/openai-agents-python/realtime/quickstart/)\\n        *   [Guide](https://openai.github.io/openai-agents-python/realtime/guide/)\\n\\n*   - [x]  API Reference   API Reference  \\n    *   - [x]  Agents   Agents  \\n        *   [Agents module](https://openai.github.io/openai-agents-python/ref/)\\n        *   [Agents](https://openai.github.io/openai-agents-python/ref/agent/)\\n        *   [Runner](https://openai.github.io/openai-agents-python/ref/run/)\\n        *   [Memory](https://openai.github.io/openai-agents-python/ref/memory/)\\n        *   [repl](https://openai.github.io/openai-agents-python/ref/repl/)\\n        *   [Tools](https://openai.github.io/openai-agents-python/ref/tool/)\\n        *   [Tool Context](https://openai.github.io/openai-agents-python/ref/tool_context/)\\n        *   [Results](https://openai.github.io/openai-agents-python/ref/result/)\\n        *   [Streaming events](https://openai.github.io/openai-agents-python/ref/stream_events/)\\n        *   [Handoffs](https://openai.github.io/openai-agents-python/ref/handoffs/)\\n        *   [Lifecycle](https://openai.github.io/openai-agents-python/ref/lifecycle/)\\n        *   [Items](https://openai.github.io/openai-agents-python/ref/items/)\\n        *   [Run context](https://openai.github.io/openai-agents-python/ref/run_context/)\\n        *   [Tool Context](https://openai.github.io/openai-agents-python/ref/tool_context/)\\n        *   [Usage](https://openai.github.io/openai-agents-python/ref/usage/)\\n        *   [Exceptions](https://openai.github.io/openai-agents-python/ref/exceptions/)\\n        *   [Guardrails](https://openai.github.io/openai-agents-python/ref/guardrail/)\\n        *   [Model settings](https://openai.github.io/openai-agents-python/ref/model_settings/)\\n        *   [Agent output](https://openai.github.io/openai-agents-python/ref/agent_output/)\\n        *   [Function schema](https://openai.github.io/openai-agents-python/ref/function_schema/)\\n        *   [Model interface](https://openai.github.io/openai-agents-python/ref/models/interface/)\\n        *   [OpenAI Chat Completions model](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/)\\n        *   [OpenAI Responses model](https://openai.github.io/openai-agents-python/ref/models/openai_responses/)\\n        *   [MCP Servers](https://openai.github.io/openai-agents-python/ref/mcp/server/)\\n        *   [MCP Util](https://openai.github.io/openai-agents-python/ref/mcp/util/)\\n\\n    *   - [x]  Tracing   Tracing  \\n        *   [Tracing module](https://openai.github.io/openai-agents-python/ref/tracing/)\\n        *   [Creating traces/spans](https://openai.github.io/openai-agents-python/ref/tracing/create/)\\n        *   [Traces](https://openai.github.io/openai-agents-python/ref/tracing/traces/)\\n        *   [Spans](https://openai.github.io/openai-agents-python/ref/tracing/spans/)\\n        *   [Processor interface](https://openai.github.io/openai-agents-python/ref/tracing/processor_interface/)\\n        *   [Processors](https://openai.github.io/openai-agents-python/ref/tracing/processors/)\\n        *   [Scope](https://openai.github.io/openai-agents-python/ref/tracing/scope/)\\n        *   [Setup](https://openai.github.io/openai-agents-python/ref/tracing/setup/)\\n        *   [Span data](https://openai.github.io/openai-agents-python/ref/tracing/span_data/)\\n        *   [Util](https://openai.github.io/openai-agents-python/ref/tracing/util/)\\n\\n    *   - [x]  Realtime   Realtime  \\n        *   [RealtimeAgent](https://openai.github.io/openai-agents-python/ref/realtime/agent/)\\n        *   [RealtimeRunner](https://openai.github.io/openai-agents-python/ref/realtime/runner/)\\n        *   [RealtimeSession](https://openai.github.io/openai-agents-python/ref/realtime/session/)\\n        *   [Realtime Events](https://openai.github.io/openai-agents-python/ref/realtime/events/)\\n        *   [Realtime Configuration](https://openai.github.io/openai-agents-python/ref/realtime/config/)\\n        *   [Model](https://openai.github.io/openai-agents-python/ref/realtime/model/)\\n\\n    *   - [x]  Voice   Voice  \\n        *   [Pipeline](https://openai.github.io/openai-agents-python/ref/voice/pipeline/)\\n        *   [Workflow](https://openai.github.io/openai-agents-python/ref/voice/workflow/)\\n        *   [Input](https://openai.github.io/openai-agents-python/ref/voice/input/)\\n        *   [Result](https://openai.github.io/openai-agents-python/ref/voice/result/)\\n        *   [Pipeline Config](https://openai.github.io/openai-agents-python/ref/voice/pipeline_config/)\\n        *   [Events](https://openai.github.io/openai-agents-python/ref/voice/events/)\\n        *   [Exceptions](https://openai.github.io/openai-agents-python/ref/voice/exceptions/)\\n        *   [Model](https://openai.github.io/openai-agents-python/ref/voice/model/)\\n        *   [Utils](https://openai.github.io/openai-agents-python/ref/voice/utils/)\\n        *   [OpenAIVoiceModelProvider](https://openai.github.io/openai-agents-python/ref/voice/models/openai_provider/)\\n        *   [OpenAI STT](https://openai.github.io/openai-agents-python/ref/voice/models/openai_stt/)\\n        *   [OpenAI TTS](https://openai.github.io/openai-agents-python/ref/voice/models/openai_tts/)\\n\\n    *   - [x]  Extensions   Extensions  \\n        *   [Handoff filters](https://openai.github.io/openai-agents-python/ref/extensions/handoff_filters/)\\n        *   [Handoff prompt](https://openai.github.io/openai-agents-python/ref/extensions/handoff_prompt/)\\n        *   [LiteLLM Models](https://openai.github.io/openai-agents-python/ref/extensions/litellm/)\\n        *   [SQLAlchemySession](https://openai.github.io/openai-agents-python/ref/extensions/memory/sqlalchemy_session/)\\n        *   [EncryptedSession](https://openai.github.io/openai-agents-python/ref/extensions/memory/encrypt_session/)\\n\\n Table of contents  \\n*   [Why use the Agents SDK](https://openai.github.io/openai-agents-python/#why-use-the-agents-sdk)\\n*   [Installation](https://openai.github.io/openai-agents-python/#installation)\\n*   [Hello world example](https://openai.github.io/openai-agents-python/#hello-world-example)\\n\\nOpenAI Agents SDK\\n=================\\n\\nThe [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It\\'s a production-ready upgrade of our previous experimentation for agents, [Swarm](https://github.com/openai/swarm/tree/main). The Agents SDK has a very small set of primitives:\\n\\n*   **Agents**, which are LLMs equipped with instructions and tools\\n*   **Handoffs**, which allow agents to delegate to other agents for specific tasks\\n*   **Guardrails**, which enable validation of agent inputs and outputs\\n*   **Sessions**, which automatically maintains conversation history across agent runs\\n\\nIn combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in **tracing** that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.\\n\\nWhy use the Agents SDK\\n----------------------\\n\\nThe SDK has two driving design principles:\\n\\n1.   Enough features to be worth using, but few enough primitives to make it quick to learn.\\n2.   Works great out of the box, but you can customize exactly what happens.\\n\\nHere are the main features of the SDK:\\n\\n*   Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\\n*   Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\\n*   Handoffs: A powerful feature to coordinate and delegate between multiple agents.\\n*   Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\\n*   Sessions: Automatic conversation history management across agent runs, eliminating manual state handling.\\n*   Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\\n*   Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\\n\\nInstallation\\n------------\\n\\n```\\npip install openai-agents\\n```\\n\\nHello world example\\n-------------------\\n\\n```\\nfrom agents import Agent, Runner\\n\\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\\n\\nresult = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\\nprint(result.final_output)\\n\\n# Code within the code,\\n# Functions calling themselves,\\n# Infinite loop\\'s dance.\\n```\\n\\n(_If running this, ensure you set the `OPENAI\\\\_API\\\\_KEY` environment variable_)\\n\\n```\\nexport OPENAI_API_KEY=sk-...\\n```\\n', 'type': 'function_call_output'}, output='Title: OpenAI Agents SDK\\n\\nURL Source: https://openai.github.io/openai-agents-python/\\n\\nPublished Time: Wed, 15 Oct 2025 00:12:28 GMT\\n\\nMarkdown Content:\\nOpenAI Agents SDK\\n\\n===============\\n- [x] - [x] \\n\\n[Skip to content](https://openai.github.io/openai-agents-python/#openai-agents-sdk)\\n\\n[![Image 1: logo](https://openai.github.io/openai-agents-python/assets/logo.svg)](https://openai.github.io/openai-agents-python/ \"OpenAI Agents SDK\")\\n\\n OpenAI Agents SDK \\n\\n Intro \\n\\n*   [English](https://openai.github.io/openai-agents-python/)\\n*   [日本語](https://openai.github.io/openai-agents-python/ja/)\\n*   [한국어](https://openai.github.io/openai-agents-python/ko/)\\n*   [简体中文](https://openai.github.io/openai-agents-python/zh/)\\n\\nType to start searching\\n\\n[openai-agents-python * v0.3.3 * 16.5k * 2.7k](https://github.com/openai/openai-agents-python \"Go to repository\")\\n\\n[![Image 2: logo](https://openai.github.io/openai-agents-python/assets/logo.svg)](https://openai.github.io/openai-agents-python/ \"OpenAI Agents SDK\") OpenAI Agents SDK  \\n\\n[openai-agents-python * v0.3.3 * 16.5k * 2.7k](https://github.com/openai/openai-agents-python \"Go to repository\")\\n\\n*   - [x]  Intro  [Intro](https://openai.github.io/openai-agents-python/) Table of contents  \\n    *   [Why use the Agents SDK](https://openai.github.io/openai-agents-python/#why-use-the-agents-sdk)\\n    *   [Installation](https://openai.github.io/openai-agents-python/#installation)\\n    *   [Hello world example](https://openai.github.io/openai-agents-python/#hello-world-example)\\n\\n*   [Quickstart](https://openai.github.io/openai-agents-python/quickstart/)\\n*   [Examples](https://openai.github.io/openai-agents-python/examples/)\\n*   - [x]  Documentation   Documentation  \\n    *   [Agents](https://openai.github.io/openai-agents-python/agents/)\\n    *   [Running agents](https://openai.github.io/openai-agents-python/running_agents/)\\n    *   [Sessions](https://openai.github.io/openai-agents-python/sessions/)\\n    *   [Results](https://openai.github.io/openai-agents-python/results/)\\n    *   [Streaming](https://openai.github.io/openai-agents-python/streaming/)\\n    *   [REPL utility](https://openai.github.io/openai-agents-python/repl/)\\n    *   [Tools](https://openai.github.io/openai-agents-python/tools/)\\n    *   [Model context protocol (MCP)](https://openai.github.io/openai-agents-python/mcp/)\\n    *   [Handoffs](https://openai.github.io/openai-agents-python/handoffs/)\\n    *   [Tracing](https://openai.github.io/openai-agents-python/tracing/)\\n    *   [Context management](https://openai.github.io/openai-agents-python/context/)\\n    *   [Guardrails](https://openai.github.io/openai-agents-python/guardrails/)\\n    *   [Orchestrating multiple agents](https://openai.github.io/openai-agents-python/multi_agent/)\\n    *   [Usage](https://openai.github.io/openai-agents-python/usage/)\\n    *   - [x]  Models   Models  \\n        *   [Models](https://openai.github.io/openai-agents-python/models/)\\n        *   [Using any model via LiteLLM](https://openai.github.io/openai-agents-python/models/litellm/)\\n\\n    *   [Configuring the SDK](https://openai.github.io/openai-agents-python/config/)\\n    *   [Agent Visualization](https://openai.github.io/openai-agents-python/visualization/)\\n    *   [Release process/changelog](https://openai.github.io/openai-agents-python/release/)\\n    *   - [x]  Voice agents   Voice agents  \\n        *   [Quickstart](https://openai.github.io/openai-agents-python/voice/quickstart/)\\n        *   [Pipelines and workflows](https://openai.github.io/openai-agents-python/voice/pipeline/)\\n        *   [Tracing](https://openai.github.io/openai-agents-python/voice/tracing/)\\n\\n    *   - [x]  Realtime agents   Realtime agents  \\n        *   [Quickstart](https://openai.github.io/openai-agents-python/realtime/quickstart/)\\n        *   [Guide](https://openai.github.io/openai-agents-python/realtime/guide/)\\n\\n*   - [x]  API Reference   API Reference  \\n    *   - [x]  Agents   Agents  \\n        *   [Agents module](https://openai.github.io/openai-agents-python/ref/)\\n        *   [Agents](https://openai.github.io/openai-agents-python/ref/agent/)\\n        *   [Runner](https://openai.github.io/openai-agents-python/ref/run/)\\n        *   [Memory](https://openai.github.io/openai-agents-python/ref/memory/)\\n        *   [repl](https://openai.github.io/openai-agents-python/ref/repl/)\\n        *   [Tools](https://openai.github.io/openai-agents-python/ref/tool/)\\n        *   [Tool Context](https://openai.github.io/openai-agents-python/ref/tool_context/)\\n        *   [Results](https://openai.github.io/openai-agents-python/ref/result/)\\n        *   [Streaming events](https://openai.github.io/openai-agents-python/ref/stream_events/)\\n        *   [Handoffs](https://openai.github.io/openai-agents-python/ref/handoffs/)\\n        *   [Lifecycle](https://openai.github.io/openai-agents-python/ref/lifecycle/)\\n        *   [Items](https://openai.github.io/openai-agents-python/ref/items/)\\n        *   [Run context](https://openai.github.io/openai-agents-python/ref/run_context/)\\n        *   [Tool Context](https://openai.github.io/openai-agents-python/ref/tool_context/)\\n        *   [Usage](https://openai.github.io/openai-agents-python/ref/usage/)\\n        *   [Exceptions](https://openai.github.io/openai-agents-python/ref/exceptions/)\\n        *   [Guardrails](https://openai.github.io/openai-agents-python/ref/guardrail/)\\n        *   [Model settings](https://openai.github.io/openai-agents-python/ref/model_settings/)\\n        *   [Agent output](https://openai.github.io/openai-agents-python/ref/agent_output/)\\n        *   [Function schema](https://openai.github.io/openai-agents-python/ref/function_schema/)\\n        *   [Model interface](https://openai.github.io/openai-agents-python/ref/models/interface/)\\n        *   [OpenAI Chat Completions model](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/)\\n        *   [OpenAI Responses model](https://openai.github.io/openai-agents-python/ref/models/openai_responses/)\\n        *   [MCP Servers](https://openai.github.io/openai-agents-python/ref/mcp/server/)\\n        *   [MCP Util](https://openai.github.io/openai-agents-python/ref/mcp/util/)\\n\\n    *   - [x]  Tracing   Tracing  \\n        *   [Tracing module](https://openai.github.io/openai-agents-python/ref/tracing/)\\n        *   [Creating traces/spans](https://openai.github.io/openai-agents-python/ref/tracing/create/)\\n        *   [Traces](https://openai.github.io/openai-agents-python/ref/tracing/traces/)\\n        *   [Spans](https://openai.github.io/openai-agents-python/ref/tracing/spans/)\\n        *   [Processor interface](https://openai.github.io/openai-agents-python/ref/tracing/processor_interface/)\\n        *   [Processors](https://openai.github.io/openai-agents-python/ref/tracing/processors/)\\n        *   [Scope](https://openai.github.io/openai-agents-python/ref/tracing/scope/)\\n        *   [Setup](https://openai.github.io/openai-agents-python/ref/tracing/setup/)\\n        *   [Span data](https://openai.github.io/openai-agents-python/ref/tracing/span_data/)\\n        *   [Util](https://openai.github.io/openai-agents-python/ref/tracing/util/)\\n\\n    *   - [x]  Realtime   Realtime  \\n        *   [RealtimeAgent](https://openai.github.io/openai-agents-python/ref/realtime/agent/)\\n        *   [RealtimeRunner](https://openai.github.io/openai-agents-python/ref/realtime/runner/)\\n        *   [RealtimeSession](https://openai.github.io/openai-agents-python/ref/realtime/session/)\\n        *   [Realtime Events](https://openai.github.io/openai-agents-python/ref/realtime/events/)\\n        *   [Realtime Configuration](https://openai.github.io/openai-agents-python/ref/realtime/config/)\\n        *   [Model](https://openai.github.io/openai-agents-python/ref/realtime/model/)\\n\\n    *   - [x]  Voice   Voice  \\n        *   [Pipeline](https://openai.github.io/openai-agents-python/ref/voice/pipeline/)\\n        *   [Workflow](https://openai.github.io/openai-agents-python/ref/voice/workflow/)\\n        *   [Input](https://openai.github.io/openai-agents-python/ref/voice/input/)\\n        *   [Result](https://openai.github.io/openai-agents-python/ref/voice/result/)\\n        *   [Pipeline Config](https://openai.github.io/openai-agents-python/ref/voice/pipeline_config/)\\n        *   [Events](https://openai.github.io/openai-agents-python/ref/voice/events/)\\n        *   [Exceptions](https://openai.github.io/openai-agents-python/ref/voice/exceptions/)\\n        *   [Model](https://openai.github.io/openai-agents-python/ref/voice/model/)\\n        *   [Utils](https://openai.github.io/openai-agents-python/ref/voice/utils/)\\n        *   [OpenAIVoiceModelProvider](https://openai.github.io/openai-agents-python/ref/voice/models/openai_provider/)\\n        *   [OpenAI STT](https://openai.github.io/openai-agents-python/ref/voice/models/openai_stt/)\\n        *   [OpenAI TTS](https://openai.github.io/openai-agents-python/ref/voice/models/openai_tts/)\\n\\n    *   - [x]  Extensions   Extensions  \\n        *   [Handoff filters](https://openai.github.io/openai-agents-python/ref/extensions/handoff_filters/)\\n        *   [Handoff prompt](https://openai.github.io/openai-agents-python/ref/extensions/handoff_prompt/)\\n        *   [LiteLLM Models](https://openai.github.io/openai-agents-python/ref/extensions/litellm/)\\n        *   [SQLAlchemySession](https://openai.github.io/openai-agents-python/ref/extensions/memory/sqlalchemy_session/)\\n        *   [EncryptedSession](https://openai.github.io/openai-agents-python/ref/extensions/memory/encrypt_session/)\\n\\n Table of contents  \\n*   [Why use the Agents SDK](https://openai.github.io/openai-agents-python/#why-use-the-agents-sdk)\\n*   [Installation](https://openai.github.io/openai-agents-python/#installation)\\n*   [Hello world example](https://openai.github.io/openai-agents-python/#hello-world-example)\\n\\nOpenAI Agents SDK\\n=================\\n\\nThe [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It\\'s a production-ready upgrade of our previous experimentation for agents, [Swarm](https://github.com/openai/swarm/tree/main). The Agents SDK has a very small set of primitives:\\n\\n*   **Agents**, which are LLMs equipped with instructions and tools\\n*   **Handoffs**, which allow agents to delegate to other agents for specific tasks\\n*   **Guardrails**, which enable validation of agent inputs and outputs\\n*   **Sessions**, which automatically maintains conversation history across agent runs\\n\\nIn combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in **tracing** that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.\\n\\nWhy use the Agents SDK\\n----------------------\\n\\nThe SDK has two driving design principles:\\n\\n1.   Enough features to be worth using, but few enough primitives to make it quick to learn.\\n2.   Works great out of the box, but you can customize exactly what happens.\\n\\nHere are the main features of the SDK:\\n\\n*   Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\\n*   Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\\n*   Handoffs: A powerful feature to coordinate and delegate between multiple agents.\\n*   Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\\n*   Sessions: Automatic conversation history management across agent runs, eliminating manual state handling.\\n*   Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\\n*   Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\\n\\nInstallation\\n------------\\n\\n```\\npip install openai-agents\\n```\\n\\nHello world example\\n-------------------\\n\\n```\\nfrom agents import Agent, Runner\\n\\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\\n\\nresult = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\\nprint(result.final_output)\\n\\n# Code within the code,\\n# Functions calling themselves,\\n# Infinite loop\\'s dance.\\n```\\n\\n(_If running this, ensure you set the `OPENAI\\\\_API\\\\_KEY` environment variable_)\\n\\n```\\nexport OPENAI_API_KEY=sk-...\\n```\\n', type='tool_call_output_item'), MessageOutputItem(agent=Agent(name='web_agent', handoff_description=None, tools=[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)], mcp_servers=[], mcp_config={}, instructions=\"\\nYou're a helpful assistant that helps answer user questions.\\n\", prompt=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='msg_0054eb01405294e40068ef25c69f548196b254846a2a53bdf7', content=[ResponseOutputText(annotations=[], text='The **OpenAI Agents SDK** provides a framework for building agentic AI applications using a simplified and lightweight approach. Key features include:\\n\\n- **Agents**: Large Language Models (LLMs) that utilize instructions and tools.\\n- **Handoffs**: Allow agents to delegate tasks to other agents.\\n- **Guardrails**: Validate inputs and outputs for improved reliability.\\n- **Sessions**: Automatically manage conversation history during agent operations.\\n\\n### Why Use the SDK\\n- It balances feature richness with ease of learning.\\n- Designed to work seamlessly with Python, supporting built-in features without needing new abstractions.\\n- Facilitates agent coordination, input validation, and conversation tracking.\\n- Includes tracing capabilities for workflow monitoring and debugging.\\n\\n### Installation\\nYou can install the SDK using:\\n```bash\\npip install openai-agents\\n```\\n\\n### Example Usage\\nA simple example demonstrates creating an assistant agent to generate a haiku.\\n\\nFor more in-depth information, visit the [OpenAI Agents SDK documentation](https://openai.github.io/openai-agents-python/).', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), type='message_output_item')], raw_responses=[ModelResponse(output=[ResponseFunctionToolCall(arguments='{\"url\":\"https://openai.github.io/openai-agents-python/\"}', call_id='call_omvYjRh7FkwVE54GzArC0kt9', name='fetch_url', type='function_call', id='fc_0054eb01405294e40068ef25c4d08c8196bda5641a7831e31d', status='completed')], usage=Usage(requests=1, input_tokens=89, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=26, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=115), response_id='resp_0054eb01405294e40068ef25c3c87481968abc790448e61cd9'), ModelResponse(output=[ResponseOutputMessage(id='msg_0054eb01405294e40068ef25c69f548196b254846a2a53bdf7', content=[ResponseOutputText(annotations=[], text='The **OpenAI Agents SDK** provides a framework for building agentic AI applications using a simplified and lightweight approach. Key features include:\\n\\n- **Agents**: Large Language Models (LLMs) that utilize instructions and tools.\\n- **Handoffs**: Allow agents to delegate tasks to other agents.\\n- **Guardrails**: Validate inputs and outputs for improved reliability.\\n- **Sessions**: Automatically manage conversation history during agent operations.\\n\\n### Why Use the SDK\\n- It balances feature richness with ease of learning.\\n- Designed to work seamlessly with Python, supporting built-in features without needing new abstractions.\\n- Facilitates agent coordination, input validation, and conversation tracking.\\n- Includes tracing capabilities for workflow monitoring and debugging.\\n\\n### Installation\\nYou can install the SDK using:\\n```bash\\npip install openai-agents\\n```\\n\\n### Example Usage\\nA simple example demonstrates creating an assistant agent to generate a haiku.\\n\\nFor more in-depth information, visit the [OpenAI Agents SDK documentation](https://openai.github.io/openai-agents-python/).', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=3291, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=215, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=3506), response_id='resp_0054eb01405294e40068ef25c5c14081968b48a4d9c824e0ac')], final_output='The **OpenAI Agents SDK** provides a framework for building agentic AI applications using a simplified and lightweight approach. Key features include:\\n\\n- **Agents**: Large Language Models (LLMs) that utilize instructions and tools.\\n- **Handoffs**: Allow agents to delegate tasks to other agents.\\n- **Guardrails**: Validate inputs and outputs for improved reliability.\\n- **Sessions**: Automatically manage conversation history during agent operations.\\n\\n### Why Use the SDK\\n- It balances feature richness with ease of learning.\\n- Designed to work seamlessly with Python, supporting built-in features without needing new abstractions.\\n- Facilitates agent coordination, input validation, and conversation tracking.\\n- Includes tracing capabilities for workflow monitoring and debugging.\\n\\n### Installation\\nYou can install the SDK using:\\n```bash\\npip install openai-agents\\n```\\n\\n### Example Usage\\nA simple example demonstrates creating an assistant agent to generate a haiku.\\n\\nFor more in-depth information, visit the [OpenAI Agents SDK documentation](https://openai.github.io/openai-agents-python/).', input_guardrail_results=[], output_guardrail_results=[], tool_input_guardrail_results=[], tool_output_guardrail_results=[], context_wrapper=RunContextWrapper(context=None, usage=Usage(requests=2, input_tokens=3380, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=241, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=3621)), _last_agent=Agent(name='web_agent', handoff_description=None, tools=[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)], mcp_servers=[], mcp_config={}, instructions=\"\\nYou're a helpful assistant that helps answer user questions.\\n\", prompt=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab104fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = results.new_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d047ab79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolCallItem(agent=Agent(name='web_agent', handoff_description=None, tools=[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)], mcp_servers=[], mcp_config={}, instructions=\"\\nYou're a helpful assistant that helps answer user questions.\\n\", prompt=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{\"url\":\"https://openai.github.io/openai-agents-python/\"}', call_id='call_omvYjRh7FkwVE54GzArC0kt9', name='fetch_url', type='function_call', id='fc_0054eb01405294e40068ef25c4d08c8196bda5641a7831e31d', status='completed'), type='tool_call_item'),\n",
       " ToolCallOutputItem(agent=Agent(name='web_agent', handoff_description=None, tools=[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)], mcp_servers=[], mcp_config={}, instructions=\"\\nYou're a helpful assistant that helps answer user questions.\\n\", prompt=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': 'call_omvYjRh7FkwVE54GzArC0kt9', 'output': 'Title: OpenAI Agents SDK\\n\\nURL Source: https://openai.github.io/openai-agents-python/\\n\\nPublished Time: Wed, 15 Oct 2025 00:12:28 GMT\\n\\nMarkdown Content:\\nOpenAI Agents SDK\\n\\n===============\\n- [x] - [x] \\n\\n[Skip to content](https://openai.github.io/openai-agents-python/#openai-agents-sdk)\\n\\n[![Image 1: logo](https://openai.github.io/openai-agents-python/assets/logo.svg)](https://openai.github.io/openai-agents-python/ \"OpenAI Agents SDK\")\\n\\n OpenAI Agents SDK \\n\\n Intro \\n\\n*   [English](https://openai.github.io/openai-agents-python/)\\n*   [日本語](https://openai.github.io/openai-agents-python/ja/)\\n*   [한국어](https://openai.github.io/openai-agents-python/ko/)\\n*   [简体中文](https://openai.github.io/openai-agents-python/zh/)\\n\\nType to start searching\\n\\n[openai-agents-python * v0.3.3 * 16.5k * 2.7k](https://github.com/openai/openai-agents-python \"Go to repository\")\\n\\n[![Image 2: logo](https://openai.github.io/openai-agents-python/assets/logo.svg)](https://openai.github.io/openai-agents-python/ \"OpenAI Agents SDK\") OpenAI Agents SDK  \\n\\n[openai-agents-python * v0.3.3 * 16.5k * 2.7k](https://github.com/openai/openai-agents-python \"Go to repository\")\\n\\n*   - [x]  Intro  [Intro](https://openai.github.io/openai-agents-python/) Table of contents  \\n    *   [Why use the Agents SDK](https://openai.github.io/openai-agents-python/#why-use-the-agents-sdk)\\n    *   [Installation](https://openai.github.io/openai-agents-python/#installation)\\n    *   [Hello world example](https://openai.github.io/openai-agents-python/#hello-world-example)\\n\\n*   [Quickstart](https://openai.github.io/openai-agents-python/quickstart/)\\n*   [Examples](https://openai.github.io/openai-agents-python/examples/)\\n*   - [x]  Documentation   Documentation  \\n    *   [Agents](https://openai.github.io/openai-agents-python/agents/)\\n    *   [Running agents](https://openai.github.io/openai-agents-python/running_agents/)\\n    *   [Sessions](https://openai.github.io/openai-agents-python/sessions/)\\n    *   [Results](https://openai.github.io/openai-agents-python/results/)\\n    *   [Streaming](https://openai.github.io/openai-agents-python/streaming/)\\n    *   [REPL utility](https://openai.github.io/openai-agents-python/repl/)\\n    *   [Tools](https://openai.github.io/openai-agents-python/tools/)\\n    *   [Model context protocol (MCP)](https://openai.github.io/openai-agents-python/mcp/)\\n    *   [Handoffs](https://openai.github.io/openai-agents-python/handoffs/)\\n    *   [Tracing](https://openai.github.io/openai-agents-python/tracing/)\\n    *   [Context management](https://openai.github.io/openai-agents-python/context/)\\n    *   [Guardrails](https://openai.github.io/openai-agents-python/guardrails/)\\n    *   [Orchestrating multiple agents](https://openai.github.io/openai-agents-python/multi_agent/)\\n    *   [Usage](https://openai.github.io/openai-agents-python/usage/)\\n    *   - [x]  Models   Models  \\n        *   [Models](https://openai.github.io/openai-agents-python/models/)\\n        *   [Using any model via LiteLLM](https://openai.github.io/openai-agents-python/models/litellm/)\\n\\n    *   [Configuring the SDK](https://openai.github.io/openai-agents-python/config/)\\n    *   [Agent Visualization](https://openai.github.io/openai-agents-python/visualization/)\\n    *   [Release process/changelog](https://openai.github.io/openai-agents-python/release/)\\n    *   - [x]  Voice agents   Voice agents  \\n        *   [Quickstart](https://openai.github.io/openai-agents-python/voice/quickstart/)\\n        *   [Pipelines and workflows](https://openai.github.io/openai-agents-python/voice/pipeline/)\\n        *   [Tracing](https://openai.github.io/openai-agents-python/voice/tracing/)\\n\\n    *   - [x]  Realtime agents   Realtime agents  \\n        *   [Quickstart](https://openai.github.io/openai-agents-python/realtime/quickstart/)\\n        *   [Guide](https://openai.github.io/openai-agents-python/realtime/guide/)\\n\\n*   - [x]  API Reference   API Reference  \\n    *   - [x]  Agents   Agents  \\n        *   [Agents module](https://openai.github.io/openai-agents-python/ref/)\\n        *   [Agents](https://openai.github.io/openai-agents-python/ref/agent/)\\n        *   [Runner](https://openai.github.io/openai-agents-python/ref/run/)\\n        *   [Memory](https://openai.github.io/openai-agents-python/ref/memory/)\\n        *   [repl](https://openai.github.io/openai-agents-python/ref/repl/)\\n        *   [Tools](https://openai.github.io/openai-agents-python/ref/tool/)\\n        *   [Tool Context](https://openai.github.io/openai-agents-python/ref/tool_context/)\\n        *   [Results](https://openai.github.io/openai-agents-python/ref/result/)\\n        *   [Streaming events](https://openai.github.io/openai-agents-python/ref/stream_events/)\\n        *   [Handoffs](https://openai.github.io/openai-agents-python/ref/handoffs/)\\n        *   [Lifecycle](https://openai.github.io/openai-agents-python/ref/lifecycle/)\\n        *   [Items](https://openai.github.io/openai-agents-python/ref/items/)\\n        *   [Run context](https://openai.github.io/openai-agents-python/ref/run_context/)\\n        *   [Tool Context](https://openai.github.io/openai-agents-python/ref/tool_context/)\\n        *   [Usage](https://openai.github.io/openai-agents-python/ref/usage/)\\n        *   [Exceptions](https://openai.github.io/openai-agents-python/ref/exceptions/)\\n        *   [Guardrails](https://openai.github.io/openai-agents-python/ref/guardrail/)\\n        *   [Model settings](https://openai.github.io/openai-agents-python/ref/model_settings/)\\n        *   [Agent output](https://openai.github.io/openai-agents-python/ref/agent_output/)\\n        *   [Function schema](https://openai.github.io/openai-agents-python/ref/function_schema/)\\n        *   [Model interface](https://openai.github.io/openai-agents-python/ref/models/interface/)\\n        *   [OpenAI Chat Completions model](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/)\\n        *   [OpenAI Responses model](https://openai.github.io/openai-agents-python/ref/models/openai_responses/)\\n        *   [MCP Servers](https://openai.github.io/openai-agents-python/ref/mcp/server/)\\n        *   [MCP Util](https://openai.github.io/openai-agents-python/ref/mcp/util/)\\n\\n    *   - [x]  Tracing   Tracing  \\n        *   [Tracing module](https://openai.github.io/openai-agents-python/ref/tracing/)\\n        *   [Creating traces/spans](https://openai.github.io/openai-agents-python/ref/tracing/create/)\\n        *   [Traces](https://openai.github.io/openai-agents-python/ref/tracing/traces/)\\n        *   [Spans](https://openai.github.io/openai-agents-python/ref/tracing/spans/)\\n        *   [Processor interface](https://openai.github.io/openai-agents-python/ref/tracing/processor_interface/)\\n        *   [Processors](https://openai.github.io/openai-agents-python/ref/tracing/processors/)\\n        *   [Scope](https://openai.github.io/openai-agents-python/ref/tracing/scope/)\\n        *   [Setup](https://openai.github.io/openai-agents-python/ref/tracing/setup/)\\n        *   [Span data](https://openai.github.io/openai-agents-python/ref/tracing/span_data/)\\n        *   [Util](https://openai.github.io/openai-agents-python/ref/tracing/util/)\\n\\n    *   - [x]  Realtime   Realtime  \\n        *   [RealtimeAgent](https://openai.github.io/openai-agents-python/ref/realtime/agent/)\\n        *   [RealtimeRunner](https://openai.github.io/openai-agents-python/ref/realtime/runner/)\\n        *   [RealtimeSession](https://openai.github.io/openai-agents-python/ref/realtime/session/)\\n        *   [Realtime Events](https://openai.github.io/openai-agents-python/ref/realtime/events/)\\n        *   [Realtime Configuration](https://openai.github.io/openai-agents-python/ref/realtime/config/)\\n        *   [Model](https://openai.github.io/openai-agents-python/ref/realtime/model/)\\n\\n    *   - [x]  Voice   Voice  \\n        *   [Pipeline](https://openai.github.io/openai-agents-python/ref/voice/pipeline/)\\n        *   [Workflow](https://openai.github.io/openai-agents-python/ref/voice/workflow/)\\n        *   [Input](https://openai.github.io/openai-agents-python/ref/voice/input/)\\n        *   [Result](https://openai.github.io/openai-agents-python/ref/voice/result/)\\n        *   [Pipeline Config](https://openai.github.io/openai-agents-python/ref/voice/pipeline_config/)\\n        *   [Events](https://openai.github.io/openai-agents-python/ref/voice/events/)\\n        *   [Exceptions](https://openai.github.io/openai-agents-python/ref/voice/exceptions/)\\n        *   [Model](https://openai.github.io/openai-agents-python/ref/voice/model/)\\n        *   [Utils](https://openai.github.io/openai-agents-python/ref/voice/utils/)\\n        *   [OpenAIVoiceModelProvider](https://openai.github.io/openai-agents-python/ref/voice/models/openai_provider/)\\n        *   [OpenAI STT](https://openai.github.io/openai-agents-python/ref/voice/models/openai_stt/)\\n        *   [OpenAI TTS](https://openai.github.io/openai-agents-python/ref/voice/models/openai_tts/)\\n\\n    *   - [x]  Extensions   Extensions  \\n        *   [Handoff filters](https://openai.github.io/openai-agents-python/ref/extensions/handoff_filters/)\\n        *   [Handoff prompt](https://openai.github.io/openai-agents-python/ref/extensions/handoff_prompt/)\\n        *   [LiteLLM Models](https://openai.github.io/openai-agents-python/ref/extensions/litellm/)\\n        *   [SQLAlchemySession](https://openai.github.io/openai-agents-python/ref/extensions/memory/sqlalchemy_session/)\\n        *   [EncryptedSession](https://openai.github.io/openai-agents-python/ref/extensions/memory/encrypt_session/)\\n\\n Table of contents  \\n*   [Why use the Agents SDK](https://openai.github.io/openai-agents-python/#why-use-the-agents-sdk)\\n*   [Installation](https://openai.github.io/openai-agents-python/#installation)\\n*   [Hello world example](https://openai.github.io/openai-agents-python/#hello-world-example)\\n\\nOpenAI Agents SDK\\n=================\\n\\nThe [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It\\'s a production-ready upgrade of our previous experimentation for agents, [Swarm](https://github.com/openai/swarm/tree/main). The Agents SDK has a very small set of primitives:\\n\\n*   **Agents**, which are LLMs equipped with instructions and tools\\n*   **Handoffs**, which allow agents to delegate to other agents for specific tasks\\n*   **Guardrails**, which enable validation of agent inputs and outputs\\n*   **Sessions**, which automatically maintains conversation history across agent runs\\n\\nIn combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in **tracing** that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.\\n\\nWhy use the Agents SDK\\n----------------------\\n\\nThe SDK has two driving design principles:\\n\\n1.   Enough features to be worth using, but few enough primitives to make it quick to learn.\\n2.   Works great out of the box, but you can customize exactly what happens.\\n\\nHere are the main features of the SDK:\\n\\n*   Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\\n*   Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\\n*   Handoffs: A powerful feature to coordinate and delegate between multiple agents.\\n*   Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\\n*   Sessions: Automatic conversation history management across agent runs, eliminating manual state handling.\\n*   Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\\n*   Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\\n\\nInstallation\\n------------\\n\\n```\\npip install openai-agents\\n```\\n\\nHello world example\\n-------------------\\n\\n```\\nfrom agents import Agent, Runner\\n\\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\\n\\nresult = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\\nprint(result.final_output)\\n\\n# Code within the code,\\n# Functions calling themselves,\\n# Infinite loop\\'s dance.\\n```\\n\\n(_If running this, ensure you set the `OPENAI\\\\_API\\\\_KEY` environment variable_)\\n\\n```\\nexport OPENAI_API_KEY=sk-...\\n```\\n', 'type': 'function_call_output'}, output='Title: OpenAI Agents SDK\\n\\nURL Source: https://openai.github.io/openai-agents-python/\\n\\nPublished Time: Wed, 15 Oct 2025 00:12:28 GMT\\n\\nMarkdown Content:\\nOpenAI Agents SDK\\n\\n===============\\n- [x] - [x] \\n\\n[Skip to content](https://openai.github.io/openai-agents-python/#openai-agents-sdk)\\n\\n[![Image 1: logo](https://openai.github.io/openai-agents-python/assets/logo.svg)](https://openai.github.io/openai-agents-python/ \"OpenAI Agents SDK\")\\n\\n OpenAI Agents SDK \\n\\n Intro \\n\\n*   [English](https://openai.github.io/openai-agents-python/)\\n*   [日本語](https://openai.github.io/openai-agents-python/ja/)\\n*   [한국어](https://openai.github.io/openai-agents-python/ko/)\\n*   [简体中文](https://openai.github.io/openai-agents-python/zh/)\\n\\nType to start searching\\n\\n[openai-agents-python * v0.3.3 * 16.5k * 2.7k](https://github.com/openai/openai-agents-python \"Go to repository\")\\n\\n[![Image 2: logo](https://openai.github.io/openai-agents-python/assets/logo.svg)](https://openai.github.io/openai-agents-python/ \"OpenAI Agents SDK\") OpenAI Agents SDK  \\n\\n[openai-agents-python * v0.3.3 * 16.5k * 2.7k](https://github.com/openai/openai-agents-python \"Go to repository\")\\n\\n*   - [x]  Intro  [Intro](https://openai.github.io/openai-agents-python/) Table of contents  \\n    *   [Why use the Agents SDK](https://openai.github.io/openai-agents-python/#why-use-the-agents-sdk)\\n    *   [Installation](https://openai.github.io/openai-agents-python/#installation)\\n    *   [Hello world example](https://openai.github.io/openai-agents-python/#hello-world-example)\\n\\n*   [Quickstart](https://openai.github.io/openai-agents-python/quickstart/)\\n*   [Examples](https://openai.github.io/openai-agents-python/examples/)\\n*   - [x]  Documentation   Documentation  \\n    *   [Agents](https://openai.github.io/openai-agents-python/agents/)\\n    *   [Running agents](https://openai.github.io/openai-agents-python/running_agents/)\\n    *   [Sessions](https://openai.github.io/openai-agents-python/sessions/)\\n    *   [Results](https://openai.github.io/openai-agents-python/results/)\\n    *   [Streaming](https://openai.github.io/openai-agents-python/streaming/)\\n    *   [REPL utility](https://openai.github.io/openai-agents-python/repl/)\\n    *   [Tools](https://openai.github.io/openai-agents-python/tools/)\\n    *   [Model context protocol (MCP)](https://openai.github.io/openai-agents-python/mcp/)\\n    *   [Handoffs](https://openai.github.io/openai-agents-python/handoffs/)\\n    *   [Tracing](https://openai.github.io/openai-agents-python/tracing/)\\n    *   [Context management](https://openai.github.io/openai-agents-python/context/)\\n    *   [Guardrails](https://openai.github.io/openai-agents-python/guardrails/)\\n    *   [Orchestrating multiple agents](https://openai.github.io/openai-agents-python/multi_agent/)\\n    *   [Usage](https://openai.github.io/openai-agents-python/usage/)\\n    *   - [x]  Models   Models  \\n        *   [Models](https://openai.github.io/openai-agents-python/models/)\\n        *   [Using any model via LiteLLM](https://openai.github.io/openai-agents-python/models/litellm/)\\n\\n    *   [Configuring the SDK](https://openai.github.io/openai-agents-python/config/)\\n    *   [Agent Visualization](https://openai.github.io/openai-agents-python/visualization/)\\n    *   [Release process/changelog](https://openai.github.io/openai-agents-python/release/)\\n    *   - [x]  Voice agents   Voice agents  \\n        *   [Quickstart](https://openai.github.io/openai-agents-python/voice/quickstart/)\\n        *   [Pipelines and workflows](https://openai.github.io/openai-agents-python/voice/pipeline/)\\n        *   [Tracing](https://openai.github.io/openai-agents-python/voice/tracing/)\\n\\n    *   - [x]  Realtime agents   Realtime agents  \\n        *   [Quickstart](https://openai.github.io/openai-agents-python/realtime/quickstart/)\\n        *   [Guide](https://openai.github.io/openai-agents-python/realtime/guide/)\\n\\n*   - [x]  API Reference   API Reference  \\n    *   - [x]  Agents   Agents  \\n        *   [Agents module](https://openai.github.io/openai-agents-python/ref/)\\n        *   [Agents](https://openai.github.io/openai-agents-python/ref/agent/)\\n        *   [Runner](https://openai.github.io/openai-agents-python/ref/run/)\\n        *   [Memory](https://openai.github.io/openai-agents-python/ref/memory/)\\n        *   [repl](https://openai.github.io/openai-agents-python/ref/repl/)\\n        *   [Tools](https://openai.github.io/openai-agents-python/ref/tool/)\\n        *   [Tool Context](https://openai.github.io/openai-agents-python/ref/tool_context/)\\n        *   [Results](https://openai.github.io/openai-agents-python/ref/result/)\\n        *   [Streaming events](https://openai.github.io/openai-agents-python/ref/stream_events/)\\n        *   [Handoffs](https://openai.github.io/openai-agents-python/ref/handoffs/)\\n        *   [Lifecycle](https://openai.github.io/openai-agents-python/ref/lifecycle/)\\n        *   [Items](https://openai.github.io/openai-agents-python/ref/items/)\\n        *   [Run context](https://openai.github.io/openai-agents-python/ref/run_context/)\\n        *   [Tool Context](https://openai.github.io/openai-agents-python/ref/tool_context/)\\n        *   [Usage](https://openai.github.io/openai-agents-python/ref/usage/)\\n        *   [Exceptions](https://openai.github.io/openai-agents-python/ref/exceptions/)\\n        *   [Guardrails](https://openai.github.io/openai-agents-python/ref/guardrail/)\\n        *   [Model settings](https://openai.github.io/openai-agents-python/ref/model_settings/)\\n        *   [Agent output](https://openai.github.io/openai-agents-python/ref/agent_output/)\\n        *   [Function schema](https://openai.github.io/openai-agents-python/ref/function_schema/)\\n        *   [Model interface](https://openai.github.io/openai-agents-python/ref/models/interface/)\\n        *   [OpenAI Chat Completions model](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/)\\n        *   [OpenAI Responses model](https://openai.github.io/openai-agents-python/ref/models/openai_responses/)\\n        *   [MCP Servers](https://openai.github.io/openai-agents-python/ref/mcp/server/)\\n        *   [MCP Util](https://openai.github.io/openai-agents-python/ref/mcp/util/)\\n\\n    *   - [x]  Tracing   Tracing  \\n        *   [Tracing module](https://openai.github.io/openai-agents-python/ref/tracing/)\\n        *   [Creating traces/spans](https://openai.github.io/openai-agents-python/ref/tracing/create/)\\n        *   [Traces](https://openai.github.io/openai-agents-python/ref/tracing/traces/)\\n        *   [Spans](https://openai.github.io/openai-agents-python/ref/tracing/spans/)\\n        *   [Processor interface](https://openai.github.io/openai-agents-python/ref/tracing/processor_interface/)\\n        *   [Processors](https://openai.github.io/openai-agents-python/ref/tracing/processors/)\\n        *   [Scope](https://openai.github.io/openai-agents-python/ref/tracing/scope/)\\n        *   [Setup](https://openai.github.io/openai-agents-python/ref/tracing/setup/)\\n        *   [Span data](https://openai.github.io/openai-agents-python/ref/tracing/span_data/)\\n        *   [Util](https://openai.github.io/openai-agents-python/ref/tracing/util/)\\n\\n    *   - [x]  Realtime   Realtime  \\n        *   [RealtimeAgent](https://openai.github.io/openai-agents-python/ref/realtime/agent/)\\n        *   [RealtimeRunner](https://openai.github.io/openai-agents-python/ref/realtime/runner/)\\n        *   [RealtimeSession](https://openai.github.io/openai-agents-python/ref/realtime/session/)\\n        *   [Realtime Events](https://openai.github.io/openai-agents-python/ref/realtime/events/)\\n        *   [Realtime Configuration](https://openai.github.io/openai-agents-python/ref/realtime/config/)\\n        *   [Model](https://openai.github.io/openai-agents-python/ref/realtime/model/)\\n\\n    *   - [x]  Voice   Voice  \\n        *   [Pipeline](https://openai.github.io/openai-agents-python/ref/voice/pipeline/)\\n        *   [Workflow](https://openai.github.io/openai-agents-python/ref/voice/workflow/)\\n        *   [Input](https://openai.github.io/openai-agents-python/ref/voice/input/)\\n        *   [Result](https://openai.github.io/openai-agents-python/ref/voice/result/)\\n        *   [Pipeline Config](https://openai.github.io/openai-agents-python/ref/voice/pipeline_config/)\\n        *   [Events](https://openai.github.io/openai-agents-python/ref/voice/events/)\\n        *   [Exceptions](https://openai.github.io/openai-agents-python/ref/voice/exceptions/)\\n        *   [Model](https://openai.github.io/openai-agents-python/ref/voice/model/)\\n        *   [Utils](https://openai.github.io/openai-agents-python/ref/voice/utils/)\\n        *   [OpenAIVoiceModelProvider](https://openai.github.io/openai-agents-python/ref/voice/models/openai_provider/)\\n        *   [OpenAI STT](https://openai.github.io/openai-agents-python/ref/voice/models/openai_stt/)\\n        *   [OpenAI TTS](https://openai.github.io/openai-agents-python/ref/voice/models/openai_tts/)\\n\\n    *   - [x]  Extensions   Extensions  \\n        *   [Handoff filters](https://openai.github.io/openai-agents-python/ref/extensions/handoff_filters/)\\n        *   [Handoff prompt](https://openai.github.io/openai-agents-python/ref/extensions/handoff_prompt/)\\n        *   [LiteLLM Models](https://openai.github.io/openai-agents-python/ref/extensions/litellm/)\\n        *   [SQLAlchemySession](https://openai.github.io/openai-agents-python/ref/extensions/memory/sqlalchemy_session/)\\n        *   [EncryptedSession](https://openai.github.io/openai-agents-python/ref/extensions/memory/encrypt_session/)\\n\\n Table of contents  \\n*   [Why use the Agents SDK](https://openai.github.io/openai-agents-python/#why-use-the-agents-sdk)\\n*   [Installation](https://openai.github.io/openai-agents-python/#installation)\\n*   [Hello world example](https://openai.github.io/openai-agents-python/#hello-world-example)\\n\\nOpenAI Agents SDK\\n=================\\n\\nThe [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It\\'s a production-ready upgrade of our previous experimentation for agents, [Swarm](https://github.com/openai/swarm/tree/main). The Agents SDK has a very small set of primitives:\\n\\n*   **Agents**, which are LLMs equipped with instructions and tools\\n*   **Handoffs**, which allow agents to delegate to other agents for specific tasks\\n*   **Guardrails**, which enable validation of agent inputs and outputs\\n*   **Sessions**, which automatically maintains conversation history across agent runs\\n\\nIn combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in **tracing** that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.\\n\\nWhy use the Agents SDK\\n----------------------\\n\\nThe SDK has two driving design principles:\\n\\n1.   Enough features to be worth using, but few enough primitives to make it quick to learn.\\n2.   Works great out of the box, but you can customize exactly what happens.\\n\\nHere are the main features of the SDK:\\n\\n*   Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\\n*   Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\\n*   Handoffs: A powerful feature to coordinate and delegate between multiple agents.\\n*   Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\\n*   Sessions: Automatic conversation history management across agent runs, eliminating manual state handling.\\n*   Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\\n*   Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\\n\\nInstallation\\n------------\\n\\n```\\npip install openai-agents\\n```\\n\\nHello world example\\n-------------------\\n\\n```\\nfrom agents import Agent, Runner\\n\\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\\n\\nresult = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\\nprint(result.final_output)\\n\\n# Code within the code,\\n# Functions calling themselves,\\n# Infinite loop\\'s dance.\\n```\\n\\n(_If running this, ensure you set the `OPENAI\\\\_API\\\\_KEY` environment variable_)\\n\\n```\\nexport OPENAI_API_KEY=sk-...\\n```\\n', type='tool_call_output_item'),\n",
       " MessageOutputItem(agent=Agent(name='web_agent', handoff_description=None, tools=[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)], mcp_servers=[], mcp_config={}, instructions=\"\\nYou're a helpful assistant that helps answer user questions.\\n\", prompt=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='msg_0054eb01405294e40068ef25c69f548196b254846a2a53bdf7', content=[ResponseOutputText(annotations=[], text='The **OpenAI Agents SDK** provides a framework for building agentic AI applications using a simplified and lightweight approach. Key features include:\\n\\n- **Agents**: Large Language Models (LLMs) that utilize instructions and tools.\\n- **Handoffs**: Allow agents to delegate tasks to other agents.\\n- **Guardrails**: Validate inputs and outputs for improved reliability.\\n- **Sessions**: Automatically manage conversation history during agent operations.\\n\\n### Why Use the SDK\\n- It balances feature richness with ease of learning.\\n- Designed to work seamlessly with Python, supporting built-in features without needing new abstractions.\\n- Facilitates agent coordination, input validation, and conversation tracking.\\n- Includes tracing capabilities for workflow monitoring and debugging.\\n\\n### Installation\\nYou can install the SDK using:\\n```bash\\npip install openai-agents\\n```\\n\\n### Example Usage\\nA simple example demonstrates creating an assistant agent to generate a haiku.\\n\\nFor more in-depth information, visit the [OpenAI Agents SDK documentation](https://openai.github.io/openai-agents-python/).', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), type='message_output_item')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d429276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)]\n",
      "[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)]\n",
      "[FunctionTool(name='fetch_url', description='Fetch the textual content of a webpage.', params_json_schema={'properties': {'url': {'description': 'The target URL to fetch content from.', 'title': 'Url', 'type': 'string'}}, 'required': ['url'], 'title': 'fetch_url_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7f8160989510>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)]\n"
     ]
    }
   ],
   "source": [
    "for item in items:\n",
    "    print(item.agent.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3960756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIAgentsSDKRunner\n",
    "\n",
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIAgentsSDKRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=web_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0d4e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await runner.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a430e55",
   "metadata": {},
   "source": [
    "### Youtube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79ef4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube video\n",
    "\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def format_timestamp(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to H:MM:SS if > 1 hour, else M:SS\"\"\"\n",
    "    total_seconds = int(seconds)\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, secs = divmod(remainder, 60)\n",
    "\n",
    "    if hours > 0:\n",
    "        return f\"{hours}:{minutes:02}:{secs:02}\"\n",
    "    else:\n",
    "        return f\"{minutes}:{secs:02}\"\n",
    "\n",
    "\n",
    "def make_subtitles(transcript) -> str:\n",
    "    lines = []\n",
    "\n",
    "    for entry in transcript:\n",
    "        ts = format_timestamp(entry.start)\n",
    "        text = entry.text.replace('\\n', ' ')\n",
    "        lines.append(ts + ' ' + text)\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def fetch_transcript_raw(video_id):\n",
    "    ytt_api = YouTubeTranscriptApi()\n",
    "    transcript = ytt_api.fetch(video_id)\n",
    "    return transcript\n",
    "\n",
    "\n",
    "def fetch_transcript_text(video_id):\n",
    "    transcript = fetch_transcript_raw(video_id)\n",
    "    subtitles = make_subtitles(transcript)\n",
    "    return subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39b487cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_youtube_transcript(video_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the transcript of a YouTube video and converts it into a subtitle-formatted string.\n",
    "\n",
    "    \n",
    "\n",
    "    Example:\n",
    "    0:00 Hey everyone, welcome to our event. This\n",
    "    0:02 event is brought to you by data talks\n",
    "    0:03 club which is a community of people who\n",
    "    0:05 love data. We have weekly events today.\n",
    "    0:08 Uh this is one of such events. Um if you\n",
    "    0:11 want to find out more about the events\n",
    "    0:13 we have, there is a link in the\n",
    "    0:14 description. Um so click on that link,\n",
    "    0:16 check it out right now. We actually have\n",
    "    0:19 quite a few events in our pipeline, but\n",
    "    0:21 we need to put them on the website. Uh\n",
    "    0:24 but keep a\n",
    "\n",
    "    Args:\n",
    "        video_id (str): The unique YouTube video ID.\n",
    "\n",
    "    Returns:\n",
    "        str: The subtitles generated from the video's transcript.\n",
    "    \"\"\"\n",
    "    return fetch_transcript_text(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a02619b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00 Hey everyone, welcome to our event. This\n",
      "0:02 event is brought to you by data talks\n",
      "0:03 club which is a community of people who\n",
      "0:05 love data. We have weekly events today.\n",
      "0:08 Uh this is one of such events. Um if you\n",
      "0:11 want to find out more about the events\n",
      "0:13 we have, there is a link in the\n",
      "0:14 description. Um so click on that link,\n",
      "0:16 check it out right now. We actually have\n",
      "0:19 quite a few events in our pipeline, but\n",
      "0:21 we need to put them on the website. Uh\n",
      "0:24 but keep a\n"
     ]
    }
   ],
   "source": [
    "subtitles = fetch_youtube_transcript('vK_SxyqIfwk')\n",
    "print(subtitles[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c047d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_instructions = \"\"\"\n",
    "You're a helpful assistant that helps answer user questions\n",
    "about YouTube videos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "138308b7-048a-4244-bf8f-228bdf1d29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "\n",
    "# Start running ..     with user prompt\n",
    "runner = Runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d3beb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_agent = Agent(\n",
    "    name='youtube_agent',\n",
    "    instructions=summary_instructions,\n",
    "    tools=[function_tool(fetch_youtube_transcript)],\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9280aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"What is this video about https://www.youtube.com/watch?v=vK_SxyqIfwk$t=172s\n",
    "    Output the Example like this:\n",
    "    0:00 Hey everyone, welcome to our event. This\n",
    "    0:02 event is brought to you by data talks\n",
    "    0:03 club which is a community of people who\n",
    "    0:05 love data. We have weekly events today.\n",
    "    0:08 Uh this is one of such events. Um if you\n",
    "    0:11 want to find out more about the events\n",
    "    0:13 we have, there is a link in the\n",
    "    0:14 description. Um so click on that link,\n",
    "    0:16 check it out right now. We actually have\n",
    "    0:19 quite a few events in our pipeline, but\n",
    "    0:21 we need to put them on the website. Uh\n",
    "    0:24 but keep a\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74389539",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner()\n",
    "results = await runner.run(youtube_agent, input=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a895435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'agents.result.RunResult'>\n",
      "RunResult:\n",
      "- Last agent: Agent(name=\"youtube_agent\", ...)\n",
      "- Final output (str):\n",
      "    Here's a summary of the video, formatted as requested:\n",
      "    \n",
      "    ```\n",
      "    0:00 Hey everyone, welcome to our event. This\n",
      "    0:02 event is brought to you by data talks\n",
      "    0:03 club which is a community of people who\n",
      "    0:05 love data. We have weekly events today.\n",
      "    0:08 Uh this is one of such events. Um if you\n",
      "    0:11 want to find out more about the events\n",
      "    0:13 we have, there is a link in the\n",
      "    0:14 description. Um so click on that link,\n",
      "    0:16 check it out right now. We actually have\n",
      "    0:19 quite a few events in our pipeline, but\n",
      "    0:21 we need to put them on the website. Uh\n",
      "    0:24 but keep an eye on it anyways. Um so we\n",
      "    0:27 will put um them. you'll see them. And\n",
      "    0:30 then don't forget to subscribe to our\n",
      "    0:32 YouTube channel. So this is the most um\n",
      "    0:35 reliable way of getting notified when\n",
      "    0:37 our stream starts. And last but not\n",
      "    0:40 least, do not forget to join our um data\n",
      "    0:43 community where you can hang out with\n",
      "    0:45 data with other data enthusiasts. During\n",
      "    0:48 today's interview, you can ask any\n",
      "    0:49 question you want. There is a pinned\n",
      "    0:51 link in the live chat. Click on that\n",
      "    0:53 link, ask your questions and we'll be\n",
      "    0:56 covering your questions during the\n",
      "    0:58 interview. So that's the usual\n",
      "    1:01 introduction I do.\n",
      "    1:04 Also, I'm a bit sleepy. Um, but I hope\n",
      "    1:07 it goes well.\n",
      "    1:10 So I guess if you're ready, I have the\n",
      "    1:13 questions prepared\n",
      "    1:16 in front of me.\n",
      "    1:17 Yeah,\n",
      "    1:18 if you're ready, we can start.\n",
      "    1:20 Yeah, sounds good. Ready? Um, today on\n",
      "    1:24 the podcast we are joined by Aishwara.\n",
      "    1:26 Do you pronounce your name correctly?\n",
      "    1:28 Yes, that is right.\n",
      "    1:29 Yeah, good. A machine learning engineer\n",
      "    1:32 at Vimo, formerly part of Tesla's\n",
      "    1:34 autopilot AI team and a Cambridge Melon\n",
      "    1:36 University Alumni. Aishwara has worked\n",
      "    1:39 across some of the toughest applied AI\n",
      "    1:42 problems. Financial recommendation\n",
      "    1:44 systems at Morgan Stanley, multimodel\n",
      "    1:46 research at CMU, perception and video\n",
      "    1:49 understanding and Tesla and now gesture\n",
      "    1:51 and pedestrial semantics at IMA. She has\n",
      "    1:54 also contributed to AI for social good\n",
      "    1:56 including a malaria mapping project in\n",
      "    1:58 Africa that achieved real world impact\n",
      "    2:00 at scale.\n",
      "    2:02 Welcome to this event.\n",
      "    2:06 Hi, thank you for having me. Yeah,\n",
      "    2:09 that's um quite a nice bio. Um\n",
      "    2:12 especially um I don't know probably\n",
      "    2:16 um what you do now uh at and Tesla\n",
      "    2:19 is challenging but for me also Morgan\n",
      "    2:22 Stanley sounds very challenging cuz um I\n",
      "    2:26 worked a little near high frequency\n",
      "    2:29 trading. So I wasn't actually working on\n",
      "    2:32 the system that we were doing tra uh\n",
      "    2:33 high frequency trading but we were doing\n",
      "    2:36 some um\n",
      "    2:38 how to say analytics on top of this\n",
      "    2:40 data. It was huge. So probably quite\n",
      "    2:44 challenging. Um so can you tell us uh I\n",
      "    2:48 just outlined your uh career journey but\n",
      "    2:51 can you tell us more about this? Um\n",
      "    ...\n",
      "    ```\n",
      "    \n",
      "    The video features a discussion with Aishwara, a machine learning engineer, about her career in applied AI, including her experiences at Tesla, Morgan Stanley, and her work on AI for social good projects. They cover a variety of topics such as self-driving technology, machine learning techniques, and the importance of data in AI development.\n",
      "    \n",
      "    If you want more specific details from the video, let me know!\n",
      "- 3 new item(s)\n",
      "- 2 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n"
     ]
    }
   ],
   "source": [
    "print(type(results))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc286e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361fdf6-62d9-4efb-b636-992cf001f3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c2d3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIAgentsSDKRunner\n",
    "\n",
    "chat_interface = IPythonChatInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf7c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5bf2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = OpenAIAgentsSDKRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=youtube_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1e607e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What is this video about https://www.youtube.com/watch?v=vK_SxyqIfwk$t=172s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>fetch_youtube_transcript({\"video_id\":\"vK_SxyqIfwk\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"video_id\":\"vK_SxyqIfwk\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>0:00 Hey everyone, welcome to our event. This\n",
       "0:02 event is brought to you by data talks\n",
       "0:03 club which is a community of people who\n",
       "0:05 love data. We have weekly events today.\n",
       "0:08 Uh this is one of such events. Um if you\n",
       "0:11 want to find out more about the events\n",
       "0:13 we have, there is a link in the\n",
       "0:14 description. Um so click on that link,\n",
       "0:16 check it out right now. We actually have\n",
       "0:19 quite a few events in our pipeline, but\n",
       "0:21 we need to put them on the website. Uh\n",
       "0:24 but keep an eye on it anyways. Um so we\n",
       "0:27 will put um them. you'll see them. And\n",
       "0:30 then don't forget to subscribe to our\n",
       "0:32 YouTube channel. So this is the most um\n",
       "0:35 reliable way of getting notified when\n",
       "0:37 our stream starts. And last but not\n",
       "0:40 least, do not forget to join our um data\n",
       "0:43 community where you can hang out with\n",
       "0:45 data with other data enthusiasts. During\n",
       "0:48 today's interview, you can ask any\n",
       "0:49 question you want. There is a pinned\n",
       "0:51 link in the live chat. Click on that\n",
       "0:53 link, ask your questions and we'll be\n",
       "0:56 covering your questions during the\n",
       "0:58 interview. So that's the usual\n",
       "1:01 introduction I do.\n",
       "1:04 Also, I'm a bit sleepy. Um, but I hope\n",
       "1:07 it goes well.\n",
       "1:10 So I guess if you're ready, I have the\n",
       "1:13 questions prepared\n",
       "1:16 in front of me.\n",
       "1:17 >> Yeah,\n",
       "1:18 >> if you're ready, we can start.\n",
       "1:20 >> Yeah, sounds good. Ready? Um, today on\n",
       "1:24 the podcast we are joined by Aishwara.\n",
       "1:26 Do you pronounce your name correctly?\n",
       "1:28 >> Yes, that is right.\n",
       "1:29 >> Yeah, good. A machine learning engineer\n",
       "1:32 at Vimo, formerly part of Tesla's\n",
       "1:34 autopilot AI team and a Cambridge Melon\n",
       "1:36 University Alumni. Aishwara has worked\n",
       "1:39 across some of the toughest applied AI\n",
       "1:42 problems. Financial recommendation\n",
       "1:44 systems at Morgan Stanley, multimodel\n",
       "1:46 research at CMU, perception and video\n",
       "1:49 understanding and Tesla and now gesture\n",
       "1:51 and pedestrial semantics at IMA. She has\n",
       "1:54 also contributed to AI for social good\n",
       "1:56 including a malaria mapping project in\n",
       "1:58 Africa that achieved real world impact\n",
       "2:00 at scale.\n",
       "2:02 Welcome to this event.\n",
       "2:06 >> Hi, thank you for having me. Yeah,\n",
       "2:09 that's um quite a nice bio. Um\n",
       "2:12 especially um I don't know probably\n",
       "2:16 um what you do now uh at and Tesla\n",
       "2:19 is challenging but for me also Morgan\n",
       "2:22 Stanley sounds very challenging cuz um I\n",
       "2:26 worked a little near high frequency\n",
       "2:29 trading. So I wasn't actually working on\n",
       "2:32 the system that we were doing tra uh\n",
       "2:33 high frequency trading but we were doing\n",
       "2:36 some um\n",
       "2:38 how to say analytics on top of this\n",
       "2:40 data. It was huge. So probably quite\n",
       "2:44 challenging. Um so can you tell us uh I\n",
       "2:48 just outlined your uh career journey but\n",
       "2:51 can you tell us more about this? Um\n",
       "2:55 >> yeah uh so I I think uh like you\n",
       "2:58 mentioned at Morgan Stanley it was a lot\n",
       "3:01 of data. So I was a big data engineer at\n",
       "3:03 Morgan Stanley and I basically did that\n",
       "3:06 handle all this huge amounts of data and\n",
       "3:10 I was there when Morgan Stanley you know\n",
       "3:12 they were doing this acquisition of\n",
       "3:14 Erade. So we had like a lot more data\n",
       "3:16 coming in. So my role there was you know\n",
       "3:19 handling all this data. How do we\n",
       "3:21 connect the different dots? how do we\n",
       "3:22 analyze them together? And from there I\n",
       "3:26 uh like realized that you know there's\n",
       "3:29 so much of data that it has so much of\n",
       "3:30 value that we don't need to do all the\n",
       "3:33 things that we do manually and that was\n",
       "3:35 back I guess in 2018 and the whole AI\n",
       "3:39 domain the AI bubble hadn't like formed\n",
       "3:41 yet but you know it was getting there\n",
       "3:43 people were realizing how important it\n",
       "3:45 was and uh finance was one of the last\n",
       "3:48 fields to take on uh the machine\n",
       "3:50 learning aspect the AI aspect so we were\n",
       "3:53 onboarding systems And that's when I\n",
       "3:55 decided to like you know get a hand at\n",
       "3:58 it and like try some of like begin with\n",
       "4:00 some of the smaller systems like you\n",
       "4:02 know recommendation engines and uh stuff\n",
       "4:05 that was already well known in the uh AI\n",
       "4:09 domain. So I started off as that and\n",
       "4:12 then we decided to get a bit more uh I\n",
       "4:16 guess researchy with that. We tried out\n",
       "4:18 like you know graph neural networks\n",
       "4:20 which were some of the more complicated\n",
       "4:22 topics at that point and that's when I\n",
       "4:24 realized that oh it's you know there's\n",
       "4:26 so much to be learned here and there's\n",
       "4:28 so many this this field is so vast so\n",
       "4:30 that's when I decided to like uh pursue\n",
       "4:33 a masters\n",
       "4:34 >> and I like decided to join Kangi Melon\n",
       "4:37 University and my program it was like\n",
       "4:41 sorry it was a mix of data science and\n",
       "4:44 uh machine learning so I had like I\n",
       "4:46 could draw upon both my experiences and\n",
       "4:48 what I wanted to learn and where I\n",
       "4:50 wanted to get at and dur during CMU I\n",
       "4:53 was more like uh inclined towards\n",
       "4:56 projects that involved a lot of computer\n",
       "4:58 vision. So I was involved in this\n",
       "5:00 project project for like navigational\n",
       "5:03 app for blind people. Uh so it's called\n",
       "5:06 like AI guide dog. So it it like takes\n",
       "5:09 in the world and navigates uh people\n",
       "5:11 without vision. And from there, you\n",
       "5:14 know, I got into Tesla because it's also\n",
       "5:16 similar like computer vision domain and\n",
       "5:18 navigation related stuff. Uh and that's\n",
       "5:21 where my self-driving journey began. And\n",
       "5:23 then from Tesla to Wayob, it's like a\n",
       "5:26 similar domain but uh different kinds of\n",
       "5:28 products. Uh like uh some different like\n",
       "5:31 differences are there. Uh and yeah, uh\n",
       "5:34 that's where I am at Whimo right now. um\n",
       "5:37 in the self-driving domain started from\n",
       "5:39 finance uh reached in a different domain\n",
       "5:42 >> but yeah\n",
       "5:43 >> that's that's an interesting journey uh\n",
       "5:45 this app that you developed for blind\n",
       "5:48 people\n",
       "5:49 um can you tell us more about it like\n",
       "5:52 how does it work uh is it uh like they\n",
       "5:55 hold their phone and then it tells uh\n",
       "5:57 where to go or describes things\n",
       "6:01 >> yeah sorry\n",
       "6:03 so yeah for this uh app like the goal\n",
       "6:06 was that you know uh people without\n",
       "6:08 vision they should be able to navigate\n",
       "6:10 the world just as cited people do. So\n",
       "6:13 this app is basically their eyes. So it\n",
       "6:16 you just like hang it around your neck\n",
       "6:18 and you walk with it and it gets a world\n",
       "6:21 view of what is in front of you and then\n",
       "6:24 uh via like live audio instructions. It\n",
       "6:27 tells you like you know keep walking\n",
       "6:29 straight and uh if you've entered a\n",
       "6:31 destination based on that you know take\n",
       "6:33 left or right or stop at a traffic\n",
       "6:35 signal or there's like you know\n",
       "6:36 pedestrian crossings so it gives you\n",
       "6:39 instructions via audio.\n",
       "6:41 >> Interesting. And is it something you did\n",
       "6:44 just as a pet project? Was it a part of\n",
       "6:46 a company? Was it AI for social good\n",
       "6:49 project? How did you\n",
       "6:52 um get involved?\n",
       "6:53 >> Yeah. So uh so my program has this uh\n",
       "6:57 thing called the capstone project. So\n",
       "6:58 every year like you either pair up with\n",
       "7:01 a professor or someone from the industry\n",
       "7:03 who already has a very interesting\n",
       "7:05 project and then you work on it. So this\n",
       "7:08 project was from an alumni uh of CMU and\n",
       "7:11 he currently works at Pinterest. Uh but\n",
       "7:14 he started this whole project and uh\n",
       "7:16 this was like the third iteration where\n",
       "7:18 you know he worked with two groups of\n",
       "7:19 folks before and then uh my team on\n",
       "7:22 boarded on it. Mhm. Okay. But was it um\n",
       "7:26 like a community AI social good project\n",
       "7:29 or was it a company?\n",
       "7:32 >> Um I would say it was like more of a\n",
       "7:35 volunteer project. So uh it's it's just\n",
       "7:39 like they every year a group of\n",
       "7:41 volunteers from CMU work on it and then\n",
       "7:45 we make some progress and then we pass\n",
       "7:46 it on to the next group.\n",
       "7:48 >> That's a very interesting concept.\n",
       "7:51 >> Yeah. So before me like two batches of\n",
       "7:53 people had worked on it. So they were\n",
       "7:55 like mentors to us and then when I got\n",
       "7:57 done I mentored another batch that came\n",
       "8:00 after me. So this is like in its fifth\n",
       "8:02 iteration now like there have been two\n",
       "8:04 batches after me and they are making\n",
       "8:06 more and more progress on this print.\n",
       "8:09 That's really\n",
       "8:12 that's a really nice idea because well\n",
       "8:15 we have courses at data talks club and\n",
       "8:17 for me I immediately uh started thinking\n",
       "8:21 how can I implement something like this\n",
       "8:22 cuz it sounds so amazing and with the\n",
       "8:25 community we have uh I see that the\n",
       "8:28 people from the uh previous iterations\n",
       "8:30 they have people who are currently doing\n",
       "8:32 courses. So having a project like that\n",
       "8:35 for them to actually um sharpen the\n",
       "8:38 skills they picked up during the courses\n",
       "8:40 that's a really good idea.\n",
       "8:42 >> Yeah. And it doesn't need to be like you\n",
       "8:44 know done like it's a big thing right?\n",
       "8:46 You need there's so many moving\n",
       "8:48 components. It can't be built in one\n",
       "8:50 year by a given cohort or even in 6\n",
       "8:52 months by a given cohort. So you like\n",
       "8:55 pick up small pieces like the first one\n",
       "8:57 started with like the data efforts. The\n",
       "8:59 second one started building like\n",
       "9:00 baseline models. the third one like\n",
       "9:02 tried improving it with evals and stuff.\n",
       "9:05 So you do it iteratively.\n",
       "9:07 >> So it's a good idea. Yeah.\n",
       "9:09 >> Do do you know if this app is accessible\n",
       "9:11 uh in app store?\n",
       "9:14 >> Uh not yet. So the the next community\n",
       "9:17 like the new batch of students are going\n",
       "9:19 to be working on the app. We have the\n",
       "9:21 model but it's still in beta phase. Uh\n",
       "9:24 we are doing a lot of testing because\n",
       "9:26 it's kind of like a sensitive use case\n",
       "9:28 and uh yeah.\n",
       "9:29 >> Yeah. Yeah. You can imagine I recently\n",
       "9:32 uh participated in a marathon. People\n",
       "9:35 who know me, they would roll their eyes\n",
       "9:36 cuz I wouldn't shut up. Of course, I've\n",
       "9:38 been preparing for so long. Uh but yeah,\n",
       "9:41 so the reason I thought about this uh\n",
       "9:44 blind people also or people with um\n",
       "9:48 how to say um with vision problems and\n",
       "9:52 also completely blind who don't see at\n",
       "9:55 all. They took part in this but they\n",
       "9:57 were running with a guide. So there was\n",
       "9:58 a person who was running with them and\n",
       "10:00 they were holding their hands uh and\n",
       "10:03 running together\n",
       "10:04 >> and I thought like it's so amazing to\n",
       "10:08 include them too like cuz they also want\n",
       "10:10 to be a part of this event but cuz they\n",
       "10:12 cannot see it's very difficult but what\n",
       "10:16 they did uh they allowed um they called\n",
       "10:20 them guides to also join and be the\n",
       "10:23 leads and this is so amazing and I guess\n",
       "10:26 well maybe with this app they wouldn't\n",
       "10:28 be able to run an event, a race. Uh but\n",
       "10:33 uh yeah, this is one step closer to\n",
       "10:37 this, right?\n",
       "10:38 >> Yeah, we that that's the hope like you\n",
       "10:40 know, they don't need like uh a person\n",
       "10:43 or some rely on someone. They can just\n",
       "10:45 have their app and that can be their\n",
       "10:47 guide for the world. Yeah. Mhm. And also\n",
       "10:50 with this um VR glasses um you probably\n",
       "10:55 heard like I think Meta has them, some\n",
       "10:57 other companies have them. So you put\n",
       "10:59 them in your eyes and they uh have\n",
       "11:02 cameras, right? And the cameras they\n",
       "11:05 have broader,\n",
       "11:08 how to say vision than mobile phones.\n",
       "11:12 >> Yeah. Yes.\n",
       "11:12 >> Right. So maybe\n",
       "11:15 >> Yeah. Maybe this is something that um\n",
       "11:18 future alumni can work on, right?\n",
       "11:22 Or like\n",
       "11:27 >> cars, right? Not necessarily cameras.\n",
       "11:30 >> Yeah, that's like, you know, those are\n",
       "11:32 the things that we're trying to work\n",
       "11:33 around because it needs to be cost\n",
       "11:35 efficient and you know, we can't afford\n",
       "11:36 to put like lighters and stuff. So, a\n",
       "11:38 mobile phone, everyone has it like in\n",
       "11:41 today's world. So, we're trying to fit\n",
       "11:42 it on what everyone has. How expensive\n",
       "11:46 is uh uh lit lighter?\n",
       "11:50 >> I think it depends on the quality like\n",
       "11:52 uh you can get from somewhere like\n",
       "11:55 really cheap to extremely high-end. Uh\n",
       "11:58 so yeah, I guess the\n",
       "12:00 >> pronounce it lighter. LAR\n",
       "12:03 >> it's LAR. Yeah,\n",
       "12:04 >> LAR. So this is I know radar. So radar\n",
       "12:08 emits uh uh radio frequencies and one\n",
       "12:12 waits back to the frequency to come back\n",
       "12:14 to the wave to come back right and based\n",
       "12:17 on that uh the the radar can estimate if\n",
       "12:21 there's something\n",
       "12:24 uh and if it's moving\n",
       "12:26 >> uh and so on. Um whales do this right?\n",
       "12:29 So they make sound\n",
       "12:31 >> or I don't know maybe not whales but\n",
       "12:33 some other\n",
       "12:35 >> bats.\n",
       "12:37 Yeah. Yeah.\n",
       "12:39 >> And the same uh uh the same thing u\n",
       "12:42 lighter has um kind of similar idea but\n",
       "12:46 it's instead of um a radio wave it uses\n",
       "12:50 lasers, right?\n",
       "12:52 >> Uh it it's light. Yes, that's right.\n",
       "12:55 Like light rays. Yes. Okay.\n",
       "12:57 >> That's why the lighter thing. Yeah.\n",
       "12:59 >> Okay. I thought it's laser there. Okay.\n",
       "13:01 >> Uh similar like I I think it's one of\n",
       "13:03 the light frequencies.\n",
       "13:05 >> Okay. Okay. And um\n",
       "13:08 I don't know if you can disclose or talk\n",
       "13:11 about things you do at work, but um I\n",
       "13:13 know that these things they are used\n",
       "13:16 often for cars, right? For self-driving.\n",
       "13:21 >> Yeah. Uh I think it uh like depends on\n",
       "13:23 the stack. Uh some companies they do use\n",
       "13:28 like uh most of the like uh fully\n",
       "13:31 self-driving where there is like\n",
       "13:33 absolutely no driver use it. Uh whereas\n",
       "13:36 if you see some of the Tesla systems,\n",
       "13:38 they do not use it at all. They rely\n",
       "13:40 solely on the cameras. Yeah.\n",
       "13:41 >> Mhm. So Tesla I um took a few times a\n",
       "13:46 taxi which turned out to be Tesla and\n",
       "13:49 for me it was so fun to watch um they\n",
       "13:52 have this uh screen and as the car\n",
       "13:55 drives they start showing uh like cars\n",
       "13:58 and people and uh stuff like around the\n",
       "14:01 car, right? And uh for me it was always\n",
       "14:04 curious to see like when it makes\n",
       "14:05 mistakes or when it doesn't make\n",
       "14:07 mistakes. Um well but for me it was kind\n",
       "14:10 of fun to watch. So the ride was more\n",
       "14:12 entertaining than a usual I don't know\n",
       "14:15 um Toyota right cuz I could look at the\n",
       "14:20 at the screen.\n",
       "14:22 >> Um so this thing works with a camera\n",
       "14:24 right\n",
       "14:27 here. Sorry, I've uh I think I came down\n",
       "14:30 with a flu in in the evening. Um yeah,\n",
       "14:34 like I have cold. U might take like\n",
       "14:36 brief.\n",
       "14:36 >> Hope you recover very quickly.\n",
       "14:40 >> Yeah. Um sorry. Uh like could you please\n",
       "14:43 quickly repeat the question for me?\n",
       "14:45 >> Yeah. Yeah. Uh so I was uh talking about\n",
       "14:47 uh these Tesla cars, right? And in Tesla\n",
       "14:50 you there's a screen that shows that\n",
       "14:54 detects people, cars, bikes or whatnot.\n",
       "14:57 Yeah.\n",
       "14:59 >> Do you know how this thing works? Like\n",
       "15:02 does it use just video cameras?\n",
       "15:05 >> Yeah.\n",
       "15:07 >> Uh that's like the USB of Tesla systems\n",
       "15:10 that you know they uh because LAR is\n",
       "15:12 expensive or at least the good quality\n",
       "15:14 ones and uh they want to be like really\n",
       "15:17 scalable. They rely on just cameras. But\n",
       "15:20 it's not just one camera. There's like\n",
       "15:23 uh you know cameras all around the car.\n",
       "15:25 So you get a view from all around like a\n",
       "15:28 360 view of the system and uh you kind\n",
       "15:31 of like run your models on top of it\n",
       "15:33 that can make use of these different\n",
       "15:35 views of the world and see basically all\n",
       "15:38 around the car. So basically the the the\n",
       "15:41 car has a much better holistic view of\n",
       "15:44 the world than a person like a p person\n",
       "15:47 driving the car. Yeah.\n",
       "15:49 >> Yeah. Cuz we cannot have cameras uh on\n",
       "15:51 our back right on our sides.\n",
       "15:54 >> Yep. That's yeah.\n",
       "15:56 >> So yeah, I think that's the goal with\n",
       "15:58 self-driving is to make like driving\n",
       "16:00 safer once it accomplishes that level\n",
       "16:04 of, you know, the the AI reaches that\n",
       "16:06 level. Yeah.\n",
       "16:07 >> Mhm. But what is this screen for? Like\n",
       "16:10 this screen is for self-driving, right?\n",
       "16:12 But in these cases when I took a taxi,\n",
       "16:14 it was Uber or some other ride hailing\n",
       "16:18 service and they were actually driving.\n",
       "16:22 The drivers were driving, not the car.\n",
       "16:24 So what's the point of this thing?\n",
       "16:27 >> I think it's more about like at this\n",
       "16:29 point uh so there's two purposes like\n",
       "16:31 you know in like long drives or if the\n",
       "16:33 stop traffic is like stop and go you can\n",
       "16:36 just put it in the autopilot mode and\n",
       "16:37 you don't need to like be constantly on\n",
       "16:40 the wheel and constantly like super\n",
       "16:42 alert and then the car tricks you. Like\n",
       "16:44 I remember 2 years ago I took a trip to\n",
       "16:47 like Vegas. It was like a 13-hour drive\n",
       "16:50 one way and then 13 hours back and I was\n",
       "16:52 the only person driving and like the car\n",
       "16:55 aided me all the way. Like it drove 95%\n",
       "16:57 of the time and I was like just there\n",
       "17:00 and it was so much better because you\n",
       "17:02 know I couldn't have made it without\n",
       "17:03 that like 12 hours is too much.\n",
       "17:05 >> So it's like an assistant system and\n",
       "17:09 sometimes like people just drive because\n",
       "17:11 they don't trust it. So it's like okay\n",
       "17:14 better me driving than the car doing\n",
       "17:16 something wrong. So it's also about the\n",
       "17:18 trust factor\n",
       "17:18 >> if there is any statistics about that.\n",
       "17:22 Um there is statistics about like uh\n",
       "17:26 failure. People people are saying that\n",
       "17:28 it's better me driving than AI, right?\n",
       "17:30 Or whatever self-driving thing, but like\n",
       "17:33 the question is who actually drives\n",
       "17:35 better\n",
       "17:37 >> because people might be overconfident in\n",
       "17:39 their driving skills, right? But for\n",
       "17:41 some simple cases like you said when you\n",
       "17:44 need to go to Vegas, I don't know what\n",
       "17:47 kind of road is there, but maybe it's\n",
       "17:49 like most of the time it's just\n",
       "17:51 straight, right?\n",
       "17:52 >> Yeah. I I think like uh it's a highway\n",
       "17:55 so it's not like you know there's a lot\n",
       "17:58 of traffic lights and stuff. You just go\n",
       "18:00 straight and you go along the route and\n",
       "18:02 it's like uh there's a there's a patch\n",
       "18:04 which is like just 150 mi straight and\n",
       "18:06 like a normal person would just be so\n",
       "18:08 bored out of their mind if you have to\n",
       "18:10 drive that much.\n",
       "18:13 So, in Berlin, uh it's probably\n",
       "18:15 difficult because there are bikes, uh\n",
       "18:18 the streets are quite narrow, um like\n",
       "18:22 crazy, uh bicycle riders who can jump on\n",
       "18:26 you out of nowhere and um like other\n",
       "18:30 things. I guess that's why they don't\n",
       "18:32 use self.\n",
       "18:34 >> I think Tesla is trying to get into the\n",
       "18:36 European markets though with its\n",
       "18:38 autopilot. Like at least when I was\n",
       "18:40 trying to like when I was there I was\n",
       "18:42 working on some European road signs like\n",
       "18:45 speed limit signs. So yeah.\n",
       "18:48 >> Yeah. But also it's regulated, right?\n",
       "18:50 You're right. So maybe they cannot use a\n",
       "18:53 self-driving yet.\n",
       "18:55 >> Not yet. Yes. Yeah. Yeah.\n",
       "18:56 >> I see. I see. Makes sense because I am\n",
       "18:59 originally from Russia and I know in\n",
       "19:01 Russia in Moscow some cars already drive\n",
       "19:04 u without drivers. I guess in San\n",
       "19:07 Francisco too, right?\n",
       "19:09 >> Yes. Yes. SF has SF also has way more\n",
       "19:13 which has like no driver at all. So I\n",
       "19:16 guess very people people are more\n",
       "19:18 interesting.\n",
       "19:19 >> So you get something like Uber and then\n",
       "19:22 a car comes and there is no driver,\n",
       "19:24 right?\n",
       "19:25 >> Yes. There's no one there.\n",
       "19:28 >> If you ever visit if you visit SF next\n",
       "19:31 time, like be sure to take away no. It's\n",
       "19:34 it's quite the tourist attraction right\n",
       "19:36 now over here.\n",
       "19:37 >> Okay. Okay. Ah, that's where you work,\n",
       "19:39 right?\n",
       "19:40 >> Yeah. Yeah.\n",
       "19:41 >> Okay. That's why um is there an app\n",
       "19:44 called Whimo, right?\n",
       "19:46 >> Yes, there's an app called Whimo through\n",
       "19:48 which you can hail it and I think in\n",
       "19:49 some cities they've also partnered with\n",
       "19:51 Uber and Lyft so that you can call via\n",
       "19:54 Uber as well.\n",
       "19:57 >> Yeah. How much about your current uh\n",
       "20:00 position can you talk cuz you mentioned\n",
       "20:02 that uh or we at least know that you\n",
       "20:04 work on gesture recognition right so can\n",
       "20:08 you tell us more about that and how much\n",
       "20:10 I don't know but I would be pretty\n",
       "20:12 pretty curious to know\n",
       "20:15 >> yeah I I think I can like give a high\n",
       "20:17 level picture it's basically about\n",
       "20:19 trying to understand uh you know if\n",
       "20:21 there's like a police officer or a\n",
       "20:24 construction worker trying to guide\n",
       "20:25 traffic like uh you know there's like a\n",
       "20:28 big event or\n",
       "20:29 >> so there is a police officer slow down,\n",
       "20:31 right?\n",
       "20:32 >> Yeah. No, like they they tell you to\n",
       "20:35 stop or they tell you to go or they tell\n",
       "20:38 you to like Yeah. So yeah, that's the\n",
       "20:41 humans like slow down when there's a\n",
       "20:42 police officer.\n",
       "20:44 >> That's the human brain. Uh the car tries\n",
       "20:47 to stay in the traffic rules.\n",
       "20:50 uh but uh yeah I basically try to\n",
       "20:52 understand what they want to say and\n",
       "20:54 communicate with the car and like try to\n",
       "20:57 uh you know modify its root or modify\n",
       "21:00 its behavior according to what what\n",
       "21:02 they're communicating.\n",
       "21:04 So\n",
       "21:05 >> I guess in your case in case of my mom\n",
       "21:07 if you say that there is a right hailing\n",
       "21:10 service the car comes without the driver\n",
       "21:13 and then all of a sudden there is an\n",
       "21:15 event like um I don't know Friday or\n",
       "21:19 whatever with a lot of people like what\n",
       "21:22 the car should do right so cuz in\n",
       "21:24 training data I guess it's less common\n",
       "21:26 to have things like that or like a\n",
       "21:29 traffic light uh breaks\n",
       "21:32 >> and there is a police officer are\n",
       "21:34 controlled in the traffic.\n",
       "21:36 >> Yeah, I think all of these cases are\n",
       "21:38 covered like Whimo has been in business\n",
       "21:40 since like I think 15 years I would say\n",
       "21:43 like they've been working on the product\n",
       "21:45 and try to cover many of the cases that\n",
       "21:49 we have. So you know the broken traffic\n",
       "21:51 light uh large crowds of people uh it's\n",
       "21:56 it's pretty pretty good around it. you\n",
       "21:58 know there are sometimes events and game\n",
       "22:00 nights and it actually does really well\n",
       "22:02 over there. So uh and also like you know\n",
       "22:05 during these cases there are these\n",
       "22:07 police officers controlling the traffic\n",
       "22:09 and like directing traffic and my job is\n",
       "22:12 to make the way more better at\n",
       "22:14 understanding\n",
       "22:17 >> how much can you talk about this project\n",
       "22:19 because I wonder how does it actually\n",
       "22:20 work like what kind of tech do you use\n",
       "22:24 it must be something super fast right?\n",
       "22:26 Uh\n",
       "22:27 >> yeah,\n",
       "22:28 >> so I know you you cannot go into into\n",
       "22:30 details. Uh but I guess it should be\n",
       "22:33 something super fast cuz like you need\n",
       "22:35 to make this decision in real life in\n",
       "22:37 real time, right? So something like I\n",
       "22:40 don't know yellow or something. Um yeah\n",
       "22:43 there are like a bunch of in-house\n",
       "22:44 models that use like you know uh cameras\n",
       "22:47 LA data and like all all the sensor\n",
       "22:50 information that we have or we get from\n",
       "22:52 the car and uh it's like uh way more\n",
       "22:56 does not publish its models what it\n",
       "22:59 uses. So uh these are in internal model\n",
       "23:02 that uh you know they are optimized to\n",
       "23:04 run on the car and they optimized to run\n",
       "23:07 really fast. So it's uh it's probably\n",
       "23:10 not the same neural network that was\n",
       "23:12 trained. It was like you know uh we use\n",
       "23:15 various techniques to make it much\n",
       "23:17 faster and run much faster on the car to\n",
       "23:19 like detect in very quick amounts of\n",
       "23:22 time and detect like multiple times a\n",
       "23:24 second uh about what's what's happening\n",
       "23:26 with the world. Yeah.\n",
       "23:28 >> How is this process called when you take\n",
       "23:30 a big model and make it smaller and\n",
       "23:33 faster?\n",
       "23:35 Um I I guess there's a bunch of ways to\n",
       "23:37 do it like you know publicly available\n",
       "23:40 ways are like something like you know\n",
       "23:41 you quantize the model.\n",
       "23:43 >> Yeah exactly this is this is the word I\n",
       "23:45 was looking for quantization.\n",
       "23:47 >> Quantization. Yes. So you make it like\n",
       "23:50 you make it smaller you make it faster\n",
       "23:52 with with quantization. So many of\n",
       "23:55 similar techniques but uh there are a\n",
       "23:57 whole bunch of other stuff that we also\n",
       "23:59 do uh internally.\n",
       "24:03 Yeah, I I guess like I I think that's\n",
       "24:05 the extent of it.\n",
       "24:09 >> I was just thinking not to torture you\n",
       "24:11 and put you in a uncomfortable position\n",
       "24:14 by asking more and I was thinking what\n",
       "24:16 should I ask next cuz you also did a few\n",
       "24:18 other interesting things and one of them\n",
       "24:22 which I find quite interesting is uh\n",
       "24:26 this malaria mapping project in Africa.\n",
       "24:29 Um can you tell us more about these\n",
       "24:31 projects?\n",
       "24:33 >> Yeah, I think this was when I was in\n",
       "24:35 Morgan Stanley and I was like oh this\n",
       "24:37 domain is really interesting and I want\n",
       "24:39 to do more projects about this. Uh so I\n",
       "24:42 I joined this organization called OMina\n",
       "24:44 and basically they work they have like\n",
       "24:46 AI for good projects where you have a\n",
       "24:50 like a nonprofit company that comes in\n",
       "24:52 with their uh problem and you have these\n",
       "24:55 volunteer ML engineers. Some of them are\n",
       "24:58 trying to learn, some of them are\n",
       "25:00 experts and they want to contribute to\n",
       "25:01 the community and they put you in like\n",
       "25:03 groups of 30 40 people who work together\n",
       "25:06 on this problem. So uh there was this\n",
       "25:10 nonprofit organization called Zap\n",
       "25:12 Malaria. They were trying to like uh\n",
       "25:14 lead efforts for fumigation in Africa.\n",
       "25:18 So you know they the the first iteration\n",
       "25:21 was to just like go in places and\n",
       "25:23 fumigate places uh where there could be\n",
       "25:26 high possibility of malaria mosquitoes\n",
       "25:28 so that to prevent like you know the the\n",
       "25:30 breeding of mosquitoes and the spread of\n",
       "25:32 malaria in those region. What they\n",
       "25:34 wanted us to do was to like determine\n",
       "25:36 you know make this process more\n",
       "25:38 efficient like uh they don't want to go\n",
       "25:40 to every region or every city in a given\n",
       "25:43 region and fumigate. they only want to\n",
       "25:45 target places where there's a high\n",
       "25:47 probability of mosquitoes. So they\n",
       "25:49 wanted to use machine learning models or\n",
       "25:51 AI to do that. So what what what we\n",
       "25:55 thought that okay if we have like\n",
       "25:56 satellite images or we have some\n",
       "25:58 knowledge of where could be like you\n",
       "26:00 know marshy lands or uh where areas\n",
       "26:03 where like you know water stagnates\n",
       "26:05 which is typically where mosquitoes have\n",
       "26:07 breeding grounds. we could just like\n",
       "26:09 identify these areas and these people\n",
       "26:12 could just go with their fumigation and\n",
       "26:14 just fumigate those areas and that would\n",
       "26:16 be much more efficient. It would save\n",
       "26:18 like so many manpower, so much cost and\n",
       "26:20 you know for these kind of nonprofits\n",
       "26:22 cost is a very big I guess\n",
       "26:24 consideration. So I guess with this team\n",
       "26:28 we worked in multiple groups. One team\n",
       "26:31 tried to use satellite images to detect\n",
       "26:33 like water bodies, stagnant water\n",
       "26:34 bodies. What my model did was it used\n",
       "26:37 topographic information. So we have this\n",
       "26:40 Google data gives you uh information\n",
       "26:42 about the geography of a particular\n",
       "26:44 region and then you try to basically\n",
       "26:47 train a model to identify what could be\n",
       "26:49 like low-lying areas where there's a\n",
       "26:51 high possibility of water stagnating and\n",
       "26:53 detect those areas and like you know let\n",
       "26:55 them know. So uh I think uh at the end\n",
       "26:59 like we used kind of an ensemble system\n",
       "27:01 that could use satellite images,\n",
       "27:02 topographic information and give you\n",
       "27:04 this data which they later made some\n",
       "27:06 changes and integrated into their whole\n",
       "27:08 model and I think their website also has\n",
       "27:11 some good very good results that they\n",
       "27:13 achieved with our models uh with this\n",
       "27:15 but yeah it was like a completely\n",
       "27:17 volunteerbased AI for good project.\n",
       "27:20 So you worked at Morgan Stanley and\n",
       "27:22 managed to also contribute to AI for\n",
       "27:26 social good project because I I don't\n",
       "27:28 know about Morgan Stanley but usually\n",
       "27:30 these financial institutions\n",
       "27:33 are not known for um\n",
       "27:36 um you know good work life balance.\n",
       "27:40 Yeah, I I think like it's um I was there\n",
       "27:44 for 2 years so I was already pretty\n",
       "27:47 efficient with all the systems and I\n",
       "27:49 knew where everything was. So I uh I was\n",
       "27:54 and I was interested in learning more\n",
       "27:55 because in financial institutions\n",
       "27:57 generally like you know these newer\n",
       "27:59 technologies like ML and all it's not\n",
       "28:02 easily adoptable. So it's it's really\n",
       "28:04 hard to get started a project. So I I I\n",
       "28:08 worked basically on weekends for this\n",
       "28:09 project that I had.\n",
       "28:11 >> Yeah, makes sense. Yeah, like I have\n",
       "28:14 PTSD now like when you talk about\n",
       "28:16 because I worked at a bank too. It was\n",
       "28:18 2012 so it was a while ago and it was so\n",
       "28:21 difficult to get even like a new\n",
       "28:23 database like to use.\n",
       "28:26 >> Mhm.\n",
       "28:26 >> Uh like let alone starting a new project\n",
       "28:29 that involves machine learning. No,\n",
       "28:31 forget about that.\n",
       "28:33 uh and we deploy it even uh once per\n",
       "28:36 month and if we didn't have a chance to\n",
       "28:39 prepare some things for the deployment\n",
       "28:43 uh cuz there is a process like you need\n",
       "28:44 to do some sort of audit I don't know I\n",
       "28:47 don't remember\n",
       "28:48 >> and if you miss this line then you wait\n",
       "28:51 for one month for the next cycle right\n",
       "28:53 and then only then you can see your work\n",
       "28:56 um actually being applied to real data\n",
       "29:00 and that was frustrating right Uh so\n",
       "29:03 well well I guess there are reasons why\n",
       "29:04 there is there are these bureaucratic\n",
       "29:06 processes right cuz like it's money.\n",
       "29:09 Yeah, I I think I 100% like agree and\n",
       "29:12 you know I can resonate with what you're\n",
       "29:14 saying. You know there are these like\n",
       "29:16 big processes that happen before every\n",
       "29:18 release and deployment that you must do\n",
       "29:20 every single time and it after a point\n",
       "29:23 it gets repetitive and it's like even\n",
       "29:25 for like putting the smallest thing in\n",
       "29:28 production you need to go through this\n",
       "29:29 and at some point it gets to you and\n",
       "29:32 you're like I I want to like do\n",
       "29:34 innovation. I want to like make impact.\n",
       "29:36 I don't want to do like get caught in\n",
       "29:38 processes and stuff.\n",
       "29:40 >> So yeah,\n",
       "29:41 >> which makes me wonder now how does it\n",
       "29:43 work with self-driving cars cuz it's\n",
       "29:45 also not a thing you can easily roll\n",
       "29:47 out, right? So first of all, there is\n",
       "29:49 this component that um like you cannot\n",
       "29:53 really afford having bucks there, right?\n",
       "29:55 cuz like if there is a buck and then a\n",
       "29:58 car misbehaves and I don't know hits\n",
       "30:00 traffic light uh and then like\n",
       "30:04 it's good if nobody gets injured but\n",
       "30:06 like what if somebody gets injured and\n",
       "30:08 it's super bad right? uh especially with\n",
       "30:12 um competitiveness like there are so\n",
       "30:14 many there are multiple\n",
       "30:17 um cars who like how to say companies\n",
       "30:20 that work on self-driving of course you\n",
       "30:23 want to be the best one right so I guess\n",
       "30:26 you have to be super careful when you\n",
       "30:28 work on things like that so how do you\n",
       "30:30 go about deployment in this case\n",
       "30:33 >> yeah I I think like there is again a lot\n",
       "30:37 a I would say multi-month process of you\n",
       "30:40 know deploying any I guess news release\n",
       "30:43 or software because like you mentioned\n",
       "30:46 it's a very sensitive and safety\n",
       "30:48 critical domain. So we want to be\n",
       "30:50 absolutely sure that it doesn't like\n",
       "30:52 negatively impact and doesn't have any\n",
       "30:54 bad behavior out in the open or out in\n",
       "30:57 the wild uh in the world. Mhm.\n",
       "30:58 >> So uh yes uh I think there are different\n",
       "31:01 components you know every time you push\n",
       "31:03 a change you do these whole bunch of\n",
       "31:05 evals you uh try to rerun uh from\n",
       "31:09 existing logs you rerun a lot of\n",
       "31:12 different evaluations and then you\n",
       "31:15 combine your changes with all other\n",
       "31:16 changes and then again there like for\n",
       "31:18 many months you have drivers driving in\n",
       "31:22 multiple areas testing out the software\n",
       "31:24 and like after you have accumulated\n",
       "31:27 enough data points joints that okay this\n",
       "31:29 is safe and this is good to go only then\n",
       "31:32 you deploy it. Uh but yeah the joint\n",
       "31:34 evals and you know uh the drivers\n",
       "31:37 actually driving around like we\n",
       "31:38 engineers are not involved so much with\n",
       "31:40 that. So that process has like it has a\n",
       "31:44 very different team and that is very\n",
       "31:45 well managed and they go through it.\n",
       "31:48 >> But even as engineers we\n",
       "31:50 >> Yeah. Yeah. There are there's tons of I\n",
       "31:52 would say there's lot more than\n",
       "31:53 financial company because\n",
       "31:55 >> you know in like in Morgan Stanley I've\n",
       "31:58 seen there's been losses there's been\n",
       "32:00 millions of dollars of losses but that\n",
       "32:01 you can recover but you know this this\n",
       "32:03 is like someone's life or someone's\n",
       "32:05 safety and it's much more important.\n",
       "32:09 >> Yeah that's interesting. Um, which\n",
       "32:12 reminds me of this uh case. Remember a\n",
       "32:14 few years back um there was a bug in\n",
       "32:17 Windows computers and all the airports\n",
       "32:19 had blue screen of death. Uh what was\n",
       "32:22 that? Crowd Strike, right? It was crowd\n",
       "32:24 strike.\n",
       "32:24 >> Yeah, Crowd Strike I think. Yes, Crowd\n",
       "32:27 Strike.\n",
       "32:27 >> Nobody knew about this thing that this\n",
       "32:29 thing existed until like I had no idea\n",
       "32:33 this company ever existed. uh they got\n",
       "32:37 provider\n",
       "32:38 >> lot of they got a lot of publicity out\n",
       "32:40 of that negative one but uh now we know\n",
       "32:44 what it is.\n",
       "32:45 >> Yeah. And uh so the story there was that\n",
       "32:48 they wrote uh something out and then it\n",
       "32:50 broke right.\n",
       "32:52 >> Yes. Yeah. I think the test thing they\n",
       "32:55 they missed some test cases and\n",
       "32:56 something very unexpected happened over\n",
       "32:59 there.\n",
       "32:59 >> Yeah. I don't know what happened though\n",
       "33:01 but it was a talk of the town I would\n",
       "33:03 say. Mhm. Yeah. But I guess what you\n",
       "33:06 work with is kind of similar in a sense\n",
       "33:09 that there is also the hardware\n",
       "33:11 component, right? So in Morgan Stanley,\n",
       "33:14 I don't know about you, but I assume it\n",
       "33:16 was probably just software, right? You\n",
       "33:17 didn't need to,\n",
       "33:18 >> of course, like you run on a computer.\n",
       "33:21 Uh but chances are that um you know, you\n",
       "33:25 don't run native C code. Most likely\n",
       "33:26 it's Java or Python or I don't know\n",
       "33:29 something like that.\n",
       "33:30 >> Yes. Yes.\n",
       "33:30 >> Um but like once you need to use uh\n",
       "33:34 something low level, something native\n",
       "33:36 and you need to be closer to hardware\n",
       "33:39 uh then things can get complicated,\n",
       "33:42 right? And uh in case of self-driving,\n",
       "33:44 you're very very closely connected with\n",
       "33:46 hardware cuz you actually need to I\n",
       "33:48 don't know steer the wheel or\n",
       "33:50 hit the brakes or whatever.\n",
       "33:53 Yeah, I guess uh definitely connected to\n",
       "33:56 hardware but for my role specifically I\n",
       "33:59 mostly work on the software that drives\n",
       "34:01 it and then there are different teams\n",
       "34:03 that work on the hardware aspect like\n",
       "34:06 you know or connection between the\n",
       "34:07 hardware and software where you do this\n",
       "34:09 whole like making the model faster and\n",
       "34:12 stuff\n",
       "34:13 >> but uh yes it like every time you push\n",
       "34:15 any new change there's this whole suite\n",
       "34:18 that needs to run and it's it's it's a\n",
       "34:20 very comprehensive very thorough ES Yes.\n",
       "34:22 Yes.\n",
       "34:24 >> Are you working on any um um side\n",
       "34:27 projects right now?\n",
       "34:29 >> Uh not right now. Uh what I'm doing\n",
       "34:32 right now is the AI guide dog thing that\n",
       "34:35 I was doing. I'm I'm still mentoring a\n",
       "34:37 group uh on that project but not like\n",
       "34:41 it's just like in a mentor capacity. Not\n",
       "34:43 like working on anything right now. I\n",
       "34:46 guess right now the work work aspect is\n",
       "34:48 a bit busy at this time of the year. you\n",
       "34:51 know it's before the November like when\n",
       "34:54 everything slows down uh we have a lot\n",
       "34:56 of work in September October months\n",
       "34:59 >> and then in December it's chill\n",
       "35:01 >> yes a bit chill during the holidays here\n",
       "35:05 >> okay\n",
       "35:06 >> I guess it's the same for most companies\n",
       "35:08 I think I had similar experiences in\n",
       "35:12 >> in Europe definitely\n",
       "35:14 >> yeah yeah\n",
       "35:15 >> I don't know about states but\n",
       "35:18 >> yeah I I think side projects are in like\n",
       "35:21 I I some sometimes start something in\n",
       "35:23 December and then it's it's good if I'm\n",
       "35:25 able to complete it otherwise I wait for\n",
       "35:28 the next December to take it off.\n",
       "35:30 >> But uh yes yeah I think mentoring is\n",
       "35:33 easier because you have uh people\n",
       "35:36 actually working on something and who\n",
       "35:38 have more time to work on it and you can\n",
       "35:39 help them in their process.\n",
       "35:41 >> Yeah. Yeah.\n",
       "35:42 >> Uh what I like about mentoring is you\n",
       "35:45 can test ideas without actually spending\n",
       "35:48 a lot of time implementing them.\n",
       "35:50 >> But you know the outcome, you learn the\n",
       "35:52 outcome from the person you mentor. So\n",
       "35:54 you can just say, \"Hey, you can try this\n",
       "35:56 and that.\" And then a few days later\n",
       "35:58 they come and say if it worked or not,\n",
       "36:00 right? And then you kind of you still\n",
       "36:02 learn from this experience too. Like you\n",
       "36:04 not only teach, you also learn. Um and\n",
       "36:07 then you can know okay\n",
       "36:09 this thing for this case uh maybe not is\n",
       "36:12 the best one so we can try something\n",
       "36:15 else then you also um get experience by\n",
       "36:19 mentoring\n",
       "36:20 >> for sure I I think that's how uh it's\n",
       "36:23 been working and even like you have if\n",
       "36:24 you have like a bigger team you can try\n",
       "36:26 out multiple ideas at a time and uh the\n",
       "36:29 people actually working on it they they\n",
       "36:31 joined that project because they like it\n",
       "36:33 and they are like super invested in it\n",
       "36:36 So it's nice to work along they're super\n",
       "36:38 passionate about it. So it's you trying\n",
       "36:41 out different ideas without actually you\n",
       "36:43 know spending a lot of time but you you\n",
       "36:46 also get to mentor people. So yeah, it's\n",
       "36:48 it's a mix of good both good good\n",
       "36:51 things. Yeah.\n",
       "36:53 >> Uh with all these new things coming up\n",
       "36:55 coming out um of AI LLM and whatnot. Um\n",
       "37:00 so since you were working in computer\n",
       "37:02 vision but all these new things they are\n",
       "37:04 mostly related to NLPs and text NLP and\n",
       "37:08 texts. Um, do you also find time to try\n",
       "37:11 to stay up to date with this field or\n",
       "37:15 you you're focusing more on what you do\n",
       "37:18 at work?\n",
       "37:20 >> Actually, I in CMU I was in the language\n",
       "37:23 technologies institute. So, I actually\n",
       "37:25 studied NLP and then transitioned to CV\n",
       "37:29 just before the whole Chad GPT boom.\n",
       "37:32 Yeah, I I I graduated in December 2022\n",
       "37:36 and then like I guess Chad GPT was\n",
       "37:38 around the same time and like few months\n",
       "37:40 later it was like super big and then the\n",
       "37:42 whole AI bubble with the LLM's like\n",
       "37:45 booming off. Uh but uh yes I do\n",
       "37:49 definitely stay ab breast with\n",
       "37:51 everything like even when I was\n",
       "37:53 switching my roles like from Tesla to I\n",
       "37:56 did a lot of different interviews and\n",
       "37:59 the way it works in the industry is like\n",
       "38:02 although you are from a CV background\n",
       "38:04 it's the same underlying fundamentals.\n",
       "38:06 So you know you still are eligible to\n",
       "38:09 interview for many of the LLM roles and\n",
       "38:11 I did interview with like a bunch of\n",
       "38:13 places where it was only NLP focused uh\n",
       "38:16 not much uh um CV but many times it's\n",
       "38:20 also a hybrid like multimodel uh large\n",
       "38:23 language models are very popular now and\n",
       "38:26 um many teams who I I I guess like they\n",
       "38:30 need CV expertise along with their LL\n",
       "38:33 inhouse like NLP folks so uh definitely\n",
       "38:36 like you know I read a lot of papers um\n",
       "38:40 that deal with this kind of multimodel\n",
       "38:42 LLM stuff or read a lot of blogs or uh\n",
       "38:46 even like you know the LLM techniques\n",
       "38:48 that they use it's it's good to have\n",
       "38:50 that knowledge because many of these\n",
       "38:52 techniques are general deep learning\n",
       "38:54 techniques rather than just that are\n",
       "38:56 applicable to LLMs. So you can use some\n",
       "39:00 of these techniques even for your models\n",
       "39:02 that work with CV data. So yeah at the\n",
       "39:05 end of the day it's all deep learning uh\n",
       "39:07 and it's is as much interesting to know\n",
       "39:11 about it and like it's as much\n",
       "39:12 >> useful.\n",
       "39:13 [Music]\n",
       "39:14 So I my masters was um in 20134\n",
       "39:21 uh it wasn't focusing on um even not on\n",
       "39:24 machine learning but I was taking a lot\n",
       "39:26 of extra courses uh in particular I was\n",
       "39:29 quite interested in LP at NLP but um NLP\n",
       "39:33 was quite different so it was\n",
       "39:35 pre-transformer era\n",
       "39:37 >> yes\n",
       "39:38 >> um so we were doing like um logistic\n",
       "39:41 regression this kind of stuff right to I\n",
       "39:43 don't know.\n",
       "39:45 >> Yeah. So like we if you want to identify\n",
       "39:48 part of speech\n",
       "39:50 >> and then there was I I don't even\n",
       "39:52 remember how these models were called.\n",
       "39:54 Uh but today you just throw everything\n",
       "39:56 at LM and it tells you okay here's a\n",
       "39:58 noun or there's names there are named\n",
       "40:01 entities. uh back then uh like you had\n",
       "40:04 to train models for that and then you\n",
       "40:07 had to have a corpus uh and then this uh\n",
       "40:10 has to be had to be labeled and then you\n",
       "40:13 say for this corpus like these are my\n",
       "40:15 named entities like and then you train a\n",
       "40:17 model. So these things are way easier\n",
       "40:20 today.\n",
       "40:21 >> Mhm.\n",
       "40:21 >> I wonder in your case when you studied\n",
       "40:23 NLP you already covered deep learning\n",
       "40:26 right? So for you when these LLMs came\n",
       "40:29 up like when they started to be big and\n",
       "40:32 charge BT like people started using it\n",
       "40:35 for you cuz you studied deep learning\n",
       "40:37 techniques right so for you it wasn't\n",
       "40:39 such a big leap right\n",
       "40:42 >> yeah I think uh I still remember like\n",
       "40:44 when I was in undergrad I did take a few\n",
       "40:47 ML courses and that time it was these\n",
       "40:49 very basic uh natural language\n",
       "40:52 processing techniques like part of\n",
       "40:53 speech and uh Identity correlation and\n",
       "40:56 bag of words and logistic regression\n",
       "40:58 like started off with that but then like\n",
       "41:01 during that time deep learning was also\n",
       "41:03 very popular. So uh in my masters there\n",
       "41:06 was a lot of focus on deep learning\n",
       "41:07 transformer based systems because that\n",
       "41:09 was already big at that point. But uh\n",
       "41:12 now like if you think about it, it's all\n",
       "41:14 based on the same fundamentals like you\n",
       "41:16 know your LLMs they're based on\n",
       "41:18 transformers which is based on like the\n",
       "41:20 attention mechanism and uh that boils\n",
       "41:23 down to your representation learning\n",
       "41:25 where you have like embeddings and the\n",
       "41:27 embeddings have similarity and like the\n",
       "41:30 whole core of LLM could be like you know\n",
       "41:33 said it's like anation learning kind of\n",
       "41:36 thing\n",
       "41:36 >> which uh which is like where the basics\n",
       "41:39 began which is where the fundament\n",
       "41:41 began. So for sure like I did do a lot\n",
       "41:44 of deep learning and that was really\n",
       "41:46 helpful when jumping to LLMs but uh I\n",
       "41:49 feel like uh with LLMs there's a lot of\n",
       "41:53 more creative or ingenuity in how the\n",
       "41:58 pre-training how they made it work or\n",
       "42:00 the RL stuff like I I am I have never\n",
       "42:04 taken an RL uh reinforcement learning\n",
       "42:06 course. I don't know what it is but uh I\n",
       "42:09 did read the papers and I it was hard to\n",
       "42:12 understand in the beginning but like uh\n",
       "42:16 if you tie it down to okay some basic\n",
       "42:18 principles and there's like so many good\n",
       "42:20 blogs explaining things uh about it it\n",
       "42:24 does take some effort uh but like you\n",
       "42:27 can just like try to piece together\n",
       "42:29 things uh\n",
       "42:31 >> but it does take some effort uh of\n",
       "42:33 course like if I had not learned about\n",
       "42:35 deep learning if I had not learned about\n",
       "42:37 this representation learning or\n",
       "42:38 embeddings, it would have been much more\n",
       "42:40 harder for me.\n",
       "42:43 >> Yeah, I I remember like I mentioned in\n",
       "42:45 Morgan Stand doing these basic things\n",
       "42:47 with the recommendation systems and like\n",
       "42:49 graph neural networks. It took me like\n",
       "42:51 almost two weeks to go through one paper\n",
       "42:53 and it was just like eight pages and at\n",
       "42:56 that time I did not have any background\n",
       "42:58 about deep learning. So I think having\n",
       "42:59 that background definitely makes it much\n",
       "43:02 easier to grasp these concepts.\n",
       "43:04 >> Mhm. So now when you read papers for\n",
       "43:06 you, it doesn't take two weeks usually.\n",
       "43:10 >> No, no, it it's like probably an hour at\n",
       "43:12 max.\n",
       "43:14 >> So it's it's much more efficient now.\n",
       "43:16 >> For me, I think it would be like two\n",
       "43:18 weeks. If I needed to get into any\n",
       "43:21 modern uh papers like this uh I don't\n",
       "43:25 know if I go to archive and take any\n",
       "43:27 paper uh about an OP or computer vision.\n",
       "43:31 Yeah. Well, good that we have chat GPT\n",
       "43:33 so I can ask it to explain things.\n",
       "43:36 >> That is so good now. Like it was not\n",
       "43:38 there back when we were in school and\n",
       "43:40 college\n",
       "43:41 >> doing assignments. Where was strategy\n",
       "43:44 then? Interesting that you mentioned you\n",
       "43:48 didn't have any prior experience with\n",
       "43:49 reinforcement learning cuz I thought\n",
       "43:51 reinfor reinforcement learning is\n",
       "43:53 something that used quite often for um\n",
       "43:58 um driving too cuz for me uh before this\n",
       "44:02 whole um LLM space appeared. So for me\n",
       "44:06 AI was um so there was machine learning\n",
       "44:10 which was kind of part of AI but for me\n",
       "44:12 machine learning was machine learning\n",
       "44:13 never like AI uh but this reinforcement\n",
       "44:17 learning you can actually get an agent\n",
       "44:20 to do things in the environment\n",
       "44:23 and work there were companies I\n",
       "44:25 interviewed with one of them who were\n",
       "44:27 creating these environments for\n",
       "44:29 self-driving cars to be like a test bed\n",
       "44:31 or whatever like for testing uh so they\n",
       "44:35 have the environment with streets and\n",
       "44:36 what not. So they look very realistic\n",
       "44:39 and I don't know um it was in Germany so\n",
       "44:43 I guess like BMWs and Audi's and whatn\n",
       "44:46 not who like companies who also work at\n",
       "44:49 on self-driving here in Germany they\n",
       "44:51 could use this uh environment to test\n",
       "44:54 their cars right and the idea there was\n",
       "44:56 that they have this reinforcement\n",
       "44:59 learning framework where they can the\n",
       "45:02 car can just go wild and learn from like\n",
       "45:04 okay like if I uh hit a pedestrian and\n",
       "45:07 then there is a huge penalty and then it\n",
       "45:09 learns not to do this, right? Um, so for\n",
       "45:12 me it was interesting. It didn't work\n",
       "45:13 out so I didn't join the company and it\n",
       "45:16 was funny. It was like a company with\n",
       "45:19 four people in a basement and a lot of\n",
       "45:21 GPUs.\n",
       "45:22 >> Uh, so they needed a one and they\n",
       "45:24 thought, uh, yeah, I have a kit and\n",
       "45:26 doesn't sound very stable.\n",
       "45:30 >> Wow. Okay. Okay.\n",
       "45:32 >> Yeah. But it was a good thing.\n",
       "45:34 >> Yeah. Reinforcement learning is kind of\n",
       "45:37 interesting. I think I I my first\n",
       "45:39 interaction with reinforcement learning\n",
       "45:41 was like with all these robots like we\n",
       "45:43 had these robo wars in college where you\n",
       "45:45 build your robot and like you go to a\n",
       "45:47 tech festival and you like compete\n",
       "45:49 against each other\n",
       "45:51 >> and I I I think uh reinforcement\n",
       "45:53 learning is still a big part of\n",
       "45:55 robotics. Um for me personally like so\n",
       "45:59 far in my career I've like whatever\n",
       "46:01 career has been in computer vision or\n",
       "46:03 robotics so to speak it has mostly been\n",
       "46:05 on the computer vision side so\n",
       "46:07 perception side trying to understand the\n",
       "46:09 world and then I I believe like\n",
       "46:11 reinforcement learning comes into\n",
       "46:13 picture when you're trying to modify the\n",
       "46:14 behavior of the agent or trying to teach\n",
       "46:16 it how to uh like behave in the world it\n",
       "46:20 is in. Um so these are like two separate\n",
       "46:23 parts of the stack is one is you try to\n",
       "46:25 understand or make the agent understand\n",
       "46:27 the world which is where I work and then\n",
       "46:30 trying to make it behave a certain way\n",
       "46:31 which is I I I believe like\n",
       "46:33 reinforcement learning could be used. So\n",
       "46:35 yeah I never had to like even though I'm\n",
       "46:38 in the self-driving industry I never had\n",
       "46:39 to like work on the other part of the\n",
       "46:42 stack. I don't think I'll be very good\n",
       "46:43 at it. I've like I skipped all\n",
       "46:46 reinforcement learning courses in\n",
       "46:47 college because I just found it too\n",
       "46:50 hard. So yeah.\n",
       "46:52 >> Yeah. But I imagine also it's not uh you\n",
       "46:54 will not let a car go wild and learn uh\n",
       "46:58 the way to interact with pedestrians\n",
       "47:01 right outside\n",
       "47:03 >> uh to actually do reinforcement learning\n",
       "47:07 fun.\n",
       "47:07 >> Yeah. Yeah. That's for sure. Uh\n",
       "47:09 >> it's karmagon. Honest honestly, I don't\n",
       "47:12 know if we use reinforcement learning.\n",
       "47:14 I've never tried to find out\n",
       "47:16 >> because it looks like more like a fun uh\n",
       "47:19 project to do like when you have a\n",
       "47:22 emulator, right?\n",
       "47:24 >> Yeah.\n",
       "47:24 >> Uh but like in real life probably you\n",
       "47:27 still have there's actually a question\n",
       "47:28 from Ole. Ole is asking uh I think the\n",
       "47:32 question is about self-driving.\n",
       "47:34 The question is is this full AI or mix\n",
       "47:36 of rules and AI? So I am assuming Le is\n",
       "47:39 referring to uh to self-driving cuz I\n",
       "47:44 guess like full AI would be this\n",
       "47:45 reinforcement learning right when the\n",
       "47:47 car just learns to drive by itself. Uh\n",
       "47:50 but we still need to add some rules,\n",
       "47:52 right? Like what's um what's the current\n",
       "47:55 state of the art in AI or in\n",
       "47:56 self-driving?\n",
       "47:59 Um yeah, I think like all environments\n",
       "48:01 like even in reinforcement learning or\n",
       "48:04 uh uh other ways to teach the car there\n",
       "48:07 would be some constraints that you\n",
       "48:09 impose upon it like you know the the\n",
       "48:12 rules of the world like you shouldn't\n",
       "48:13 like go against the traffic there are\n",
       "48:17 the these you need to put these\n",
       "48:19 constraints u so it's like I I I don't\n",
       "48:24 think it's full do whatever learn\n",
       "48:28 however you want to drive.\n",
       "48:30 >> Uh definitely constrained by a lot of\n",
       "48:32 rules and also I I would say like as you\n",
       "48:35 try to expand into different countries\n",
       "48:37 or different continents there are a\n",
       "48:39 whole bunch of new rules that go about\n",
       "48:41 over there.\n",
       "48:42 >> Uh sometimes you like even in the same\n",
       "48:46 country even different cities have\n",
       "48:48 different patterns of driving. Sometimes\n",
       "48:50 the travels are very aggressive.\n",
       "48:51 Sometimes it's like more rule following,\n",
       "48:53 right?\n",
       "48:54 >> So it needs to be adaptable that way and\n",
       "48:56 constrained in specifically.\n",
       "48:59 >> Yeah. So in Italy and in Germany, it's\n",
       "49:02 so so different like the way people\n",
       "49:05 drive. So in Germany, at least in\n",
       "49:06 Berlin, people are driving slow and it's\n",
       "49:09 very easy to cross the street. But in\n",
       "49:11 Italy, especially in the south, good\n",
       "49:12 luck. like you have to be you just have\n",
       "49:14 to go across the street then they will\n",
       "49:17 stop otherwise they will just keep\n",
       "49:18 driving right\n",
       "49:20 >> okay\n",
       "49:21 yeah yeah I I think like uh it still\n",
       "49:24 needs to learn all these patterns\n",
       "49:26 >> in the world so a lot of constraints\n",
       "49:28 over that's why it's such a hard problem\n",
       "49:30 like you know it changes so so so much\n",
       "49:34 with geographies\n",
       "49:35 >> yeah I was thinking about chess because\n",
       "49:37 in chess and in also in go um they used\n",
       "49:40 eventually reinforcement learning to\n",
       "49:42 build this uh state-of-the-art uh models\n",
       "49:45 for playing these games. Um but what\n",
       "49:47 they did is they let um the I don't know\n",
       "49:51 how to say agent or whatever the players\n",
       "49:53 um the AI go wild and do whatever they\n",
       "49:56 want. So instead of learning from\n",
       "49:59 previous games, they just let the game\n",
       "50:01 the AI explore the game and then\n",
       "50:05 eventually because they were not bound\n",
       "50:07 by the training data by the what people\n",
       "50:10 played in the past, they could play\n",
       "50:12 better than humans, right? Um but I\n",
       "50:15 guess with AI, with self-driving, it's\n",
       "50:18 kind of different, right? So you still\n",
       "50:20 need to obey. But yeah, in chess you\n",
       "50:23 have rules. the knight jumps like uh\n",
       "50:26 this right or the bishop goes by on\n",
       "50:29 diagonals right so you have these rules\n",
       "50:31 in chess too\n",
       "50:32 >> yeah I I think like the problem here or\n",
       "50:35 or the difference here what I see is\n",
       "50:36 with chess the rules are fixed like you\n",
       "50:39 know\n",
       "50:39 >> yeah very\n",
       "50:40 >> every piece has a given purpose and\n",
       "50:43 there's no rules beyond that like you\n",
       "50:45 have these pieces and 16 pieces and then\n",
       "50:49 you have the 16 set of rules that's it\n",
       "50:52 and then you can do whatever and then\n",
       "50:54 you can explore and then AI just has\n",
       "50:57 like full reign to do whatever uh it\n",
       "51:00 wants to do. But in self-driving the\n",
       "51:02 rules are also constantly evolving like\n",
       "51:05 you have like infinite number of rules I\n",
       "51:08 would say. So it's it's hard to like\n",
       "51:13 teach the model in such a changing\n",
       "51:15 environment uh so to speak. But uh yeah\n",
       "51:19 uh yeah I honestly don't know if we use\n",
       "51:22 reinforcement learning but yeah\n",
       "51:24 definitely the constraints do come into\n",
       "51:26 picture whatever model we use. Yeah, I\n",
       "51:28 see that there's a question from Adonis.\n",
       "51:30 He just asked it. How is uh how does the\n",
       "51:33 testing process go for sensitive cases\n",
       "51:35 like autonomous driving? Two developers\n",
       "51:38 inherit some general tests and they have\n",
       "51:41 to pass uh are there any stages? That's\n",
       "51:44 a very large a big loaded question. But\n",
       "51:48 uh I guess yeah the question is like uh\n",
       "51:50 how does testing uh how is testing\n",
       "51:53 organized for self-driving cars?\n",
       "51:56 Yeah, I I guess like uh it it depends on\n",
       "51:59 like what what change you're trying to\n",
       "52:00 do. So I work on pedestrians and\n",
       "52:03 gestures. So we have a bunch of like\n",
       "52:06 evaluations around cases where you have\n",
       "52:09 these pedestrians and or cases which\n",
       "52:11 have happened in the past and you try to\n",
       "52:14 like rerun your new model on those cases\n",
       "52:16 and then that is the first stage and\n",
       "52:18 then the next stage you uh evaluate\n",
       "52:20 overall with various different scenarios\n",
       "52:22 from the world where other pedestrians\n",
       "52:24 are involved and uh like the like the\n",
       "52:28 question mentioned there are different\n",
       "52:29 stages like you starts off small and\n",
       "52:31 then you slowly get like bigger and more\n",
       "52:34 comprehensive eval sets and then you\n",
       "52:37 roll it out slowly like you let drivers\n",
       "52:40 take it to the world and drive few miles\n",
       "52:42 around then you deploy it to the larger\n",
       "52:44 field. So it happens the rollout happens\n",
       "52:47 in stages uh that way.\n",
       "52:49 >> Mhm.\n",
       "52:51 And um\n",
       "52:53 another question about LLMs. We already\n",
       "52:56 talked LLMs can do computer vision too,\n",
       "52:59 right? Uh but they are\n",
       "53:03 kind of slow, right? Is there do you\n",
       "53:06 think um we can apply this with for\n",
       "53:11 self-driving at some point? Like does it\n",
       "53:13 actually make sense to use generative\n",
       "53:15 models for that?\n",
       "53:17 Yeah, I I think like there uh there have\n",
       "53:20 been a lot of attempts. So if you see\n",
       "53:22 the latest state-of-the-art even in\n",
       "53:24 literature or even some companies such\n",
       "53:26 as wave, they are trying to in fact use\n",
       "53:29 like multimodel LLMs for a self like end\n",
       "53:32 to end driving self-driving case. So\n",
       "53:36 there is definitely a lot of room mainly\n",
       "53:38 because LLMs are trained or pre-trained\n",
       "53:40 on so much amount of data. they have so\n",
       "53:43 much of world knowledge that's e easy to\n",
       "53:46 teach them more or like you know have\n",
       "53:48 them behave certain ways because they\n",
       "53:50 already have a lot of knowledge. Uh the\n",
       "53:52 challenge still remains in you know how\n",
       "53:54 do you make them fast enough. So the you\n",
       "53:57 have to like probably do a lot of\n",
       "53:59 tradeoffs. There needs to be a lot of\n",
       "54:01 different techniques but definitely it's\n",
       "54:04 something that's been explored very\n",
       "54:06 actively in current research as well as\n",
       "54:08 by some companies. And there are in fact\n",
       "54:11 systems out there that use LLMs for the\n",
       "54:14 self-driving use case\n",
       "54:16 >> because I guess uh we talked about\n",
       "54:17 different patterns in different\n",
       "54:19 countries like Italy versus Germany.\n",
       "54:22 LLM might know might not might know\n",
       "54:25 about these things, right? It might know\n",
       "54:26 that uh I don't know if you go to south\n",
       "54:31 it's more chaotic. If you go to north\n",
       "54:32 it's more strict, right? So something\n",
       "54:34 like this. Uh so maybe for it it's\n",
       "54:36 easier to actually use it for I don't\n",
       "54:41 know I'm just making this up I guess but\n",
       "54:43 uh maybe it already knows because it uh\n",
       "54:45 the ways are trained they trained from\n",
       "54:48 internet forums where Germans can go to\n",
       "54:52 forums and complain about drivers in\n",
       "54:53 Italy right\n",
       "54:55 >> yeah I guess that's the hope that it has\n",
       "54:58 some semblance of knowledge about like a\n",
       "55:01 lot of different things that probably\n",
       "55:04 you're well curated ated data set might\n",
       "55:06 not have like when you try to train a\n",
       "55:08 model you curate a data set and you only\n",
       "55:10 put in cases that you might be aware of\n",
       "55:14 but with LLMs the thing is that it has\n",
       "55:16 so much of world knowledge that you\n",
       "55:19 there are high possibility that it might\n",
       "55:21 know these things and it it might help\n",
       "55:22 you in your tuning process to get it\n",
       "55:25 across the world.\n",
       "55:28 >> Okay, maybe last question and we wrap it\n",
       "55:30 up. So if I want to work on self-driving\n",
       "55:33 cars, uh um\n",
       "55:36 what should I do? What should I study?\n",
       "55:38 Uh how do you uh can I get into this\n",
       "55:40 industry?\n",
       "55:42 >> Yeah, I I guess like uh it again starts\n",
       "55:45 with uh\n",
       "55:47 deep learning. So the way I did it was I\n",
       "55:50 was good at deep learning. I got got\n",
       "55:52 into the AI guard dog project which was\n",
       "55:54 a similar use case like you know you use\n",
       "55:56 vision and you use navigation and that's\n",
       "55:59 how like that that based on that one\n",
       "56:01 project I got into Tesla. So uh I think\n",
       "56:05 it's a combination of knowing your\n",
       "56:07 fundamentals and doing relevant\n",
       "56:08 projects. So if you are if you do some\n",
       "56:12 computer vision related projects it's a\n",
       "56:14 very good start. It's very good to have\n",
       "56:16 on your resume so that the company knows\n",
       "56:19 that okay you're familiar with the space\n",
       "56:21 so they pick your resume and you can you\n",
       "56:23 at least get the chance to interview and\n",
       "56:24 then you like iterate and get better.\n",
       "56:27 >> Mhm. So, a good pet project could be uh\n",
       "56:31 you write an app that you just show like\n",
       "56:35 with use your that uses your camera to\n",
       "56:37 describe things that you have like you\n",
       "56:40 just point at your room and it says okay\n",
       "56:42 there is a bed uh uh there is like a\n",
       "56:46 clock and things like that right\n",
       "56:48 >> I think I saw that big with yo\n",
       "56:52 >> yeah like even with the LLMs it's gotten\n",
       "56:55 so much easier that you even if you like\n",
       "56:57 prompt an LM with the appropriate\n",
       "56:58 prompt. You don't need to train\n",
       "57:00 anything. It will just tell you whatever\n",
       "57:02 it is in the room.\n",
       "57:03 >> What else you can ask Chad GPT to write\n",
       "57:05 you this app and then you learn how we\n",
       "57:07 did this, I guess.\n",
       "57:10 >> Yeah, that's like two stages. You don't\n",
       "57:12 even write the app yourself. Corp can do\n",
       "57:15 it.\n",
       "57:16 >> But yeah, I asked recently um a tool\n",
       "57:21 like Corser, it's a coding agent. I\n",
       "57:24 asked it to implement a multi- aent\n",
       "57:27 system for evaluating uh projects that\n",
       "57:31 are on GitHub according to a set of\n",
       "57:33 criteria and then half a half a hour\n",
       "57:37 later I had a system that kind of works.\n",
       "57:40 I needed to tweak a it a little bit uh\n",
       "57:42 also with prompts and then it was\n",
       "57:44 working and then I could start studying\n",
       "57:46 the how it was implemented and I was\n",
       "57:48 okay that's nice now I also know how to\n",
       "57:51 do it. Yeah, it's it's fascinating to me\n",
       "57:55 like you know these kind of projects\n",
       "57:56 used to be projects and pet projects in\n",
       "57:58 college and you used to spend like weeks\n",
       "58:00 on it and now it's just done but still\n",
       "58:03 yeah I I tried using it once it gets\n",
       "58:05 stuck in some weird bugs that you\n",
       "58:07 >> yeah then you have to intervene and\n",
       "58:09 >> and like you have to also learn\n",
       "58:11 everything that the AI system came up\n",
       "58:14 with before you can fix the bug you have\n",
       "58:17 to know the entire code base\n",
       "58:19 >> and sometimes\n",
       "58:20 >> I think it's very good\n",
       "58:22 >> sorry go ahead\n",
       "58:23 Yeah, I think it's very good for setting\n",
       "58:24 a framework like when you're starting a\n",
       "58:26 whole new project from scratch and you\n",
       "58:28 want like a framework, it's really good\n",
       "58:29 with that. But if you want to get into\n",
       "58:31 the nitty-g gritties, you have to know\n",
       "58:33 what you're doing. Yeah.\n",
       "58:35 >> Okay.\n",
       "58:36 >> Okay. Um it was amazing talking to you.\n",
       "58:40 Um\n",
       "58:41 so thanks a lot. Uh good that we managed\n",
       "58:43 to work out work it out and find time\n",
       "58:46 that worked for us. Um, I'm sorry about\n",
       "58:50 uh you getting sick, so I hope you\n",
       "58:51 recover super quickly. So now I know\n",
       "58:54 it's very late for you, so you should go\n",
       "58:55 and rest and I'll go have breakfast.\n",
       "58:59 >> All right, have a good day. Nice to meet\n",
       "59:01 you.\n",
       "59:01 >> So thank you and thanks everyone for\n",
       "59:03 joining us today. Um, and</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The video features an interview focusing on the speaker's journey and experiences in the field of artificial intelligence (AI), particularly in machine learning and self-driving technologies. The main guest is Aishwara, a machine learning engineer who has worked for companies like Tesla and is currently at Whimo.</p>\n",
       "<h3>Key Highlights:</h3>\n",
       "<ol>\n",
       "<li><p><strong>Introduction to the Event</strong>:</p>\n",
       "<ul>\n",
       "<li>A community event organized by Data Talks Club focusing on data and machine learning.</li>\n",
       "<li>Audience engagement via live questions.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Aishwara's Background</strong>:</p>\n",
       "<ul>\n",
       "<li>Previously worked at Tesla, focusing on self-driving technology, and at Morgan Stanley on financial recommendation systems.</li>\n",
       "<li>Discussed involvement in AI for social good projects, including a malaria mapping project in Africa.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Project Details</strong>:</p>\n",
       "<ul>\n",
       "<li>Description of the &quot;AI Guide Dog&quot; project, aimed at helping visually impaired individuals navigate using AI.</li>\n",
       "<li>The app sends audio instructions to the user based on live analysis of their surroundings.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Self-Driving Technology</strong>:</p>\n",
       "<ul>\n",
       "<li>Insights into the complexities of developing self-driving cars, including handling pedestrians and learning from various real-world situations.</li>\n",
       "<li>Discussion of the integration of AI models and the evaluation/testing processes for safety.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Current Trends in AI</strong>:</p>\n",
       "<ul>\n",
       "<li>Exploration of the rise of large language models (LLMs) and their potential applications in self-driving technology.</li>\n",
       "<li>The importance of having a solid foundation in deep learning and computer vision for anyone looking to enter this field.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Advice for Aspiring Engineers</strong>:</p>\n",
       "<ul>\n",
       "<li>Recommendations for engaging in relevant projects and staying updated with advancements in AI and machine learning.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Overall, the video provides an informative look into AI's application in real-world challenges, emphasizing the ongoing evolution in the field and the role of community and collaboration in driving innovation.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What is this video about https://www.youtube.com/watch?v=vK_SxyqIfwk$t=172s Give the example like this       Example:     0:00 Hey everyone, welcome to our event. This     0:02 event is brought to you by data talks     0:03 club which is a community of people who     0:05 love data. We have weekly events today.     0:08 Uh this is one of such events. Um if you     0:11 want to find out more about the events     0:13 we have, there is a link in the     0:14 description. Um so click on that link,     0:16 check it out right now. We actually have     0:19 quite a few events in our pipeline, but     0:21 we need to put them on the website. Uh     0:24 but keep a\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here’s a summary of the video with time-stamped examples in the style you've provided:</p>\n",
       "<hr />\n",
       "<p><strong>0:00</strong> Hey everyone, welcome to our event. This<br />\n",
       "<strong>0:02</strong> event is brought to you by Data Talks Club, which is a community for data enthusiasts.<br />\n",
       "<strong>0:05</strong> We have weekly events; today is one of them.<br />\n",
       "<strong>0:08</strong> If you want to find out more about our events, there's a link in the description.<br />\n",
       "<strong>0:11</strong> Click on that link to check it out.<br />\n",
       "<strong>0:14</strong> We have several upcoming events, and we'll update the website soon.<br />\n",
       "<strong>0:19</strong> Don’t forget to subscribe to our YouTube channel to get notified about our streams.<br />\n",
       "<strong>0:21</strong> Please join our data community to interact with other enthusiasts.<br />\n",
       "<strong>0:27</strong> During today's interview, you can ask any questions via a pinned link in the chat.<br />\n",
       "<strong>0:30</strong> I'll now introduce our guest, Aishwara, a machine learning engineer.<br />\n",
       "<strong>0:33</strong> Aishwara has an impressive background, having worked at Tesla and Morgan Stanley.<br />\n",
       "<strong>0:37</strong> She’s involved in important AI projects, like a malaria mapping initiative in Africa.<br />\n",
       "<strong>0:40</strong> Welcome, Aishwara!<br />\n",
       "<strong>0:43</strong> Can you tell us about your career journey?<br />\n",
       "<strong>0:50</strong> Aishwara explains her work at Morgan Stanley with big data and financial recommendation engines.<br />\n",
       "<strong>1:00</strong> She transitioned to machine learning and focused more on computer vision during her studies.<br />\n",
       "<strong>1:05</strong> Aishwara worked on an AI project to help visually impaired individuals navigate using smartphones.<br />\n",
       "<strong>1:08</strong> She emphasizes the importance of using learned skills in real-world applications.<br />\n",
       "<strong>1:15</strong> The discussion then shifts to self-driving technology and its challenges.</p>\n",
       "<hr />\n",
       "<p>Let me know if you need more details or further breakdown!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "await runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ebcafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac729e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp-codespace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
