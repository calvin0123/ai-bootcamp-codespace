{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e989b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "\n",
    "file_index = {d['filename']: d['content'] for d in parsed_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca7076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Output formats',\n",
       " 'description': 'How to export the evaluation results.',\n",
       " 'content': 'You can view or export Reports in multiple formats.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Log to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>\\n\\n## View in Jupyter notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.\\n\\n## HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(“file.html”)\\n```\\n\\nThis option is useful for sharing Reports with others or if you\\'re working in a Python environment that doesn’t display interactive visuals.\\n\\n## JSON\\n\\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```\\n\\n## Python dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```',\n",
       " 'filename': 'docs/library/output_formats.mdx'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76289ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('eval-run-v2-2025-10-29-20-15.bin', 'rb') as f_in:\n",
    "    rows = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60c0e271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'install Evidently for LLM',\n",
       " 'answer': '# Installing Evidently for LLM\\n\\n## Installation Steps\\n\\n1. **Create an Account:** If you haven\\'t done so, sign up for a free account at [Evidently Cloud](https://app.evidently.cloud/signup). After logging in, create an organization.\\n\\n2. **Install the Evidently Python Package:** You can install the Evidently library using pip. For general installation:\\n   ```bash\\n   pip install evidently\\n   ```\\n   If you are specifically working with large language models (LLMs), use the following command to install additional dependencies:\\n   ```bash\\n   pip install evidently[llm]\\n   ```\\n\\n3. **Connect to the Cloud Workspace:** Once installed, you need to import the CloudWorkspace from the Evidently library and provide your API token to connect:\\n   ```python\\n   from evidently.ui.workspace import CloudWorkspace\\n   ws = CloudWorkspace(\\ntoken=\"API_KEY\",\\n   url=\"https://app.evidently.cloud\")\\n   ```\\n   Alternatively, set the environment variable `EVIDENTLY_API_KEY` to provide your token.\\n\\n### References\\n- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\\n- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\\n- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\\n## References\\n- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\\n- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\\n- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\\n',\n",
       " 'messages': [{'kind': 'user-prompt', 'content': 'install Evidently for LLM'},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'install Evidently for LLM'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently installation guide'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently usage with Large Language Models'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently LLM integration'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently library requirements for LLM'}}],\n",
       " 'num_tool_calls': 5,\n",
       " 'original_question': {'question': 'install Evidently for LLM',\n",
       "  'summary_answer': 'Instructions for installing the Evidently library to use for creating and evaluating LLM judges are provided within the tutorial.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " 'original_result': AgentRunResult(output=SearchResultArticle(found_answer=True, title='Installing Evidently for LLM', sections=[Section(heading='Installation Steps', content='1. **Create an Account:** If you haven\\'t done so, sign up for a free account at [Evidently Cloud](https://app.evidently.cloud/signup). After logging in, create an organization.\\n\\n2. **Install the Evidently Python Package:** You can install the Evidently library using pip. For general installation:\\n   ```bash\\n   pip install evidently\\n   ```\\n   If you are specifically working with large language models (LLMs), use the following command to install additional dependencies:\\n   ```bash\\n   pip install evidently[llm]\\n   ```\\n\\n3. **Connect to the Cloud Workspace:** Once installed, you need to import the CloudWorkspace from the Evidently library and provide your API token to connect:\\n   ```python\\n   from evidently.ui.workspace import CloudWorkspace\\n   ws = CloudWorkspace(\\ntoken=\"API_KEY\",\\n   url=\"https://app.evidently.cloud\")\\n   ```\\n   Alternatively, set the environment variable `EVIDENTLY_API_KEY` to provide your token.', references=[Reference(title='Evidently Cloud', filename='docs/setup/cloud.mdx'), Reference(title='Installation', filename='docs/setup/installation.mdx'), Reference(title='Why Evidently?', filename='faq/why_evidently.mdx')])], references=[Reference(title='Evidently Cloud', filename='docs/setup/cloud.mdx'), Reference(title='Installation', filename='docs/setup/installation.mdx'), Reference(title='Why Evidently?', filename='faq/why_evidently.mdx')]))}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da383525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "\n",
    "df_evals['filename'] = df_evals.original_question.apply(lambda x: x['filename'])\n",
    "df_evals['reference'] = df_evals.filename.apply(file_index.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac77cc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>messages</th>\n",
       "      <th>num_tool_calls</th>\n",
       "      <th>original_question</th>\n",
       "      <th>original_result</th>\n",
       "      <th>filename</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SemanticSimilarity descriptor explained</td>\n",
       "      <td># Understanding SemanticSimilarity Descriptor ...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'SemanticS...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'question': 'SemanticSimilarity descriptor ex...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>metrics/all_descriptors.mdx</td>\n",
       "      <td>&lt;Info&gt;\\n  For an intro, read about [Core Conce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>using is_critical parameter in alerts</td>\n",
       "      <td># Using `is_critical` Parameter in Evidently A...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'using is_...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'question': 'using is_critical parameter in a...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>docs/platform/alerts.mdx</td>\n",
       "      <td>&lt;Check&gt;\\n  Built-in alerting is a Pro feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quickstart Evidently setup</td>\n",
       "      <td># Quickstart Setup for Evidently\\n\\n## Creatin...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'Quickstar...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'question': 'Quickstart Evidently setup', 'su...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>docs/library/overview.mdx</td>\n",
       "      <td>The Evidently Python library is an open-source...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drift detection for numerical columns</td>\n",
       "      <td># Drift Detection for Numerical Columns in Evi...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'Drift det...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'question': 'Drift detection for numerical co...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>metrics/customize_data_drift.mdx</td>\n",
       "      <td>All Metrics and Presets that evaluate shift in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-letter character percentage function</td>\n",
       "      <td># Non-letter Character Percentage Function in ...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'non-lette...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'question': 'non-letter character percentage ...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>metrics/all_descriptors.mdx</td>\n",
       "      <td>&lt;Info&gt;\\n  For an intro, read about [Core Conce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   question  \\\n",
       "0   SemanticSimilarity descriptor explained   \n",
       "1     using is_critical parameter in alerts   \n",
       "2                Quickstart Evidently setup   \n",
       "3     Drift detection for numerical columns   \n",
       "4  non-letter character percentage function   \n",
       "\n",
       "                                              answer  \\\n",
       "0  # Understanding SemanticSimilarity Descriptor ...   \n",
       "1  # Using `is_critical` Parameter in Evidently A...   \n",
       "2  # Quickstart Setup for Evidently\\n\\n## Creatin...   \n",
       "3  # Drift Detection for Numerical Columns in Evi...   \n",
       "4  # Non-letter Character Percentage Function in ...   \n",
       "\n",
       "                                            messages  num_tool_calls  \\\n",
       "0  [{'kind': 'user-prompt', 'content': 'SemanticS...               5   \n",
       "1  [{'kind': 'user-prompt', 'content': 'using is_...               4   \n",
       "2  [{'kind': 'user-prompt', 'content': 'Quickstar...               5   \n",
       "3  [{'kind': 'user-prompt', 'content': 'Drift det...               5   \n",
       "4  [{'kind': 'user-prompt', 'content': 'non-lette...               5   \n",
       "\n",
       "                                   original_question  \\\n",
       "0  {'question': 'SemanticSimilarity descriptor ex...   \n",
       "1  {'question': 'using is_critical parameter in a...   \n",
       "2  {'question': 'Quickstart Evidently setup', 'su...   \n",
       "3  {'question': 'Drift detection for numerical co...   \n",
       "4  {'question': 'non-letter character percentage ...   \n",
       "\n",
       "                                     original_result  \\\n",
       "0  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "1  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "2  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "3  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "4  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "\n",
       "                           filename  \\\n",
       "0       metrics/all_descriptors.mdx   \n",
       "1          docs/platform/alerts.mdx   \n",
       "2         docs/library/overview.mdx   \n",
       "3  metrics/customize_data_drift.mdx   \n",
       "4       metrics/all_descriptors.mdx   \n",
       "\n",
       "                                           reference  \n",
       "0  <Info>\\n  For an intro, read about [Core Conce...  \n",
       "1  <Check>\\n  Built-in alerting is a Pro feature ...  \n",
       "2  The Evidently Python library is an open-source...  \n",
       "3  All Metrics and Presets that evaluate shift in...  \n",
       "4  <Info>\\n  For an intro, read about [Core Conce...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8a4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7db02bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/evidently/descriptors/text_match.py:140: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  TextMatch(text_column=\"description\", match_items=r\"\\b\\d{3}-\\d{3}-\\d{4}\\b\", match_type=\"regex\")\n"
     ]
    }
   ],
   "source": [
    "# Configure LLM evaluation\n",
    "# evidently \n",
    "\n",
    "from evidently import Dataset, DataDefinition\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.templates import MulticlassClassificationPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5901af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = MulticlassClassificationPromptTemplate(\n",
    "    pre_messages=[\n",
    "        (\"system\", \"You are a judge that evaluates the factual alignment of two chatbot answers.\")\n",
    "    ],\n",
    "    criteria=\"\"\"\n",
    "    You are given a question, a new answer and a reference answer. \n",
    "    Classify the new answer based on how it compares to the reference.\n",
    "    ===\n",
    "    Question: {question}\n",
    "    Reference: {reference}\n",
    "    \"\"\",\n",
    "    category_criteria={\n",
    "        \"match\": \"The answer matches the reference in all factual and semantic details.\",\n",
    "        \"partial_match\": \"The answer is correct in what it says but leaves out details from the reference.\",\n",
    "        \"mismatch\": \"The answer doesn't match the reference answer.\",\n",
    "        \"not_available\": \"The answer says that information is not available.\",\n",
    "    },\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    "    include_scores=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f20d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evalution dataset\n",
    "# # Create an Evidently Dataset with LLM evaluation descriptor. This will evaluate each answer against its reference using \"gpt-4o-mini\":\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(\n",
    "    data=df_evals,\n",
    "    data_definition=DataDefinition(),\n",
    "    descriptors=[\n",
    "        LLMEval(\n",
    "            column_name=\"answer\",\n",
    "            additional_columns={\"question\": \"question\", \"reference\": \"reference\"},\n",
    "            template=matcher,\n",
    "            provider=\"openai\",\n",
    "            model=\"gpt-4o-mini\",\n",
    "            alias=\"eval\"\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceb47baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_result = eval_dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bf4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01f6ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new answer provides a valid installation process for Evidently, similar to the reference, but it focuses more on connecting to Evidently Cloud and suggests using an additional `[llm]` dependency, which is not explicitly mentioned in the reference. Therefore, while it is mostly aligned, it lacks some details present in the reference.\n"
     ]
    }
   ],
   "source": [
    "# Analyze evaluation results\n",
    "\n",
    "print(df_eval_result.iloc[10]['eval reasoning'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3181a6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'install Evidently for LLM'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_result.iloc[10]['question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8771636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Installing Evidently for LLM\n",
      "\n",
      "## Installation Steps\n",
      "\n",
      "1. **Create an Account:** If you haven't done so, sign up for a free account at [Evidently Cloud](https://app.evidently.cloud/signup). After logging in, create an organization.\n",
      "\n",
      "2. **Install the Evidently Python Package:** You can install the Evidently library using pip. For general installation:\n",
      "   ```bash\n",
      "   pip install evidently\n",
      "   ```\n",
      "   If you are specifically working with large language models (LLMs), use the following command to install additional dependencies:\n",
      "   ```bash\n",
      "   pip install evidently[llm]\n",
      "   ```\n",
      "\n",
      "3. **Connect to the Cloud Workspace:** Once installed, you need to import the CloudWorkspace from the Evidently library and provide your API token to connect:\n",
      "   ```python\n",
      "   from evidently.ui.workspace import CloudWorkspace\n",
      "   ws = CloudWorkspace(\n",
      "token=\"API_KEY\",\n",
      "   url=\"https://app.evidently.cloud\")\n",
      "   ```\n",
      "   Alternatively, set the environment variable `EVIDENTLY_API_KEY` to provide your token.\n",
      "\n",
      "### References\n",
      "- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\n",
      "- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\n",
      "- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\n",
      "## References\n",
      "- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\n",
      "- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\n",
      "- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_eval_result.iloc[10]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6576da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3203101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create report\n",
    "\n",
    "from evidently import Report\n",
    "from evidently.presets import TextEvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29244e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report([\n",
    "    TextEvals()\n",
    "])\n",
    "\n",
    "my_eval = report.run(eval_dataset, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9aeba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
