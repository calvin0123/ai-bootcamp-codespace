{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1882942a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docs\n",
    "\n",
    "raw_documents = docs.read_github_data()\n",
    "documents = docs.parse_data(raw_documents)\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e54f4721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data definition\n",
      "11046 11\n",
      "------------\n",
      "Descriptors\n",
      "12588 12\n",
      "------------\n",
      "Overview\n",
      "3231 3\n",
      "------------\n",
      "Metric generators\n",
      "2949 2\n",
      "------------\n",
      "Output formats\n",
      "1584 1\n",
      "------------\n",
      "Introduction\n",
      "22069 22\n",
      "------------\n",
      "Report\n",
      "4989 4\n",
      "------------\n",
      "Add tags and metadata\n",
      "2340 2\n",
      "------------\n",
      "Tests\n",
      "9154 9\n",
      "------------\n",
      "Alerts\n",
      "1282 1\n",
      "------------\n",
      "Add dashboard panels (API)\n",
      "13004 13\n",
      "------------\n",
      "Add dashboard panels (UI)\n",
      "4258 4\n",
      "------------\n",
      "Overview\n",
      "2735 2\n",
      "------------\n",
      "Overview\n",
      "2216 2\n",
      "------------\n",
      "Work with datasets\n",
      "2114 2\n",
      "------------\n",
      "Run evals via API\n",
      "2162 2\n",
      "------------\n",
      "Explore view\n",
      "1899 1\n",
      "------------\n",
      "No code evals\n",
      "4377 4\n",
      "------------\n",
      "Overview\n",
      "2138 2\n",
      "------------\n",
      "Batch monitoring\n",
      "2384 2\n",
      "------------\n",
      "Overview\n",
      "3768 3\n",
      "------------\n",
      "Introduction\n",
      "2408 2\n",
      "------------\n",
      "Manage Projects\n",
      "4614 4\n",
      "------------\n",
      "Overview\n",
      "1392 1\n",
      "------------\n",
      "Overview\n",
      "1507 1\n",
      "------------\n",
      "Set up tracing\n",
      "10120 10\n",
      "------------\n",
      "Evidently Cloud\n",
      "1218 1\n",
      "------------\n",
      "Self-hosting\n",
      "5515 5\n",
      "------------\n",
      "Evidently and GitHub actions\n",
      "1375 1\n",
      "------------\n",
      "LLM evaluations\n",
      "2314 2\n",
      "------------\n",
      "LLM as a judge\n",
      "21834 21\n",
      "------------\n",
      "LLM-as-a-jury\n",
      "9235 9\n",
      "------------\n",
      "RAG evals\n",
      "13227 13\n",
      "------------\n",
      "LLM regression testing\n",
      "21712 21\n",
      "------------\n",
      "Tutorials and guides\n",
      "12043 12\n",
      "------------\n",
      "Evidently Cloud v2\n",
      "1897 1\n",
      "------------\n",
      "Migration Guide\n",
      "7621 7\n",
      "------------\n",
      "Open-source vs. Cloud\n",
      "6013 6\n",
      "------------\n",
      "Telemetry\n",
      "10449 10\n",
      "------------\n",
      "Why Evidently?\n",
      "4876 4\n",
      "------------\n",
      "What is Evidently?\n",
      "1725 1\n",
      "------------\n",
      "All Descriptors\n",
      "31874 31\n",
      "------------\n",
      "All Metrics\n",
      "54996 54\n",
      "------------\n",
      "Overview\n",
      "1140 1\n",
      "------------\n",
      "Customize Data Drift\n",
      "17825 17\n",
      "------------\n",
      "Custom Text Descriptor\n",
      "3569 3\n",
      "------------\n",
      "Use HuggingFace models\n",
      "10737 10\n",
      "------------\n",
      "Configure LLM Judges\n",
      "26737 26\n",
      "------------\n",
      "Custom Metric\n",
      "4162 4\n",
      "------------\n",
      "Classification metrics\n",
      "8031 8\n",
      "------------\n",
      "Data stats and quality\n",
      "7536 7\n",
      "------------\n",
      "Data drift\n",
      "8366 8\n",
      "------------\n",
      "Ranking and RecSys metrics\n",
      "10174 10\n",
      "------------\n",
      "Regression metrics\n",
      "9333 9\n",
      "------------\n",
      "Classification\n",
      "3972 3\n",
      "------------\n",
      "Data Drift\n",
      "5670 5\n",
      "------------\n",
      "Data Summary\n",
      "3898 3\n",
      "------------\n",
      "Recommendations\n",
      "3120 3\n",
      "------------\n",
      "Regression\n",
      "3473 3\n",
      "------------\n",
      "Text Evals\n",
      "3373 3\n",
      "------------\n",
      "LLM Evaluation\n",
      "9621 9\n",
      "------------\n",
      "Data and ML checks\n",
      "6907 6\n",
      "------------\n",
      "Tracing\n",
      "5942 5\n",
      "------------\n",
      "Adversarial testing\n",
      "2514 2\n",
      "------------\n",
      "Create synthetic inputs\n",
      "1934 1\n",
      "------------\n",
      "Synthetic data\n",
      "1274 1\n",
      "------------\n",
      "RAG evaluation dataset\n",
      "1982 1\n",
      "------------\n",
      "Why synthetic data?\n",
      "2082 2\n",
      "------------\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "num_questions_total = 0\n",
    "\n",
    "selected_documents = []\n",
    "\n",
    "for doc in documents[5:]:\n",
    "    if 'title' not in doc:\n",
    "        continue\n",
    "\n",
    "    title = doc['title']\n",
    "    if 'unpublished' in title.lower():\n",
    "        continue\n",
    "    if 'legacy' in title.lower():\n",
    "        continue\n",
    "    if 'leftovers' in title.lower():\n",
    "        continue\n",
    "\n",
    "    content = doc.get('content', '').strip()\n",
    "    if len(content) <= 1000:\n",
    "        continue\n",
    "\n",
    "    num_questions = len(content) // 1000\n",
    "    print(doc.get('title'))\n",
    "    print(len(content), num_questions)\n",
    "    num_questions_total = num_questions_total + num_questions\n",
    "    print('------------')\n",
    "\n",
    "    selected_documents.append(doc)\n",
    "\n",
    "print(num_questions_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c68ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d8bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42fc6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are given a technical article. Your task is to imagine what a person might type into a search engine \n",
    "before finding and reading this article.\n",
    "\n",
    "Generate realistic, human-like search queries — not formal questions. \n",
    "They should sound like what people actually type into Google or Stack Overflow \n",
    "when trying to solve a problem, learn a concept, or find code examples.\n",
    "\n",
    "Guidelines:\n",
    "- Avoid full-sentence questions with punctuation like \"What is...\" or \"How do I...\".\n",
    "- Use short, natural search phrases instead, such as:\n",
    "  - \"evidently data definition example\"\n",
    "  - \"map target and prediction columns evidently\"\n",
    "  - \"difference between timestamp and datetime evidently\"\n",
    "- Make queries varied and spontaneous, not repetitive or over-polished.\n",
    "- Assume users of different knowledge levels:\n",
    "  - beginner: broad or basic understanding\n",
    "  - intermediate: knows basic terms but seeks clarification or examples\n",
    "  - advanced: familiar with the tool, looking for details, edge cases, or integration options\n",
    "\n",
    "Distribution rules:\n",
    "- 60% of the queries should target beginner-level users\n",
    "- 30% should target intermediate-level users\n",
    "- 10% should target advanced-level users\n",
    "- 75% of queries should have an intent of \"code\" (looking for examples or implementation)\n",
    "- 25% should have an intent of \"text\" (looking for conceptual or theoretical explanations)\n",
    "\n",
    "For each generated query, include:\n",
    "- question: the natural, human-style search phrase\n",
    "- summary_answer: a short 1–2 sentence summary of how the article addresses it\n",
    "- difficulty: one of [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "- intent: one of [\"text\", \"code\"]\n",
    "\n",
    "Also include a description summarizing what kind of article the questions are about.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bcbfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm_structured(instructions, user_prompt, output_format, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_format\n",
    "    )\n",
    "\n",
    "    return response.output_parsed, response.usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b2e4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a realistic search-engine-style query a user might type before finding the article.\n",
    "    Each question captures the likely search phrase, a short summary answer,\n",
    "    the user's assumed skill level, and their intent (conceptual or code-focused).\n",
    "    \"\"\"\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"A natural, short search query — not a full-sentence question — phrased like something typed into Google.\"\n",
    "    )\n",
    "    summary_answer: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise 1–2 sentence summary of how the article addresses the query.\"\n",
    "    )\n",
    "    difficulty: Literal[\"beginner\", \"intermediate\", \"advanced\"] = Field(\n",
    "        ...,\n",
    "        description=\"The assumed knowledge level of the user making the query.\"\n",
    "    )\n",
    "    intent: Literal[\"text\", \"code\"] = Field(\n",
    "        ...,\n",
    "        description=\"Specifies if the user's intent is to get a theoretical explanation ('text') or an implementation example ('code').\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GeneratedQuestions(BaseModel):\n",
    "    \"\"\"\n",
    "    A structured collection of human-like search queries derived from a given article.\n",
    "    Includes a brief description of the article topic and a list of generated queries.\n",
    "    Difficulty distribution: 60% beginner, 30% intermediate, 10% advanced.\n",
    "    Intent distribution: 75% code-focused, 25% concept-focused.\n",
    "    \"\"\"\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"A summary of the article or topic these search-style questions were generated for.\"\n",
    "    )\n",
    "    questions: List[Question] = Field(\n",
    "        ...,\n",
    "        description=\"A list of realistic search queries with short summaries, difficulty levels, and user intent.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21f7853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def map_progress(pool, seq, f):\n",
    "    \"\"\"Map function f over seq using the provided executor pool while\n",
    "    displaying a tqdm progress bar. Returns a list of results in submission order.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "    \n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30bad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_document(doc):\n",
    "    content = doc['content']\n",
    "    num_questions = len(content) // 1000\n",
    "    user_prompt = f\"\"\"generate {num_questions} for this document:\n",
    "    <document>{json.dumps(doc)}</document>\n",
    "    \"\"\"\n",
    "    response, usage = llm_structured(\n",
    "        instructions=instructions,\n",
    "        user_prompt=user_prompt,\n",
    "        output_format=GeneratedQuestions\n",
    "    )\n",
    "    return {\n",
    "        'doc': doc,\n",
    "        'questions': response.questions,\n",
    "        'usage': usage\n",
    "    }\n",
    "\n",
    "doc = selected_documents[0]\n",
    "result = process_document(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "025056f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [01:11<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=6) as pool:\n",
    "    all_results = map_progress(pool, selected_documents, process_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb49b302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc': {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'content': 'To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\\n\\n- **Column types** (e.g., categorical, numerical, text).\\n- **Column roles** (e.g., id, prediction, target).\\n\\nThis allows Evidently to process the data correctly. Some evaluations need specific columns and will fail if they\\'re missing. You can define the mapping using the Python API or by assigning columns visually when uploading data to the Evidently platform.\\n\\n## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/descriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they’ll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated Mapping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping required for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<Info>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\\n    )\\n```\\n\\n### Regression\\n\\nTo run regression quality checks, you must map the columns with:\\n\\n- Target: actual values.\\n- Prediction: predicted values.\\n\\nYou can have several regression results in the dataset, for example in case of multiple regression. (Pass the mappings in a list).\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n    )\\n```\\n\\nDefaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```\\n\\n### Classification\\n\\nTo run classification checks, you must map the columns with:\\n\\n- Target: true label.\\n- Prediction: predicted labels/probabilities.\\n\\nThere two different mapping options, for binary and multi-class classification. You can also have several classification results in the dataset. (Pass the mappings in a list).\\n\\n#### Multiclass\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification=[MulticlassClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\",\\n        prediction_probas=[\"0\", \"1\", \"2\"],  # If probabilistic classification\\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}  # Optional, for display only\\n    )]\\n)\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: str = \"prediction\"\\n    prediction_probas: Optional[List[str]] = None #if probabilistic classification\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n<Note>\\n  When you have multiclass classification with predicted probabilities in separate columns, the column names in `prediction_probas` must exactly match the class labels. For example, if your classes are 0, 1, and 2, your probability columns must be named: \"0\", \"1\", \"2\". Values in `target` and `prediction` columns should be strings.\\n</Note>\\n\\n#### Binary\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import BinaryClassification\\n\\ndefinition = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " 'questions': [Question(question='data definition mapping example', summary_answer='The article provides code snippets for defining how to map input data using the `DataDefinition` class, including examples for text, numerical, and categorical columns.', difficulty='beginner', intent='code'),\n",
       "  Question(question='create Dataset object Evidently', summary_answer='To create a `Dataset` object in Evidently, you can use the `Dataset.from_pandas()` method with a data definition to specify the roles and types of your data columns.', difficulty='beginner', intent='code'),\n",
       "  Question(question='Evidently Dataset column roles', summary_answer='The article outlines various column roles in Evidently, including target and prediction columns, essential for accurate data evaluations.', difficulty='beginner', intent='text'),\n",
       "  Question(question='defining text columns Evidently', summary_answer='You can define text columns in the `DataDefinition` using the `text_columns` parameter to help the library correctly process LLM evaluations.', difficulty='intermediate', intent='code'),\n",
       "  Question(question='Evidently DataDefinition examples', summary_answer='The document provides multiple examples of specifying column types and roles using the `DataDefinition` class, including automatic and manual mapping options.', difficulty='intermediate', intent='code'),\n",
       "  Question(question='mapping regression target and prediction', summary_answer='To map regression columns in Evidently, use the `DataDefinition` class with the target and prediction parameters set appropriately for your evaluation.', difficulty='intermediate', intent='code'),\n",
       "  Question(question='definitions for binary classification', summary_answer='You can use the `BinaryClassification` mapping in the `DataDefinition` to specify target and prediction columns for binary classification tasks.', difficulty='intermediate', intent='code'),\n",
       "  Question(question='pandas DataFrame in Evidently', summary_answer='The article details how to work with pandas DataFrames directly in Evidently and emphasizes the importance of creating Dataset objects for clarity.', difficulty='beginner', intent='text'),\n",
       "  Question(question='column types in DataDefinition', summary_answer='It explains various column types like numerical, categorical, and text, and how to define them using the `DataDefinition` class.', difficulty='beginner', intent='text'),\n",
       "  Question(question='automatic mapping in Evidently', summary_answer='You can let Evidently automatically map columns by passing an empty `DataDefinition`, although manual mapping is recommended for accuracy.', difficulty='advanced', intent='text'),\n",
       "  Question(question='Evidently mapping roles and types', summary_answer='The article discusses how to accurately specify roles and types for each column in your dataset to avoid errors in evaluations.', difficulty='advanced', intent='text')],\n",
       " 'usage': ResponseUsage(input_tokens=3445, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=589, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=4034)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in selected_documents:\n",
    "#     content = doc['content']\n",
    "#     num_questions = len(content) // 1000\n",
    "\n",
    "#     user_prompt = f\"\"\"\n",
    "#     generate {num_questions} questions for this document:\n",
    "\n",
    "#     {json.dumps(doc)}    \n",
    "#     \"\"\".strip()\n",
    "    \n",
    "#     output, usage = llm_structured(\n",
    "#         instructions=instructions,\n",
    "#         user_prompt=user_prompt,\n",
    "#         output_format=GeneratedQuestions\n",
    "#     )\n",
    "\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c69c3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for q in output.questions:\n",
    "#     print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2ac0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # usage = output[1]\n",
    "# usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9a45282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from toyaikit.pricing import PricingConfig\n",
    "\n",
    "# pricing = PricingConfig()\n",
    "# pricing.calculate_cost('gpt-4o-mini', usage.input_tokens, usage.output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5364dd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CostInfo(input_cost=0.0261153, output_cost=0.0147876, total_cost=0.0409029)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "pricing = PricingConfig()\n",
    "\n",
    "total_input = 0\n",
    "total_output = 0\n",
    "\n",
    "for res in all_results:\n",
    "    usage = res['usage']\n",
    "    total_input = total_input + usage.input_tokens\n",
    "    total_output = total_output + usage.output_tokens\n",
    "\n",
    "pricing.calculate_cost('gpt-4o-mini', total_input, total_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6178e218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc': {'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'content': 'To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\\n\\n- **Column types** (e.g., categorical, numerical, text).\\n- **Column roles** (e.g., id, prediction, target).\\n\\nThis allows Evidently to process the data correctly. Some evaluations need specific columns and will fail if they\\'re missing. You can define the mapping using the Python API or by assigning columns visually when uploading data to the Evidently platform.\\n\\n## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/descriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they’ll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated Mapping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping required for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<Info>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\\n    )\\n```\\n\\n### Regression\\n\\nTo run regression quality checks, you must map the columns with:\\n\\n- Target: actual values.\\n- Prediction: predicted values.\\n\\nYou can have several regression results in the dataset, for example in case of multiple regression. (Pass the mappings in a list).\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n    )\\n```\\n\\nDefaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```\\n\\n### Classification\\n\\nTo run classification checks, you must map the columns with:\\n\\n- Target: true label.\\n- Prediction: predicted labels/probabilities.\\n\\nThere two different mapping options, for binary and multi-class classification. You can also have several classification results in the dataset. (Pass the mappings in a list).\\n\\n#### Multiclass\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification=[MulticlassClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\",\\n        prediction_probas=[\"0\", \"1\", \"2\"],  # If probabilistic classification\\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}  # Optional, for display only\\n    )]\\n)\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: str = \"prediction\"\\n    prediction_probas: Optional[List[str]] = None #if probabilistic classification\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n<Note>\\n  When you have multiclass classification with predicted probabilities in separate columns, the column names in `prediction_probas` must exactly match the class labels. For example, if your classes are 0, 1, and 2, your probability columns must be named: \"0\", \"1\", \"2\". Values in `target` and `prediction` columns should be strings.\\n</Note>\\n\\n#### Binary\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import BinaryClassification\\n\\ndefinition = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " 'questions': [Question(question='data definition mapping example', summary_answer='The article provides code snippets for defining how to map input data using the `DataDefinition` class, including examples for text, numerical, and categorical columns.', difficulty='beginner', intent='code'),\n",
       "  Question(question='create Dataset object Evidently', summary_answer='To create a `Dataset` object in Evidently, you can use the `Dataset.from_pandas()` method with a data definition to specify the roles and types of your data columns.', difficulty='beginner', intent='code'),\n",
       "  Question(question='Evidently Dataset column roles', summary_answer='The article outlines various column roles in Evidently, including target and prediction columns, essential for accurate data evaluations.', difficulty='beginner', intent='text'),\n",
       "  Question(question='defining text columns Evidently', summary_answer='You can define text columns in the `DataDefinition` using the `text_columns` parameter to help the library correctly process LLM evaluations.', difficulty='intermediate', intent='code'),\n",
       "  Question(question='Evidently DataDefinition examples', summary_answer='The document provides multiple examples of specifying column types and roles using the `DataDefinition` class, including automatic and manual mapping options.', difficulty='intermediate', intent='code'),\n",
       "  Question(question='mapping regression target and prediction', summary_answer='To map regression columns in Evidently, use the `DataDefinition` class with the target and prediction parameters set appropriately for your evaluation.', difficulty='intermediate', intent='code'),\n",
       "  Question(question='definitions for binary classification', summary_answer='You can use the `BinaryClassification` mapping in the `DataDefinition` to specify target and prediction columns for binary classification tasks.', difficulty='intermediate', intent='code'),\n",
       "  Question(question='pandas DataFrame in Evidently', summary_answer='The article details how to work with pandas DataFrames directly in Evidently and emphasizes the importance of creating Dataset objects for clarity.', difficulty='beginner', intent='text'),\n",
       "  Question(question='column types in DataDefinition', summary_answer='It explains various column types like numerical, categorical, and text, and how to define them using the `DataDefinition` class.', difficulty='beginner', intent='text'),\n",
       "  Question(question='automatic mapping in Evidently', summary_answer='You can let Evidently automatically map columns by passing an empty `DataDefinition`, although manual mapping is recommended for accuracy.', difficulty='advanced', intent='text'),\n",
       "  Question(question='Evidently mapping roles and types', summary_answer='The article discusses how to accurately specify roles and types for each column in your dataset to avoid errors in evaluations.', difficulty='advanced', intent='text')],\n",
       " 'usage': ResponseUsage(input_tokens=3445, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=589, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=4034)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e747c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = []\n",
    "\n",
    "for res in all_results:\n",
    "    doc = res['doc']\n",
    "    questions = res['questions']\n",
    "    for q in questions:\n",
    "        q_dict = q.model_dump()\n",
    "        q_dict['filename'] = doc['filename']\n",
    "        all_questions.append(q_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3136a301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'data definition mapping example',\n",
       "  'summary_answer': 'The article provides code snippets for defining how to map input data using the `DataDefinition` class, including examples for text, numerical, and categorical columns.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'create Dataset object Evidently',\n",
       "  'summary_answer': 'To create a `Dataset` object in Evidently, you can use the `Dataset.from_pandas()` method with a data definition to specify the roles and types of your data columns.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'Evidently Dataset column roles',\n",
       "  'summary_answer': 'The article outlines various column roles in Evidently, including target and prediction columns, essential for accurate data evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'defining text columns Evidently',\n",
       "  'summary_answer': 'You can define text columns in the `DataDefinition` using the `text_columns` parameter to help the library correctly process LLM evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'Evidently DataDefinition examples',\n",
       "  'summary_answer': 'The document provides multiple examples of specifying column types and roles using the `DataDefinition` class, including automatic and manual mapping options.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'mapping regression target and prediction',\n",
       "  'summary_answer': 'To map regression columns in Evidently, use the `DataDefinition` class with the target and prediction parameters set appropriately for your evaluation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'definitions for binary classification',\n",
       "  'summary_answer': 'You can use the `BinaryClassification` mapping in the `DataDefinition` to specify target and prediction columns for binary classification tasks.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'pandas DataFrame in Evidently',\n",
       "  'summary_answer': 'The article details how to work with pandas DataFrames directly in Evidently and emphasizes the importance of creating Dataset objects for clarity.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'column types in DataDefinition',\n",
       "  'summary_answer': 'It explains various column types like numerical, categorical, and text, and how to define them using the `DataDefinition` class.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'automatic mapping in Evidently',\n",
       "  'summary_answer': 'You can let Evidently automatically map columns by passing an empty `DataDefinition`, although manual mapping is recommended for accuracy.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'Evidently mapping roles and types',\n",
       "  'summary_answer': 'The article discusses how to accurately specify roles and types for each column in your dataset to avoid errors in evaluations.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'create descriptors for text evaluations',\n",
       "  'summary_answer': 'Descriptors are created to evaluate text data by computing scores or labels for each row in your dataset, enabling flexible assessments using built-in or custom definitions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'how to run text evaluations with Python',\n",
       "  'summary_answer': 'Running text evaluations involves importing necessary modules and setting up a Dataset object with descriptors, which can then be used to generate reports and analyze text input/output.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'examples of using Sentiment descriptor',\n",
       "  'summary_answer': 'The Sentiment descriptor can be added to evaluate the sentiment of text responses by setting it up with the Dataset object and using the appropriate alias for results.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'combine multiple descriptors in evaluations',\n",
       "  'summary_answer': 'You can combine multiple descriptors in evaluations by adding them to the Dataset object, allowing for thorough analysis of various text metrics at once.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'custom descriptors in text evaluation',\n",
       "  'summary_answer': 'Custom descriptors can be created using LLM prompts or Python logic to cater to specific evaluation needs beyond the built-in options provided.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'testing conditions with descriptor tests',\n",
       "  'summary_answer': 'Descriptor tests allow you to define pass/fail criteria for each row in your evaluation dataset, enhancing the analysis by checking conditions like character limits or sentiment thresholds.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'basic flow of text evaluations',\n",
       "  'summary_answer': 'The basic flow includes generating sample data, importing necessary modules, adding descriptors to the Dataset, and producing reports using those descriptors.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'reporting capabilities in text data evaluation',\n",
       "  'summary_answer': 'The article discusses how to generate and customize reports based on various descriptors used in the evaluation process, including metrics like sentiment and length.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'how to create a Dataset for text evaluation',\n",
       "  'summary_answer': 'Creating a Dataset involves using sample text data and defining the text columns along with the desired descriptors for comprehensive evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'using LLM as a judge for text evaluations',\n",
       "  'summary_answer': 'LLM-based descriptors can be integrated into the evaluations to return scores, requiring specific API keys for functionality during the evaluation process.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'DataFrame preview of evaluation results',\n",
       "  'summary_answer': 'You can preview the results of evaluations in DataFrame format, which provides a structured overview of computed metrics for each descriptor applied to your text data.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'advanced text evaluation tests',\n",
       "  'summary_answer': 'The article outlines how to create advanced tests at the dataset level, providing a framework for automated checks on evaluation metrics across multiple analyses.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'Evidently evaluation workflow step by step',\n",
       "  'summary_answer': 'The article outlines the core workflow for running evaluations using the Evidently library, detailing the preparation of input data and the creation of dataset objects.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/evaluations_overview.mdx'},\n",
       " {'question': 'How to create a Dataset object in Evidently',\n",
       "  'summary_answer': 'The article describes how to create a Dataset object using `DataDefinition()` to specify column roles and types, along with code examples for implementation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/evaluations_overview.mdx'},\n",
       " {'question': 'Configuring reports in Evidently evaluations',\n",
       "  'summary_answer': 'It provides guidance on how to create and configure reports for dataset-level evaluations and includes example code for setting metrics and presets.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/evaluations_overview.mdx'},\n",
       " {'question': 'generate metrics using ColumnMetricGenerator',\n",
       "  'summary_answer': 'The article explains how to use the ColumnMetricGenerator to apply metrics like ValueDrift to all or specific columns in a dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/metric_generator.mdx'},\n",
       " {'question': 'apply ValueDrift metric to selected columns',\n",
       "  'summary_answer': 'It provides a detailed example of how to apply the ValueDrift metric to selected columns with customizable parameters, illustrating the use of metric_kwargs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/metric_generator.mdx'},\n",
       " {'question': 'export evaluation results formats',\n",
       "  'summary_answer': 'The article provides different ways to export evaluation results, including formats like HTML, JSON, and Python dictionaries with practical code examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/output_formats.mdx'},\n",
       " {'question': 'view reports in Jupyter notebook',\n",
       "  'summary_answer': 'You can render evaluation results directly in Jupyter notebooks by calling the resulting Python object, which displays the output inline.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/output_formats.mdx'},\n",
       " {'question': 'save report as JSON',\n",
       "  'summary_answer': 'To save evaluation results in JSON format, you can use the provided code to view and save them as a JSON file for further use.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/output_formats.mdx'},\n",
       " {'question': 'output formats options',\n",
       "  'summary_answer': 'The article discusses various output format options for evaluation results, including visual and data-centric approaches.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/output_formats.mdx'},\n",
       " {'question': 'export report to HTML file example',\n",
       "  'summary_answer': 'There’s a code snippet in the article that shows how to save a report as an HTML file for easy sharing and viewing in a browser.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/output_formats.mdx'},\n",
       " {'question': 'get Python dictionary from report',\n",
       "  'summary_answer': 'You can obtain the evaluation report as a Python dictionary, which helps in using the results for automated processing in data pipelines.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/output_formats.mdx'},\n",
       " {'question': 'local workspace report saving',\n",
       "  'summary_answer': 'The article outlines how to save reports to both your local workspace and Evidently Cloud, including specific code examples for each.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/output_formats.mdx'},\n",
       " {'question': 'uploading evals quickstart',\n",
       "  'summary_answer': 'It mentions checking Quickstart examples for machine learning and LLM workflows, offering a comprehensive view of the reporting process.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/output_formats.mdx'},\n",
       " {'question': 'Evidently Python library features',\n",
       "  'summary_answer': 'The Evidently Python library features 100+ metrics for evaluating AI systems, including checks for data quality, performance, and visual reporting.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Evidently library metrics examples',\n",
       "  'summary_answer': 'Examples of metrics in the Evidently library include precision, recall, MAE, and custom LLM judges to assess AI output quality.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'How to run evaluations with Evidently',\n",
       "  'summary_answer': 'You can run evaluations using the Evidently library by preparing your data as a pandas DataFrame and creating a Dataset object.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Quickstart Evidently setup',\n",
       "  'summary_answer': 'The quickstart guide provides steps to set up the Evidently library for LLM and ML evaluation, including installation instructions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'What is a Dataset object in Evidently?',\n",
       "  'summary_answer': 'A Dataset object in Evidently allows you to structure your data with metadata for proper processing during evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Creating visual reports in Evidently',\n",
       "  'summary_answer': 'You can create visual reports in Evidently by generating metrics summaries and exporting them as HTML or using Jupyter notebooks.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Evidently data quality checks',\n",
       "  'summary_answer': 'Evidently provides data quality checks such as detecting missing values, duplicates, and ensuring proper data distributions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Using Descriptors in Evidently',\n",
       "  'summary_answer': 'Descriptors in Evidently are row-level scores that assess specific text qualities, like sentiment or length, used for evaluating LLM outputs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Exporting evaluation results in Evidently',\n",
       "  'summary_answer': 'Evidently allows you to export evaluation results in multiple formats including JSON, Python dictionaries, or as DataFrames.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Tracking evaluations over time with Evidently',\n",
       "  'summary_answer': 'Evidently includes a UI for tracking and comparing evaluation results over time, allowing you to visualize changes in metrics.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'How to handle data drift with Evidently',\n",
       "  'summary_answer': 'Manage data drift by comparing current datasets with reference datasets to detect distribution shifts effectively.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Integrating Evidently into pipelines',\n",
       "  'summary_answer': 'Evidently can be easily integrated into existing workflows by leveraging its export functionalities for scores and visual reports.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Test Suites in Evidently',\n",
       "  'summary_answer': 'Test Suites in Evidently are collections of tests that validate evaluation results against specified conditions for AI systems.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Creating custom metrics in Evidently',\n",
       "  'summary_answer': 'You can define custom metrics in Evidently by combining built-in metrics and writing your own evaluation logic.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Comparing two datasets in Evidently',\n",
       "  'summary_answer': 'You can prepare two datasets in Evidently for side-by-side comparison to analyze model performance or detect data drift.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Evidently library automated prompt optimization',\n",
       "  'summary_answer': 'The Evidently library provides tools for optimizing prompts using feedback and various evaluation metrics automatically.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'What is a Report in Evidently?',\n",
       "  'summary_answer': 'A Report in Evidently structures evaluations and summarizes results, visualizing metrics and tests for datasets or columns.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Using LLM outputs in Evidently',\n",
       "  'summary_answer': 'Evidently supports the evaluation of LLM outputs by utilizing metrics specially designed for semantic quality assessments.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Synthetic data generation with Evidently',\n",
       "  'summary_answer': 'The library includes features for generating synthetic datasets, especially useful for testing AI applications and RAG processes.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Best practices for data preparation in Evidently',\n",
       "  'summary_answer': 'For effective evaluation in Evidently, prepare your data as a pandas DataFrame with relevant features and ground truth where applicable.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Evidently platform deployment options',\n",
       "  'summary_answer': 'Users can choose between self-hosting the open-source platform or signing up for the managed Evidently Cloud service for additional features.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'generate report using Evidently',\n",
       "  'summary_answer': 'This article provides a comprehensive guide on generating reports using the Evidently library, from basic setup to creating custom reports with various metrics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'question': 'Evidently report metrics example',\n",
       "  'summary_answer': 'Examples of using different metrics in report generation are provided, showcasing how to customize reports with both dataset-level and column-level metrics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'question': 'how to use DataDriftPreset in report',\n",
       "  'summary_answer': 'Instructions on using the DataDriftPreset to evaluate data against a reference dataset are included, describing how to pass datasets during report generation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'question': 'custom reports with Evidently',\n",
       "  'summary_answer': 'The article describes how to create custom reports by selecting specific metrics and presets, offering code snippets for various scenarios.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'question': 'add metadata to evaluation reports',\n",
       "  'summary_answer': 'The article outlines methods for adding metadata to evaluation reports to facilitate filtering and searching, with code examples provided.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tags_metadata.mdx'},\n",
       " {'question': 'custom tags in reports example',\n",
       "  'summary_answer': 'Custom tags can be added to reports to classify and filter evaluation runs, as demonstrated in the provided code snippets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tags_metadata.mdx'},\n",
       " {'question': 'conditional checks in data validation',\n",
       "  'summary_answer': 'The article discusses how to use Tests for validating conditions at the dataset level, providing Pass/Fail outcomes.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'evidently Tests example',\n",
       "  'summary_answer': 'It provides Python code examples for using Tests in Evidently, such as importing necessary modules and running validations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'how to set custom tests in Evidently',\n",
       "  'summary_answer': 'The article explains how to define specific pass/fail conditions for each Test, using various parameters in Python code.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'using preset tests in Evidently',\n",
       "  'summary_answer': 'Preset Tests automatically generate a suite of Tests, which can be enabled in the Report to evaluate data quality.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'different types of Tests in Evidently',\n",
       "  'summary_answer': 'The article outlines three ways to run Tests: using presets, defaults, and custom conditions, detailing usage scenarios for each.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'evidently import modules',\n",
       "  'summary_answer': 'To leverage Tests, you need to import specific modules from the Evidently library, as shown in the code examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'how to validate missing values in Python',\n",
       "  'summary_answer': 'The article includes examples of validating missing values in specific columns using the MissingValueCount metric in Evidently.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'test conditions and parameters overview',\n",
       "  'summary_answer': 'It details various test conditions and parameters that can be used to set up tests, including equality and range checks.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'Run Tests with reference dataset',\n",
       "  'summary_answer': 'The article illustrates how to run Tests using a reference dataset to enable comparative validations between datasets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'setting alerts in Evidently',\n",
       "  'summary_answer': 'The article explains how to set up alerts in Evidently Cloud and Enterprise, focusing on choosing notification channels and defining alert conditions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/alerts.mdx'},\n",
       " {'question': 'Evidently alert notification options',\n",
       "  'summary_answer': 'It outlines available notification channels like Email, Slack, and Discord for receiving alerts when conditions are met.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/alerts.mdx'},\n",
       " {'question': 'how to configure alerts for Test failures in Evidently',\n",
       "  'summary_answer': 'The guide describes how to tie alerts to failed tests in a project, ensuring notifications are sent when conditions specified are met.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/alerts.mdx'},\n",
       " {'question': 'Custom conditions for alerts in Evidently',\n",
       "  'summary_answer': 'The article details how to set custom alerts based on specific metric values or conditions, enabling tailored monitoring of project metrics.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/alerts.mdx'},\n",
       " {'question': 'using is_critical parameter in alerts',\n",
       "  'summary_answer': 'It mentions how to use the `is_critical` parameter to manage alert levels and avoid unnecessary notifications for non-critical tests.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/alerts.mdx'},\n",
       " {'question': 'add dashboard panel python api',\n",
       "  'summary_answer': 'The article provides detailed Python code examples for adding different types of panels to a dashboard using its API.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'delete dashboard tab panel example',\n",
       "  'summary_answer': 'You can delete tabs and specific panels from your dashboard using clear examples found in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'create dashboard layout with panels',\n",
       "  'summary_answer': 'Find out how to organize your dashboard by adding and arranging custom panels through the provided API instructions.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'python add multiple dashboard panels',\n",
       "  'summary_answer': 'The article shows how to add multiple panels at once, along with their configurations through code samples.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'dashboard panel plotting options',\n",
       "  'summary_answer': 'Learn about different plot types available for dashboard panels such as line charts and bar charts, with code examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'customize dashboard panel settings',\n",
       "  'summary_answer': 'Configuration options for dashboard panels, including title, size, and plot parameters, are detailed in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'examples of text panel on dashboard',\n",
       "  'summary_answer': 'The article includes examples of how to add text-only panels for titles and descriptions in your dashboard.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'dashboard panel metrics and labels',\n",
       "  'summary_answer': 'Information on how to define metrics and their labels for dashboard panels is thoroughly explained in the document.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'add pie charts to dashboard panel',\n",
       "  'summary_answer': 'The article provides example code for adding pie charts to your dashboard, showing how to use different aggregation parameters.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'panel metric parameters summary',\n",
       "  'summary_answer': 'A summary table describing the parameters for PanelMetric, including required and optional fields, is provided for quick reference.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'python api remove all dashboard panels',\n",
       "  'summary_answer': 'To clear all tabs and panels on the dashboard, the article shows the appropriate method to accomplish this action using code.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'dashboard panel design considerations',\n",
       "  'summary_answer': 'Understand the design elements to consider when setting up your dashboard panels, including layout options and visual settings.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'dashboard panel type options',\n",
       "  'summary_answer': 'The article outlines various panel types (text, counter, plot) available for usage in your dashboard and how to implement them.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'create dashboard panels tutorial',\n",
       "  'summary_answer': 'The article provides a comprehensive guide on creating and configuring dashboard panels in Evidently, including adding tabs and customizing visualizations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'question': 'how to add a tab in dashboard Evidently',\n",
       "  'summary_answer': 'Instructions for adding tabs to organize panels on a dashboard are detailed, including entering edit mode and creating custom tabs.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'question': 'panel configuration options Evidently',\n",
       "  'summary_answer': 'The article outlines panel configuration options, including selecting metrics, filtering by tags, and setting the panel type for dashboards.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'question': 'delete dashboard panel procedure',\n",
       "  'summary_answer': 'The steps to delete or edit a dashboard panel are provided, including how to enter edit mode and manage individual panels.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'question': 'dashboard overview and features',\n",
       "  'summary_answer': 'The article outlines the key features of dashboards in Evidently, including tracking application performance and organizing data through multiple tabs and panels.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_overview.mdx'},\n",
       " {'question': 'how to create dashboard panels in Evidently',\n",
       "  'summary_answer': 'The article explains the process of creating dashboard panels, either via the Python API or the user interface, including selecting metrics and specifying panel parameters.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_overview.mdx'},\n",
       " {'question': 'creating datasets in Evidently',\n",
       "  'summary_answer': 'The article outlines various ways to create datasets in the Evidently platform, including direct uploads and synthetic data generation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/datasets_overview.mdx'},\n",
       " {'question': 'what are synthetic datasets in Evidently?',\n",
       "  'summary_answer': 'Synthetic datasets in Evidently are generated through built-in platform features, allowing users to create data based on specific scenarios or source documents.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/datasets_overview.mdx'},\n",
       " {'question': 'upload dataset to Evidently',\n",
       "  'summary_answer': 'The article provides instructions for uploading a dataset in Python and via the UI, detailing the necessary methods and parameters to use.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/datasets_workflow.mdx'},\n",
       " {'question': 'Evidently dataset creation process',\n",
       "  'summary_answer': 'It outlines the steps to prepare and upload datasets, emphasizing the importance of creating a data definition for optimal usability.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/datasets_workflow.mdx'},\n",
       " {'question': 'API to run evals in Evidently',\n",
       "  'summary_answer': 'The article explains how to run evals using the Evidently API and upload results to the platform.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_api.mdx'},\n",
       " {'question': 'how to upload reports in Evidently API',\n",
       "  'summary_answer': 'It covers the steps for uploading reports, either with raw data or just the evaluation results, using the Evidently cloud service.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_api.mdx'},\n",
       " {'question': 'view evaluation results on platform',\n",
       "  'summary_answer': \"The article outlines how to browse evaluation results by accessing the 'Reports' section of your project on the platform.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/evals_explore.mdx'},\n",
       " {'question': 'compare evaluation reports',\n",
       "  'summary_answer': \"You can analyze multiple evaluation results side by side by selecting them from the report list and clicking the 'Compare' button.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_explore.mdx'},\n",
       " {'question': 'download evaluation reports HTML JSON',\n",
       "  'summary_answer': 'The reports can be downloaded in either HTML or JSON format directly from the Reports section.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_explore.mdx'},\n",
       " {'question': 'how to hide dataset in explore view',\n",
       "  'summary_answer': \"In the explore view, clicking the 'Dataset' sign at the top allows you to hide the dataset and view the report only.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_explore.mdx'},\n",
       " {'question': 'multiple evaluations dashboard',\n",
       "  'summary_answer': 'You can create a dashboard to track progress and visualize results over time from multiple evaluation reports within a project.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/evals_explore.mdx'},\n",
       " {'question': 'no code data evaluation tutorial',\n",
       "  'summary_answer': 'The article explains how to evaluate data without coding by using a graphical user interface to prepare datasets and run evaluations effortlessly.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_no_code.mdx'},\n",
       " {'question': 'how to upload CSV for evaluation',\n",
       "  'summary_answer': 'It outlines the steps to upload a CSV file for data evaluation, including specifying data definitions upon upload.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_no_code.mdx'},\n",
       " {'question': 'LLM evaluator configuration',\n",
       "  'summary_answer': 'The article details how to configure a custom LLM evaluator within a no-code platform, allowing users to utilize machine learning models for text evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_no_code.mdx'},\n",
       " {'question': 'understanding descriptors in data evaluation',\n",
       "  'summary_answer': 'It provides an overview of descriptors used in evaluations, including how they measure and label text data within the evaluation process.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/evals_no_code.mdx'},\n",
       " {'question': 'evaluator workflows AI product',\n",
       "  'summary_answer': 'The article describes various workflows for evaluations such as ad hoc analysis, experiments, and safety testing to ensure the quality of AI outputs during development stages.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/evals_overview.mdx'},\n",
       " {'question': 'run evaluations API Evidently',\n",
       "  'summary_answer': 'You can run evaluations using the Evidently API, which supports techniques for experiments and CI/CD workflows, enabling you to analyze and compare AI output results efficiently.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_overview.mdx'},\n",
       " {'question': 'evidently batch monitoring setup',\n",
       "  'summary_answer': 'The article explains how to set up batch monitoring using the Evidently Python library, including connecting to Evidently Cloud and creating a project.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/monitoring_local_batch.mdx'},\n",
       " {'question': 'python batch evaluation example',\n",
       "  'summary_answer': 'It provides a simple example of running batch evaluations in Python, demonstrating how to get dataset stats and upload results to your workspace.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/monitoring_local_batch.mdx'},\n",
       " {'question': 'AI quality monitoring overview',\n",
       "  'summary_answer': 'The article explains how AI observability helps evaluate the quality of AI applications in production, ensuring system behavior is monitored effectively.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/monitoring_overview.mdx'},\n",
       " {'question': 'batch monitoring jobs in AI',\n",
       "  'summary_answer': 'It details setting up batch monitoring jobs for ML pipelines using the Evidently platform, emphasizing how to run evaluations and visualize results.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/monitoring_overview.mdx'},\n",
       " {'question': 'tracing data in production',\n",
       "  'summary_answer': 'The article covers how to instrument applications for tracing relevant data using the Tracely library, including storing raw data and scheduling evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/monitoring_overview.mdx'},\n",
       " {'question': 'Evidently Platform features overview',\n",
       "  'summary_answer': 'The article outlines key features of the Evidently Platform, including evaluation tracking, dataset management, and monitoring options for AI systems.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/overview.mdx'},\n",
       " {'question': 'Using Evidently Python library for evaluations',\n",
       "  'summary_answer': 'The article explains how to use the Evidently Python library for local evaluations and programmatic access to the platform, enabling users to run tests and upload datasets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/overview.mdx'},\n",
       " {'question': 'creating a project in Evidently with Python',\n",
       "  'summary_answer': 'The article provides Python code examples for creating a project in Evidently, including how to set the organization ID and project description.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/projects_manage.mdx'},\n",
       " {'question': 'how to connect to an existing project in Evidently',\n",
       "  'summary_answer': 'It explains how to use the `get_project` method to connect to an existing project by its Project ID.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/projects_manage.mdx'},\n",
       " {'question': 'list projects in Evidently workspace',\n",
       "  'summary_answer': 'The article describes the method `ws.list_projects()` to retrieve all projects available in a given workspace.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/projects_manage.mdx'},\n",
       " {'question': 'project parameters in Evidently',\n",
       "  'summary_answer': 'It includes a detailed table of project parameters, such as name, ID, and description, explaining their roles in project management.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/projects_manage.mdx'},\n",
       " {'question': 'what is a project in evidently',\n",
       "  'summary_answer': 'A Project in Evidently helps organize data and evaluations tailored for specific use cases, streamlining management and access to datasets, reports, and dashboards.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/projects_overview.mdx'},\n",
       " {'question': 'how to organize projects in evidently',\n",
       "  'summary_answer': 'You can structure your Projects in Evidently by application, model, components, test scenarios, or phases to match your workflow and needs.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/projects_overview.mdx'},\n",
       " {'question': 'project components in evidently',\n",
       "  'summary_answer': 'Each Project contains datasets, reports, traces, a dashboard, and alerting rules, serving as a hub for data management.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/projects_overview.mdx'},\n",
       " {'question': 'evidently project unique ID usage',\n",
       "  'summary_answer': 'The unique ID of a Project in Evidently allows for connecting via the Python API to manage data, dashboards, and configurations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/projects_overview.mdx'},\n",
       " {'question': 'best practices for organizing projects',\n",
       "  'summary_answer': 'For optimal organization, separate Projects by specific applications, test scenarios, or phases of development to ensure clarity and manageability.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/projects_overview.mdx'},\n",
       " {'question': 'LLM tracing overview',\n",
       "  'summary_answer': 'The article explains LLM tracing as a method to instrument AI applications, enabling detailed data collection for evaluation and analysis, including inputs, outputs, and execution steps.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_overview.mdx'},\n",
       " {'question': 'how to implement LLM tracing with Evidently',\n",
       "  'summary_answer': 'Evidently uses the Tracely library for LLM tracing, allowing users to define what data to capture and explore it through various views like trace, dataset, and dialogue.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_overview.mdx'},\n",
       " {'question': 'benefits of using tracing in AI applications',\n",
       "  'summary_answer': 'Tracing is useful for monitoring and understanding complex execution flows in AI applications, especially during experiments and in production environments.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_overview.mdx'},\n",
       " {'question': 'Evidently tracing features',\n",
       "  'summary_answer': 'The article outlines Pro features in Evidently for tracing, including trace store and viewer facilities available in the cloud and enterprise versions.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_overview.mdx'},\n",
       " {'question': 'using OpenTelemetry for tracing',\n",
       "  'summary_answer': 'The article describes how the Tracely library, based on OpenTelemetry, is utilized for implementing tracing in LLM applications to capture execution data.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_overview.mdx'},\n",
       " {'question': 'install tracely package',\n",
       "  'summary_answer': 'Instructions for installing the Tracely package are provided using pip with a simple command: `pip install tracely`.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'initialize tracing in tracely',\n",
       "  'summary_answer': \"To initiate tracing, the article explains how to use the `init_tracing` function with various necessary parameters like 'address', 'api_key', and 'project_id'.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'tracely init_tracing function arguments',\n",
       "  'summary_answer': 'The article outlines the arguments for the `init_tracing` function, detailing their purposes and corresponding environment variables for configuration.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'trace_event decorator usage',\n",
       "  'summary_answer': 'The document provides examples of how to use the `trace_event` decorator to track function calls and their arguments for tracing purposes.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'nested tracing with tracely',\n",
       "  'summary_answer': 'It explains how to implement nested tracing using the `@trace_event` decorator for multi-step workflows in LLM applications.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'using context managers for tracing',\n",
       "  'summary_answer': 'The article describes using context managers, specifically `create_trace_event`, for fine-grained control over tracing code execution without decorators.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'session_id for trace events',\n",
       "  'summary_answer': 'Guidance is given on how to use a shared `session_id` to group trace events across different functions or threads.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'get_current_span method in tracely',\n",
       "  'summary_answer': 'An explanation is provided for the `get_current_span()` method, which allows the addition of attributes to the current tracing span.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'setting custom attributes in tracing',\n",
       "  'summary_answer': 'The article details how to set custom attributes and results for tracing events, enhancing the data captured during application execution.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'manage trace_id in tracely',\n",
       "  'summary_answer': 'It emphasizes the importance of managing the trace_id manually to ensure uniqueness when linking events across different systems.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'create Evidently Cloud account',\n",
       "  'summary_answer': 'The article outlines the process for signing up for a free Evidently Cloud account and setting up an organization once logged in.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/cloud.mdx'},\n",
       " {'question': 'how to connect to Evidently Cloud with Python',\n",
       "  'summary_answer': 'It explains how to connect to Evidently Cloud using the Python library and requires an API token for access.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/cloud.mdx'},\n",
       " {'question': 'install Evidently Python library',\n",
       "  'summary_answer': 'Instructions for installing the Evidently Python library via pip are provided to facilitate connection with the cloud service.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/cloud.mdx'},\n",
       " {'question': 'get API token for Evidently Cloud',\n",
       "  'summary_answer': 'The article describes how to generate and securely save your API token from the Evidently Cloud interface.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/setup/cloud.mdx'},\n",
       " {'question': 'set up environment variable for Evidently API key',\n",
       "  'summary_answer': 'You can set the `EVIDENTLY_API_KEY` environment variable for easier access to the API key during your programmatic tasks.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/cloud.mdx'},\n",
       " {'question': 'self-hosting Evidently UI setup',\n",
       "  'summary_answer': 'The article explains how to self-host the Evidently UI service, detailing the necessary steps to create and manage workspaces.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'create local workspace Evidently UI',\n",
       "  'summary_answer': 'To create a local workspace for Evidently UI, you can use the command `ws = Workspace.create(\"evidently_ui_workspace\")` after installation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'how to launch Evidently UI service',\n",
       "  'summary_answer': 'To launch the Evidently UI service, you can run the command `evidently ui` from the terminal, specifying the workspace as needed.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'remote workspace configuration Evidently UI',\n",
       "  'summary_answer': 'The article details how to configure a remote workspace by using the `RemoteWorkspace` class and specifying the server URL.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'delete workspace in Evidently',\n",
       "  'summary_answer': 'To delete a workspace in Evidently, you must run the command `rm -r workspace`, but be cautious as it deletes all data stored in that workspace.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'Evidently GitHub Actions integration example',\n",
       "  'summary_answer': 'The article provides a clear example of how to integrate Evidently with GitHub Actions to test LLM outputs automatically during code pushes or pull requests.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/GitHub_actions.mdx'},\n",
       " {'question': 'LLM evaluation methods overview',\n",
       "  'summary_answer': 'The article offers a detailed overview of various LLM evaluation methods, including both reference-based and reference-free techniques.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_evals.mdx'},\n",
       " {'question': 'code examples for LLM evaluation',\n",
       "  'summary_answer': 'The article includes code tutorials and interactive notebooks that demonstrate how to implement different LLM evaluation methods effectively.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_evals.mdx'},\n",
       " {'question': 'create LLM judge tutorial',\n",
       "  'summary_answer': 'The article offers a step-by-step guide on how to create and evaluate an LLM judge with Python code examples for reference-based and open-ended evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'evaluate LLM model',\n",
       "  'summary_answer': 'It explains how to evaluate the performance of an LLM model by comparing its outputs against manual labels and grounding truth responses.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'Python code for LLM evaluation',\n",
       "  'summary_answer': 'The article includes specific Python code snippets for setting up and running an LLM as a judge, including dataset preparation and evaluation functions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'LLM judge reference-based evaluation',\n",
       "  'summary_answer': 'It discusses the reference-based method for evaluating LLM outputs by comparing them to approved responses, which is useful for regression testing.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'how to evaluate custom criteria with LLM',\n",
       "  'summary_answer': 'The article describes how to utilize custom criteria for evaluating LLM responses, particularly useful when no reference output exists.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'install Evidently for LLM',\n",
       "  'summary_answer': 'Instructions for installing the Evidently library to use for creating and evaluating LLM judges are provided within the tutorial.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'create evaluation dataset for LLM',\n",
       "  'summary_answer': 'It details how to create a toy dataset for evaluation purposes, providing example questions and corresponding responses.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'optimization of LLM prompts',\n",
       "  'summary_answer': 'The article highlights strategies to optimize prompts for LLM judges to improve evaluation accuracy and relevance.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'using OpenAI API for LLM',\n",
       "  'summary_answer': 'It offers instructions on how to use the OpenAI API key to power LLM evaluation processes within the tutorial.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'LLM judge accuracy evaluation',\n",
       "  'summary_answer': 'Insights are provided on how to evaluate the accuracy of your LLM judge compared to manual labels through classification metrics.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'verbosity checks in LLM evaluations',\n",
       "  'summary_answer': \"Describes the implementation of a verbosity evaluator to assess how concise the LLM's responses are.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'explain basic Python for LLM evaluation',\n",
       "  'summary_answer': 'The article assumes basic Python knowledge, making it accessible to users with foundational programming skills who want to work with LLMs.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'extend LLM capabilities using Evidently',\n",
       "  'summary_answer': 'It outlines methods to enhance LLM functionalities through integration with the Evidently platform for advanced evaluation workflows.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'setup Jupyter Notebook for LLM',\n",
       "  'summary_answer': 'Recommendations for running the tutorial in a Jupyter Notebook or Google Colab for optimal output display are discussed.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'analyze LLM evaluator results',\n",
       "  'summary_answer': 'The article explains how to generate reports from the LLM evaluations and understand key metrics like precision and recall.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'create binary classification with LLM',\n",
       "  'summary_answer': 'Instructions are provided for setting up binary classification prompts in LLMs to provide accurate evaluations of responses.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'add descriptors to LLM dataset',\n",
       "  'summary_answer': 'It covers how to add descriptors to the evaluation dataset, enhancing the analysis of LLM outputs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'reporting and visualizing LLM evaluations',\n",
       "  'summary_answer': 'Guidance is given on how to produce visual reports for effective presentation of LLM evaluation outcomes.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'requirements for LLM evaluation tutorial',\n",
       "  'summary_answer': 'Lists the prerequisites, including needing basic Python skills and access to the OpenAI API for following the tutorial effectively.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'meta-evaluation of LLM judges',\n",
       "  'summary_answer': 'The article discusses how to evaluate the quality of your LLM judge by treating it as a binary classification problem.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'evaluate model outputs with multiple LLMs',\n",
       "  'summary_answer': 'The article describes a technique for using multiple LLMs to collectively evaluate the outputs, allowing for a more reliable assessment of generated content.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'LLM jury example code',\n",
       "  'summary_answer': 'A code example in Jupyter notebook format demonstrates how to set up multiple LLMs as evaluators for outputs in email generation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'combine LLM evaluations',\n",
       "  'summary_answer': 'You can aggregate multiple evaluations from different LLMs to determine a consensus on generated content, as outlined in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'instructions for setting up LLM evaluators',\n",
       "  'summary_answer': 'The article provides detailed instructions on how to configure LLMs as judges, including API key setup and evaluation prompts.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'disagreements among LLM outputs',\n",
       "  'summary_answer': 'The article explains how to identify and flag disagreements among the LLM evaluations via a custom descriptor.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'install evidently library for LLMs',\n",
       "  'summary_answer': 'Basic instructions are provided for installing the Evidently library necessary for LLM evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'using BinaryClassificationPromptTemplate',\n",
       "  'summary_answer': 'The article discusses how to implement the BinaryClassificationPromptTemplate for defining evaluation criteria for email appropriateness.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'configure LLMs with diverse providers',\n",
       "  'summary_answer': 'It explains how to set up evaluators using different LLM providers like OpenAI and Anthropic for a broader assessment.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'viewing LLM evaluation results',\n",
       "  'summary_answer': 'Instructions are provided on how to export and visualize results of LLM evaluations both locally and on Evidently Cloud.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'evaluate RAG system metrics',\n",
       "  'summary_answer': 'The article discusses various metrics to evaluate Retrieval-Augmented Generation (RAG) systems using Evidently, focusing on retrieval and generation quality.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'install Evidently library',\n",
       "  'summary_answer': 'Instructions for installing the Evidently library are provided, including necessary imports for evaluating RAG systems.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'evaluate context quality RAG',\n",
       "  'summary_answer': 'Learn how to assess retrieval quality by checking the relevance of retrieved contexts for each query in a RAG system.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'generate pandas dataframe for RAG',\n",
       "  'summary_answer': \"The article demonstrates how to create a pandas dataframe to simulate a dataset for evaluating a RAG model's performance.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'Evidently RAG evaluation report',\n",
       "  'summary_answer': \"A guide is provided for creating a report summarizing the evaluation results of a RAG model's performance using the Evidently library.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'assess generation quality in RAG',\n",
       "  'summary_answer': 'The article explains methods to evaluate the quality of generated responses in a Retrieval-Augmented Generation system, with or without ground truth.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'context relevance metric in RAG',\n",
       "  'summary_answer': 'Learn about using the ContextRelevance metric to score the relevance of multiple contexts retrieved for queries in a RAG system.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'Evidently Cloud usage for RAG',\n",
       "  'summary_answer': 'Instructions on how to upload evaluation runs to the Evidently Cloud for tracking and comparing evaluation results of RAG systems.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'synthetic dataset for RAG',\n",
       "  'summary_answer': \"The article explains how to create a synthetic dataset for testing and evaluating a RAG system's performance.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'define test conditions RAG',\n",
       "  'summary_answer': 'Instructions on how to set up explicit pass/fail tests for metrics evaluated in a RAG system to monitor performance.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'visual reporting in Jupyter for RAG',\n",
       "  'summary_answer': 'The article highlights how to visualize evaluation results as a report or pandas dataframe in Jupyter or Colab environments.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'LLM evaluation methods RAG',\n",
       "  'summary_answer': 'Various methods for evaluating the performance of generated responses against ground truth data are discussed, including LLM-based metrics.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'upload evaluation results to Evidently Cloud',\n",
       "  'summary_answer': 'The article details how to upload evaluation results to the Evidently Cloud platform to track and analyze RAG system performance over time.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'LLM regression testing tutorial',\n",
       "  'summary_answer': 'The article provides a step-by-step guide on how to perform regression testing for LLM outputs using Python and Evidently Cloud.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'Python code for LLM regression testing',\n",
       "  'summary_answer': 'This article includes Python code samples for setting up LLM regression tests, including installing required libraries and creating test metrics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'how to compare LLM outputs',\n",
       "  'summary_answer': 'It explains how to compare old and new LLM outputs after changing parameters to identify significant changes.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'setting up Evidently Cloud for LLM testing',\n",
       "  'summary_answer': 'The tutorial outlines how to set up an Evidently Cloud account and connect it for running tests on LLM outputs.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'create project for LLM regression testing',\n",
       "  'summary_answer': 'Instructions are provided on how to create a project in Evidently Cloud for managing regression tests for LLM outputs.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'generate toy dataset for testing LLM',\n",
       "  'summary_answer': 'The article walks through creating a toy dataset for testing by defining questions and their corresponding responses.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'running evaluations on new LLM responses',\n",
       "  'summary_answer': 'It covers how to run evaluations on new LLM responses using predefined metrics to assess correctness and style.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'correctness checks for LLM outputs',\n",
       "  'summary_answer': 'This article details how to implement correctness checks using an LLM judge to classify new responses as correct or incorrect.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'understanding LLM style matching',\n",
       "  'summary_answer': 'It explains how to create a judge for style matching in LLM outputs, focusing on consistency in tone and structure.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'add descriptors for LLM tests',\n",
       "  'summary_answer': 'The tutorial illustrates how to add descriptors to a dataset to evaluate various aspects of LLM outputs through automated checks.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'Evidently report setup for LLM testing',\n",
       "  'summary_answer': 'Instructions are provided for setting up and running Reports in Evidently to summarize the results of LLM regression tests.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'visualizing test results in dashboards',\n",
       "  'summary_answer': 'It includes information on how to create dashboards to visualize the results of LLM regression tests over time using Evidently Cloud.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'monitoring LLM test results',\n",
       "  'summary_answer': 'The article discusses how to set up monitoring panels in a Dashboard to track the results of tests performed on LLM outputs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'running LLM regression tests in CI/CD',\n",
       "  'summary_answer': 'Advice is given on integrating LLM regression testing into CI/CD workflows for continuous quality assurance in updates.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'Evidently packages installation for LLM',\n",
       "  'summary_answer': 'It explains how to install necessary Evidently packages for regression testing in a Python environment.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'best practices for testing LLM outputs',\n",
       "  'summary_answer': 'The article offers best practices for running effective regression tests on LLM outputs to improve reliability and consistency.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'performance metrics for LLM testing',\n",
       "  'summary_answer': 'Details on selecting and implementing metrics for evaluating the performance of LLM outputs are provided in the article.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'Evidently Cloud account creation steps',\n",
       "  'summary_answer': 'Instructions for creating an Evidently Cloud account to facilitate the tracking of test results are included in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'data exploration in LLM regression testing',\n",
       "  'summary_answer': 'The article describes how to perform quick data exploration on your dataset involved in LLM regression testing for better insights.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'LLM regression testing edge cases',\n",
       "  'summary_answer': 'For advanced users, the article discusses how to handle edge cases when performing regression tests on LLM outputs.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'using Jupyter for LLM testing',\n",
       "  'summary_answer': 'Guidance is provided on how to use Jupyter notebooks for running LLM regression tests and visualizing results.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'LLM evaluation code example',\n",
       "  'summary_answer': 'The article offers practical code examples for evaluating the quality of LLM outputs using various methods and metrics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'how to optimize LLM judge prompts',\n",
       "  'summary_answer': \"There's a detailed section on optimizing prompts for LLM judges, including examples for both multi-class and binary classifiers.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'Machine learning quickstart tutorial',\n",
       "  'summary_answer': 'A quickstart section introduces users to testing data quality and data drift in ML applications.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'RAG evaluation metrics explained',\n",
       "  'summary_answer': 'The article dives into RAG evaluation metrics, explaining different approaches you can use to assess the quality of retrieved data.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'end-to-end LLM workflows',\n",
       "  'summary_answer': 'End-to-end examples of specific LLM workflows are showcased throughout the article, illustrating how to apply LLM evaluations in real scenarios.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'using GitHub actions with Evidently',\n",
       "  'summary_answer': 'The article describes how to integrate Evidently evaluations into CI/CD workflows using GitHub actions.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'descriptor examples in ML',\n",
       "  'summary_answer': 'You’ll find a walkthrough of different descriptors used in ML evaluations, along with practical examples and applications.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'advanced LLM evaluation methods',\n",
       "  'summary_answer': 'The article covers advanced evaluation methods for LLMs, including both reference-based and reference-free techniques.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'tracing inputs and outputs in AI apps',\n",
       "  'summary_answer': 'There’s a quickstart guide on how to trace inputs and outputs from AI applications, showcasing its importance for evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'how to visualize LLM evaluation metrics',\n",
       "  'summary_answer': 'Learn how to visualize LLM evaluation metrics using Grafana alongside Evidently to enhance data monitoring and analysis.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'multi-class classification evaluation',\n",
       "  'summary_answer': 'A tutorial within the article focuses on evaluating multi-class classification tasks using LLM outputs and predictive baselines.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'adversarial testing for LLMs',\n",
       "  'summary_answer': 'There’s a complete guide on conducting adversarial testing for LLMs, focusing on scenario-based risk testing for sensitive topics.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'Evidently Cloud v2 features',\n",
       "  'summary_answer': 'The article outlines significant improvements in Evidently Cloud v2, including a redesigned dashboard and enhanced performance features.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/cloud_v2.mdx'},\n",
       " {'question': 'migrate to Evidently 0.6',\n",
       "  'summary_answer': 'The article outlines the steps to migrate to Evidently version 0.6, highlighting the new API structure and usage.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'new API in Evidently 0.6',\n",
       "  'summary_answer': 'Evidently 0.6 introduced a new API with a Report object, detailed in the migration guide.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'how to create Dataset in Evidently',\n",
       "  'summary_answer': 'You can create a Dataset in Evidently by using the Dataset.from_pandas method, which is explained in the guide with examples.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'data definition in Evidently 0.6',\n",
       "  'summary_answer': 'Data Definition replaces column mapping in Evidently, allowing for improved mapping of input columns, which is detailed in the migration article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'example of Report in Evidently 0.6',\n",
       "  'summary_answer': 'The article provides examples of how to generate Reports using the new API in version 0.6, including necessary code snippets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'test suites integration with reports',\n",
       "  'summary_answer': 'The migration guide discusses the unification of Test Suites with Reports in Evidently, providing efficiency in reporting metrics and conditions.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'breaking changes in Evidently 0.7',\n",
       "  'summary_answer': 'Key breaking changes introduced in Evidently 0.7 include the new API becoming the default, which is explained in the migration documentation.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'Evidently AI deployment options',\n",
       "  'summary_answer': 'The article outlines various deployment options for the Evidently AI ecosystem, including open-source and cloud alternatives.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Evidently OSS vs Cloud vs Enterprise',\n",
       "  'summary_answer': \"The article compares the core and premium features of Evidently's open-source and commercial platforms, highlighting the differences in support and maintenance.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Evidently Python library features',\n",
       "  'summary_answer': 'The Evidently Python library allows for data and AI evaluations, suitable for data scientists in Python environments, and is part of the open-source offerings.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'how to deploy Evidently OSS',\n",
       "  'summary_answer': 'The article mentions that deploying Evidently OSS requires managing the platform within your own environment, including backups and scaling.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Tracely library integration',\n",
       "  'summary_answer': 'Tracely is another open-source library from Evidently that enables real-time data capture for AI applications, based on OpenTelemetry.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Evidently Cloud support features',\n",
       "  'summary_answer': 'The commercial edition of Evidently Cloud provides dedicated support, including access to developers and assistance on feature configuration for users.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Evidently telemetry data collected',\n",
       "  'summary_answer': 'The article outlines that Evidently collects anonymous usage data, such as environment and service usage data, starting from version 0.4.0.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'How to disable telemetry in Evidently',\n",
       "  'summary_answer': 'Users can disable telemetry by setting the environment variable `DO_NOT_TRACK` to any value, as explained in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'List of data collected by Evidently telemetry',\n",
       "  'summary_answer': 'The telemetry data includes environment details like OS and Python version, as well as service usage actions like starting up or listing projects.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'Evidently telemetry opt-out guide',\n",
       "  'summary_answer': 'The article provides instructions for opting out of telemetry data collection, ensuring user privacy while using Evidently.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'Anonymized user ID in Evidently telemetry',\n",
       "  'summary_answer': 'The user ID collected by Evidently telemetry is anonymized to maintain user privacy while allowing tracking of actions performed by the same user.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'Evidently data about user environment',\n",
       "  'summary_answer': 'The telemetry captures basic environment information including OS name, OS version, and Python version, which helps in understanding the usage context.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'Telemetry examples for Evidently actions',\n",
       "  'summary_answer': 'The article shares JSON examples of telemetry events for actions such as startup, index, and project dashboard interactions.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'What is included in Evidently service usage data',\n",
       "  'summary_answer': 'Evidently service usage data includes information about various actions like adding projects and retrieving snapshot data, which helps in feature usage analysis.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'Version of Evidently required for telemetry',\n",
       "  'summary_answer': 'Telemetry data collection starts from version 0.4.0 of Evidently, as specified in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'Setting up telemetry for Evidently on terminal',\n",
       "  'summary_answer': 'The article describes how to check whether telemetry is enabled by default and how to change its status via terminal commands.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'Evidently AI platform features',\n",
       "  'summary_answer': 'The article outlines the various features of the Evidently AI platform, including built-in evaluations, modularity, and support for both ML and LLM tasks.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/why_evidently.mdx'},\n",
       " {'question': 'open-source analytics tool for AI',\n",
       "  'summary_answer': 'Evidently is an open-source library that provides a comprehensive set of tools for evaluating and monitoring AI systems, ensuring transparency and a great developer experience.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/why_evidently.mdx'},\n",
       " {'question': 'how to get started with Evidently',\n",
       "  'summary_answer': 'The article emphasizes the ease of starting with Evidently, mentioning local ad hoc checks and the modular nature that allows for gradual implementation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/why_evidently.mdx'},\n",
       " {'question': 'integration options for Evidently',\n",
       "  'summary_answer': 'Evidently integrates easily with existing tools and allows for the export of metrics, making it versatile for various workflows and data handling needs.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/why_evidently.mdx'},\n",
       " {'question': 'evidently library features',\n",
       "  'summary_answer': 'The article outlines the various features of the Evidently library, including over 100 evaluation metrics, a testing API, and a visual interface for data analysis.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'question': 'quickstart evaluation with evidently',\n",
       "  'summary_answer': 'It describes how to quickly set up and run an evaluation using Evidently, making it accessible even for new users in AI and data science.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'question': 'evidently platform overview',\n",
       "  'summary_answer': 'The article provides an overview of the Evidently platform, detailing its tools for AI observability, dataset management, and synthetic data generation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'question': 'ml monitoring with evidently',\n",
       "  'summary_answer': 'It includes a guide on how to use Evidently for monitoring machine learning models and assessing data quality for tabular datasets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'question': 'evidently metrics catalogue',\n",
       "  'summary_answer': 'The article offers a look into the comprehensive catalogue of over 100 evaluation metrics available in Evidently.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'question': 'advanced features of evidently',\n",
       "  'summary_answer': 'For users already familiar with AI observability, the article touches on advanced features such as alerting and orchestration within the Evidently platform.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'question': 'code examples in evidently',\n",
       "  'summary_answer': 'It points users to the Evidently Cookbook, which contains end-to-end code tutorials and examples to help implement various functionalities.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'question': 'how to implement evidently for AI',\n",
       "  'summary_answer': 'The article serves as a starting point for implementing Evidently in AI-powered projects, discussing both basic and advanced use cases.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'question': 'deterministic evals examples',\n",
       "  'summary_answer': 'The article provides various deterministic evaluation methods, like ExactMatch, RegExp, BeginsWith, and Others, each with usage examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'how to use Contains in evaluations',\n",
       "  'summary_answer': 'Contains checks for specified items in text and is demonstrated with syntax and examples in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'descriptive text statistics functions',\n",
       "  'summary_answer': 'The article includes functions like TextLength(), WordCount(), and OOVWordsPercentage() for descriptive text statistics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'validate JSON in descriptors',\n",
       "  'summary_answer': 'The article discusses functions like IsValidJSON() and JSONSchemaMatch() for validating JSON structures in data evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'Pattern matching descriptors',\n",
       "  'summary_answer': 'It outlines descriptors such as ExactMatch() and RegExp() for pattern matching in text evaluation with examples provided.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'Content checks in evaluations',\n",
       "  'summary_answer': 'The article explains methods like Contains(), DoesNotContain(), and IncludesWords() to verify content presence in text.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'creating custom descriptors',\n",
       "  'summary_answer': 'CustomDescriptor() and CustomColumnsDescriptor() functions are outlined for implementing user-defined checks using Python functions.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'LLM evaluations reference',\n",
       "  'summary_answer': 'The article details custom LLM-based evaluations like LLMJudge() for scoring text based on user-defined criteria across different models.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'using IsValidSQL for validation',\n",
       "  'summary_answer': 'IsValidSQL() is a descriptor to check SQL query validity without executing it, with a code example included.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'Text stats functions overview',\n",
       "  'summary_answer': 'It provides an overview of text-related statistics functions like WordCount, SentenceCount, and NonLetterCharacterPercentage with descriptions and examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'using Assertions in evaluations',\n",
       "  'summary_answer': 'Various assertion descriptors like ItemMatch() and ItemNoMatch() are outlined for comparing lists in row-level evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'introduction to LLM evals',\n",
       "  'summary_answer': 'The article introduces LLM evals, discussing the scenarios and descriptors used for evaluating text outputs in machine learning contexts.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'how to Implement CustomDescriptor',\n",
       "  'summary_answer': 'The article provides guidance on implementing CustomDescriptor with examples on inputs and outputs.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'understanding bias evaluations',\n",
       "  'summary_answer': 'It addresses the need for checking bias in texts through descriptors like BiasLLMEval, detailing functionality and use cases.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'Toxicity evaluation examples',\n",
       "  'summary_answer': 'ToxicityLLMEval and similar descriptors are discussed for detecting toxic text in evaluations, with usage examples.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'using RegExp for pattern matching',\n",
       "  'summary_answer': 'RegExp() is covered with its syntax and pattern matching capabilities, providing examples for practical implementation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'validating syntax in descriptors',\n",
       "  'summary_answer': 'The article explains syntax validation using descriptors like IsValidPython() and IsValidSQL() for ensuring the accuracy of code and queries.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'examples of FAQ evaluators using LLM',\n",
       "  'summary_answer': 'It covers examples of building FAQ evaluators using the provided LLM evaluation functions with specific scenarios outlined.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'usage of CompletenessLLMEval',\n",
       "  'summary_answer': 'CompletenessLLMEval checks if LLM responses fully utilize context, and examples are provided to illustrate its use.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'SemanticSimilarity descriptor explained',\n",
       "  'summary_answer': 'SemanticSimilarity() describes how to evaluate text similarity between columns using sentence embeddings as shown in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'detecting negativity in text',\n",
       "  'summary_answer': 'NegativityLLMEval is used to assess sentiment in responses with details on its implementation discussed in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'how to calculate sentence count',\n",
       "  'summary_answer': 'SentenceCount() is covered as a function to measure sentence count in text, with examples for clear understanding.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'overview of machine learning evals',\n",
       "  'summary_answer': 'It presents an overview of ML-based evaluation methods, including SemanticSimilarity and BERTScore, detailing their applications in text evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'non-letter character percentage function',\n",
       "  'summary_answer': 'NonLetterCharacterPercentage() is provided for calculating the percentage of non-letter characters in text, with an example included.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'examples of custom LLM evals',\n",
       "  'summary_answer': 'The article gives examples of custom LLM evaluations like CorrectnessLLMEval and its parameters for scoring text outputs based on accuracy.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'dataset-level metrics overview',\n",
       "  'summary_answer': 'This article provides a comprehensive overview of various dataset-level evaluation metrics, explaining their usage and configurations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to use TextEvals()',\n",
       "  'summary_answer': 'TextEvals() is a large preset metric that summarizes text evaluation results, requiring specified descriptors for accurate scoring.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'ValueStats() parameters',\n",
       "  'summary_answer': 'ValueStats() computes descriptive statistics for columns and requires column parameters alongside optional test conditions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'missing value metrics',\n",
       "  'summary_answer': 'The article explains several metrics for counting missing values, detailing how to configure these in a dataset evaluation context.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to calculate accuracy in classification',\n",
       "  'summary_answer': 'Accuracy can be calculated using the Accuracy() metric, which compares predicted values against true labels, necessitating proper configuration for effective assessment.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'testing for missing values',\n",
       "  'summary_answer': 'Various tests for missing values are outlined, allowing users to validate JSON data integrity when invoked in the evaluation process.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'exploratory data analysis metrics',\n",
       "  'summary_answer': 'The document provides metrics suitable for exploratory data analysis, like RowCount and ColumnCount, helping users understand their dataset structure better.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'MinValue() function usage example',\n",
       "  'summary_answer': 'MinValue() computes the minimum value of a specified column, enabling users to enforce data validation against expected minimum thresholds.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'Detecting data drift methods',\n",
       "  'summary_answer': 'Users can utilize various metrics to detect data drift, with a focus on comparing distributions over multiple test methods as outlined in the document.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'example of using RecallTopK()',\n",
       "  'summary_answer': 'RecallTopK() function allows evaluation of retrieval performance by calculating recall for the top K items, essential for ranking tasks.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to assess regression quality',\n",
       "  'summary_answer': 'The article describes multiple metrics for assessing regression model quality, where regression quality metrics can be configured according to the specific use case.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'importance of descriptor mapping',\n",
       "  'summary_answer': 'Mapping descriptors plays a crucial role in text evaluations, greatly affecting quality scores reported by various metrics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'KPI metrics for report generation',\n",
       "  'summary_answer': 'Various metrics discussed can serve as KPIs in reports, aiding organizations in tracking and assessing machine learning model performance.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to check for duplicated rows',\n",
       "  'summary_answer': 'DuplicatedRowCount() metric allows users to check for and count duplicated rows, giving insights into data quality issues.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'median value computation',\n",
       "  'summary_answer': 'The MedianValue() function enables computation of the median for a given numerical column, facilitating data analysis and validation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'overview of metrics for classification',\n",
       "  'summary_answer': 'This reference provides an overview of various classification metrics which can summarize model evaluation and performance diagnostics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'top K precision definitions',\n",
       "  'summary_answer': 'The document details how PrecisionTopK() evaluates the proportion of relevant items among the top K recommendations, fundamental in information retrieval.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'understanding F1 Score definition',\n",
       "  'summary_answer': 'F1Score() is a crucial metric for balancing precision and recall, especially in imbalanced datasets, ensuring robust model evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to configure RegressionQuality',\n",
       "  'summary_answer': 'RegressionQuality allows users to summarize key regression metrics in one widget, giving a holistic view of model performance.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'details on DriftedColumnsCount()',\n",
       "  'summary_answer': 'DriftedColumnsCount() calculates the number of columns that exhibit data drift, providing a critical check for ensuring data consistency over time.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'Leveraging Dummy metrics',\n",
       "  'summary_answer': 'Dummy metrics provide baseline evaluations against which real model performance can be assessed, critical for validation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'outlier detection in dataset',\n",
       "  'summary_answer': 'Outlier detection can be facilitated through metrics such as OutRangeValueCount(), essential for ensuring data integrity and quality.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'understanding statistical variance',\n",
       "  'summary_answer': 'The document provides insights into statistical variance calculations, critical for understanding data consistency and analysis.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to evaluate multi-class classification',\n",
       "  'summary_answer': 'Multi-class evaluation can utilize metrics like ClassificationQualityByLabel() which summarises quality metrics by class label.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'use of category counts',\n",
       "  'summary_answer': 'CategoryCount() counts occurrences of specified categories, helpful for categorical column analysis in dataset evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'what is NDCG',\n",
       "  'summary_answer': 'Normalized Discounted Cumulative Gain (NDCG) is outlined in the document as a ranking metric to evaluate recommended item positions.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how much missing data is acceptable',\n",
       "  'summary_answer': 'The document discusses acceptable limits for missing data, pivotal for maintaining dataset quality across various evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'utilizing InListValueCount()',\n",
       "  'summary_answer': 'InListValueCount() checks how many values in a dataset fall within a specified list, serving a vital role in data validation and integrity checks.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'descriptive statistics summary',\n",
       "  'summary_answer': 'The article provides a structured approach to utilizing descriptive statistics metrics like ValueStats() for assessing data quality.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'applying ROC AUC',\n",
       "  'summary_answer': 'ROC AUC is an essential metric a user can apply to evaluate the robustness of binary classifiers, helping in model comparison.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'configuring test defaults',\n",
       "  'summary_answer': 'Test defaults play a key role in metrics configuration, determining the baseline conditions applied during metrics assessments.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'examples of data validation metrics',\n",
       "  'summary_answer': 'The article provides comprehensive examples of validation metrics that ensure data meets quality and integrity standards.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to read test conditions',\n",
       "  'summary_answer': 'Understanding test conditions is crucial for effectively applying evaluation metrics, as they determine how metrics pass or fail.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'explaining recall vs precision',\n",
       "  'summary_answer': 'Recall and precision are explained as essential metrics for evaluating classification model effectiveness, especially in binary contexts.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'MaxValue() function details',\n",
       "  'summary_answer': 'MaxValue() is essential for users needing to identify the highest value in a column, aiding in data analysis and validation efforts.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'statistical methods for dataset analysis',\n",
       "  'summary_answer': 'Various statistical methods are discussed for thorough dataset analysis, including tests for mean, variance, and other descriptive metrics.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to configure data quality tests',\n",
       "  'summary_answer': 'Data quality tests are crucial for identifying and addressing issues in datasets, detailed through various metrics in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'impact of constant columns',\n",
       "  'summary_answer': 'Constant columns can skew the dataset analysis results and are addressed in the document regarding metrics for assessing their presence.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'summary of regression metrics',\n",
       "  'summary_answer': 'Effective regression analysis involves multiple metrics detailed in the article, guiding users in assessing model accuracy and performance.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'preset evaluation templates for datasets',\n",
       "  'summary_answer': 'The article describes various pre-built evaluation templates suitable for datasets, making it easy to start evaluation processes without extensive setup.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_presets.mdx'},\n",
       " {'question': 'text eval templates overview',\n",
       "  'summary_answer': 'It includes a section on text evaluation presets specifically designed for handling evaluations related to text and language models.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_presets.mdx'},\n",
       " {'question': 'how to use classification presets',\n",
       "  'summary_answer': 'The article explains how to utilize pre-built classification presets to assess quality in classification tasks efficiently.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_presets.mdx'},\n",
       " {'question': 'data drift preset usage',\n",
       "  'summary_answer': 'Users can find guidelines on applying data drift detection templates to monitor changes in data distribution over time.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_presets.mdx'},\n",
       " {'question': 'advanced data evaluation techniques presets',\n",
       "  'summary_answer': 'The article mentions the use of built-in descriptors for row-level evaluations, appealing to advanced users familiar with dataset assessments.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_presets.mdx'},\n",
       " {'question': 'customize data drift detection',\n",
       "  'summary_answer': 'The article provides guidance on how to modify the drift detection method and thresholds used in data drift metrics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'data drift detection methods list',\n",
       "  'summary_answer': 'You can choose from over 20 drift detection methods, which can be customized based on the column type and other parameters.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'how to set drift share',\n",
       "  'summary_answer': 'You can set the share of drifting columns that indicates dataset drift using parameters in the Report function, such as `drift_share`.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'example of PSI for dataset drift',\n",
       "  'summary_answer': 'The article includes example code to implement PSI as the drift detection method for dataset-level drift checks, demonstrating how to set it up in the Report.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'threshold for data drift detection',\n",
       "  'summary_answer': 'You can specify thresholds for different drift detection methods, allowing for more precise conditions based on the method used.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'override data drift defaults',\n",
       "  'summary_answer': 'The article explains how to override default settings for data drift detection methods by passing custom parameters to Metrics or Presets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'dataset drift detection example',\n",
       "  'summary_answer': 'It shows how to create a Report that checks for dataset drift with a specified drift share using example code snippets.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'custom data drift method implementation',\n",
       "  'summary_answer': 'The article provides detailed instructions for implementing your custom drift detection methods using the StatTest class.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'Set categorical drift threshold',\n",
       "  'summary_answer': 'The article describes how to set a custom threshold for categorical columns by using the `cat_threshold` parameter in the relevant report.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'data drift parameters explained',\n",
       "  'summary_answer': 'It outlines the various parameters like `method`, `threshold`, and `drift_share`, explaining their role in data drift detection.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'use custom method for ValueDrift',\n",
       "  'summary_answer': 'You can register a custom method for detecting value drift by implementing the StatTest class, as demonstrated in the provided code example.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'Drift detection for numerical columns',\n",
       "  'summary_answer': 'The article specifies how to set drift methods for numerical columns using the `num_method` and `num_threshold` parameters.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'compare drift scores in text columns',\n",
       "  'summary_answer': 'It discusses how to apply drift detection methods specific to text data, including example code for text content drift.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'data drift method custom options',\n",
       "  'summary_answer': 'You can select from various drift detection methods suited for tabular and text data, allowing for flexible adjustment based on your requirements.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'implementing textual drift detection',\n",
       "  'summary_answer': 'The article describes methods for detecting drift in raw text data, including the appropriate Statistical Tests to be used.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'detection method for categorical data',\n",
       "  'summary_answer': 'Check how to customize drift detection methods specifically for categorical data, including example use cases and code snippets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'data drift customization tutorial',\n",
       "  'summary_answer': 'The article serves as a comprehensive guide to customizing data drift methodologies, covering everything from parameters to custom method implementation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'custom text evaluator in Python',\n",
       "  'summary_answer': 'The article outlines how to create a custom text evaluator using the Evidently framework by implementing functions that evaluate data in specified columns.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_descriptor.mdx'},\n",
       " {'question': 'using CustomDescriptor in Evidently',\n",
       "  'summary_answer': 'Instructions for defining and using a CustomDescriptor for evaluating multiple columns and returning multiple scores are provided in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_descriptor.mdx'},\n",
       " {'question': 'advanced custom column descriptor techniques',\n",
       "  'summary_answer': 'The article includes advanced examples of creating custom descriptors that perform complex evaluations, including multiple checks and data transformations.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_descriptor.mdx'},\n",
       " {'question': 'using huggingface models for text evaluation',\n",
       "  'summary_answer': 'The article provides a guide on how to use HuggingFace models to score and classify text using specific descriptors in Python.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'huggingface descriptors examples',\n",
       "  'summary_answer': 'It describes ready-to-use descriptors for HuggingFace models and how to implement them for text evaluation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'importing huggingface models in python',\n",
       "  'summary_answer': 'The article includes import instructions for using HuggingFace models via the Evidently library.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'custom evaluations with huggingface descriptors',\n",
       "  'summary_answer': 'You can create custom checks directly as Python functions or use the general HuggingFace descriptor for specific models, as described in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'how to score text using huggingface',\n",
       "  'summary_answer': 'Learn how to evaluate text data by applying HuggingFace models, which includes detailed coding snippets.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'built-in huggingface evaluators',\n",
       "  'summary_answer': 'The article outlines several built-in evaluators available for HuggingFace models that can be easily integrated into your text evaluation workflows.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'huggingface model parameters',\n",
       "  'summary_answer': 'It explains the parameters required for various HuggingFace models used in text classification and evaluation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'evaluate text emotions using huggingface',\n",
       "  'summary_answer': 'The article provides specific examples for classifying text by different emotions using HuggingFace models and their parameters.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'integrating huggingface with evidently',\n",
       "  'summary_answer': 'It describes how to integrate HuggingFace models into the Evidently framework for text evaluation.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'huggingface model compatibility',\n",
       "  'summary_answer': 'The article discusses the compatibility of various HuggingFace models with the implemented interface during text scoring.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'how to set up llm judges',\n",
       "  'summary_answer': 'The article provides a comprehensive guide on configuring LLM judges, including using built-in evaluators and custom templates for specific evaluation criteria.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'llm-based descriptors examples',\n",
       "  'summary_answer': 'Built-in evaluators are demonstrated in the article, highlighting examples of toxicity and context quality assessments using LLM-based descriptors.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'environment variable for openai key',\n",
       "  'summary_answer': 'To set the OpenAI key as an environment variable, you can use `os.environ[\"OPENAI_API_KEY\"] = \\'your_key\\'` in your code as shown in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'evaluate text with binary classification',\n",
       "  'summary_answer': 'The article explains how to use the `BinaryClassificationPromptTemplate` to evaluate text for concise vs verbose classification, including code snippets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'adding descriptors to evaluation',\n",
       "  'summary_answer': 'You can add descriptors to your evaluation DataFrame using the `eval_df.add_descriptors()` method, as demonstrated in various examples throughout the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'custom llm judge creation',\n",
       "  'summary_answer': 'The article details the steps for creating a custom LLM judge using templates, including defining criteria and target categories for evaluation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'change llm model in evaluation',\n",
       "  'summary_answer': 'You can specify a different model for evaluation by using the `model` parameter in the `add_descriptors` method as shown in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'integrating multi-class classifiers',\n",
       "  'summary_answer': 'Guidance on implementing multi-class classifiers with `MulticlassClassificationPromptTemplate` is provided, including category definitions and evaluation examples.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'llm evaluation parameters',\n",
       "  'summary_answer': 'The article outlines key parameters for configuring LLM evaluations, such as `template`, `provider`, and `additional_columns` to tailor the evaluation process.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'toxicity assessment using llm',\n",
       "  'summary_answer': 'To evaluate toxicity in text responses, the article illustrates how to use `ToxicityLLMEval` with appropriate parameters for analysis.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'using custom templates for evaluations',\n",
       "  'summary_answer': 'You can create custom templates to tailor evaluations to your needs by specifying grading logic and categories, as explained in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'example of context quality evaluation',\n",
       "  'summary_answer': 'The article showcases how to evaluate the context quality of responses using `ContextQualityLLMEval`, demonstrating the necessary setup and code.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'multi-column evaluation technique',\n",
       "  'summary_answer': 'For scenarios needing multi-column evaluations, the article describes how to implement these using appropriate descriptors and parameter setups.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'parameterize llm evaluators',\n",
       "  'summary_answer': 'The article discusses how to parameterize evaluators, allowing customization of output formats and other evaluation details using specific method calls.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'examples of built-in llm descriptors',\n",
       "  'summary_answer': 'A table listing various built-in LLM descriptors is included in the article, along with their characteristics and use cases.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'implementing cross-provider evaluations',\n",
       "  'summary_answer': 'The article highlights how to implement evaluations across different LLM providers, offering code snippets and best practices for integration.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'LLMEval implementation details',\n",
       "  'summary_answer': 'The article breaks down the implementation of `LLMEval`, explaining the necessary parameters and how to use them effectively in evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'evaluate contextual accuracy',\n",
       "  'summary_answer': 'To evaluate if responses are faithful to given contexts, the article demonstrates how to use `LLMEval` with multiple columns in the evaluation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'create a custom binary classifier',\n",
       "  'summary_answer': 'You can design a custom binary classification evaluator using the `BinaryClassificationPromptTemplate`, as illustrated in the examples provided.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'importance of pre-messages in prompts',\n",
       "  'summary_answer': 'The article explains the role of pre-messages in structuring prompts and guiding LLM evaluations, with practical examples provided.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'assessing answer relevance',\n",
       "  'summary_answer': 'Using the `MulticlassClassificationPromptTemplate`, you can classify responses based on their relevance to questions, as shown in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'examples of output formatting in evaluation',\n",
       "  'summary_answer': \"The article shows how to format evaluation output, including results' representations and scores, to meet specific requirements.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'environment variables for api keys',\n",
       "  'summary_answer': 'To securely use API keys for LLM evaluations, the article advises on setting them as environment variables to avoid hard coding sensitive information.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'create custom metric in Python',\n",
       "  'summary_answer': 'The article outlines the process of building a custom metric in Python, including implementing calculation methods and defining default test conditions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_metric.mdx'},\n",
       " {'question': 'examples of custom metric implementation',\n",
       "  'summary_answer': 'The article provides a detailed example of implementing a custom metric called `MyMaxMetric`, complete with code snippets and explanations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_metric.mdx'},\n",
       " {'question': 'custom metric visualization options',\n",
       "  'summary_answer': 'It discusses how to create a custom visualization for metrics using Plotly, enhancing the appearance of the metric in reports.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_metric.mdx'},\n",
       " {'question': 'default test conditions for metrics',\n",
       "  'summary_answer': 'The article explains how to define default test conditions when creating custom metrics, which are crucial for evaluating performance without custom thresholds.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_metric.mdx'},\n",
       " {'question': 'classification metrics examples',\n",
       "  'summary_answer': 'The article discusses key classification metrics like Accuracy, Precision, Recall, F1-score, and more, providing insight into their importance in model evaluation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'how to interpret confusion matrix',\n",
       "  'summary_answer': 'The confusion matrix is visualized in the article to help interpret classification errors and their types, thus aiding in performance analysis.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'ROC AUC calculation',\n",
       "  'summary_answer': 'The article outlines how the ROC AUC is calculated and its significance in evaluating model performance across different thresholds.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'visualizations for model analysis',\n",
       "  'summary_answer': 'Evidently generates interactive visualizations to analyze model performance, highlighting where improvements can be made based on predicted probabilities.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'precision-recall curve explanation',\n",
       "  'summary_answer': 'The precision-recall curve demonstrates the trade-off between precision and recall at different thresholds, as explained in the article.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'F1 score usage in classification',\n",
       "  'summary_answer': 'The article explores the F1 score as a crucial metric for balancing precision and recall in classification tasks.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'how to plot ROC curve',\n",
       "  'summary_answer': 'Code snippets and steps for plotting the ROC curve are provided to help visualize true positive and negative rates in classification models.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'model quality metrics in multi-class',\n",
       "  'summary_answer': 'In multi-class scenarios, the article details how metrics like ROC AUC and confusion matrix can be applied to evaluate model quality for each class.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'data quality overview widget',\n",
       "  'summary_answer': 'The article describes the data quality overview widget, which summarizes dataset features, including missing or empty values and feature behaviors.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'how to visualize feature statistics',\n",
       "  'summary_answer': 'It details how to use widgets to generate visualizations for each feature, tailored to feature types like categorical, numerical, and datetime.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'missing features percentage analysis',\n",
       "  'summary_answer': 'The article explains how the summary widget shows the percentage of missing or constant features in a dataset, aiding in data quality assessments.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'correlation widget functionalities',\n",
       "  'summary_answer': 'The correlation widget displays pairwise feature correlations, offering insights into relationships between features and changes across datasets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'visualize features over time',\n",
       "  'summary_answer': 'A section describes visualizing feature behavior over time, crucial for understanding trends in dynamic datasets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': \"Cramer's v correlation explanation\",\n",
       "  'summary_answer': \"The article discusses Cramer's v for categorical features and statistical methodologies for analyzing correlations, useful for more advanced data quality analysis.\",\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'generate correlation heatmaps in Evidently',\n",
       "  'summary_answer': 'It mentions that users can generate correlation heatmaps using Evidently, although this feature has been removed in later versions.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'data drift detection algorithm basics',\n",
       "  'summary_answer': 'The article covers the default data drift detection algorithm used in Evidently, explaining how it analyzes differences in data distributions between reference and current datasets.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'how to implement data drift detection',\n",
       "  'summary_answer': 'It provides guidance on implementing data drift detection by using the `DataDriftPreset` and understanding required datasets and column conditions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'what is column drift',\n",
       "  'summary_answer': 'The document discusses the concept of column drift, which refers to changes in the distributions of individual features within a dataset as compared to a reference.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'methods for statistical drift detection',\n",
       "  'summary_answer': 'The article details several statistical tests for detecting drift, including the Kolmogorov-Smirnov test and chi-squared test, and explains their applicability based on column types and data size.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'data requirements for drift analysis',\n",
       "  'summary_answer': 'It outlines specific data requirements, such as needing two non-empty datasets for evaluation, to perform effective data drift detection.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'adjusting drift detection thresholds',\n",
       "  'summary_answer': 'The article mentions that users can modify the default detection logic and set custom thresholds for drift detection metrics, to tailor the drift analysis to their needs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'advanced techniques for dataset drift',\n",
       "  'summary_answer': \"It covers advanced techniques for detecting dataset-level drift by evaluating multiple columns' distributions and establishing rules based on their outcomes.\",\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'ROC AUC in text data drift',\n",
       "  'summary_answer': 'The document explains how ROC AUC scores are used to assess text data drift using a domain classifier, providing a method to compare distributions effectively.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'recall at K metric explanation',\n",
       "  'summary_answer': 'The article explains Recall at K as a metric that measures the ability of a recommender system to retrieve relevant items within the top K results, with a formula provided for its calculation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'precision at K definition',\n",
       "  'summary_answer': 'Precision at K measures the proportion of recommended items that are relevant, emphasizing the quality of the top K recommendations made by the system.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'how to calculate MAP',\n",
       "  'summary_answer': 'The article details how to calculate Mean Average Precision (MAP) at K, focusing on assessing the ranking quality of relevant items placed higher in the list of recommendations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'F Beta formula and use cases',\n",
       "  'summary_answer': 'F Beta combines precision and recall into a single score, useful for different weighting of these metrics depending on the application context, explained with a formula and typical use cases.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'write precision at K code example',\n",
       "  'summary_answer': 'An example of code for calculating Precision at K for individual users is provided in the article, allowing for practical implementation within recommendation systems.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'NDCG metric explained',\n",
       "  'summary_answer': 'Normalized Discounted Cumulative Gain (NDCG) assesses ranking quality compared to an ideal order, discussed with methodologies for calculating DCG and ideal DCG.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'mean reciprocal rank calculation',\n",
       "  'summary_answer': 'The article outlines the method for calculating Mean Reciprocal Rank (MRR), which evaluates the position of the first relevant item in recommendation lists.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'hit rate formula',\n",
       "  'summary_answer': 'Hit Rate is defined as the percentage of users for whom at least one relevant item appears in the top K recommendations, with the calculation process explained.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'recSys metrics code example',\n",
       "  'summary_answer': 'Instructions for implementing various RecSys metrics programmatically are included, providing practical applications for developers and data scientists.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'evaluate ranking system performance',\n",
       "  'summary_answer': 'The article delves into several metrics that together provide a comprehensive performance evaluation of ranking and recommendation systems.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'regression metrics overview',\n",
       "  'summary_answer': 'The article discusses regression metrics like MAE and MAPE, offering insights into model quality assessments and visualizations for analysis.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'how to calculate mean absolute error',\n",
       "  'summary_answer': 'It explains how to compute metrics such as Mean Absolute Error (MAE) for evaluating the accuracy of regression models.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'predicted vs actual values visualization',\n",
       "  'summary_answer': 'The article details how to create scatter plots to compare predicted values against actual outcomes to identify discrepancies.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'importance of error distribution in regression',\n",
       "  'summary_answer': 'It describes how analyzing error distribution helps in understanding model performance and stability across different predictions.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'plots for model error analysis',\n",
       "  'summary_answer': 'The document outlines various plots, including predicted vs actual over time, to visualize regression model performance.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'mean percentage error calculation in regression',\n",
       "  'summary_answer': 'It provides steps to calculate Mean Absolute Percentage Error (MAPE) as a measure of predictive accuracy.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'error normality in predictive models',\n",
       "  'summary_answer': 'The article introduces methods to assess the normality of errors using Q-Q plots to ensure model reliability.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'error bias visualization per feature',\n",
       "  'summary_answer': 'It explains how to visualize error bias per feature to identify relationships between feature values and model error rates.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'model performance summary metrics explained',\n",
       "  'summary_answer': 'The document summarizes key model quality metrics and discusses their significance in evaluating regression outcomes.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'classification quality preset overview',\n",
       "  'summary_answer': 'The article provides a comprehensive overview of the Classification Quality Preset, outlining its features, metrics, and usage for evaluating model performance on classification tasks.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_classification.mdx'},\n",
       " {'question': 'classification preset report example',\n",
       "  'summary_answer': 'It shows a sample code snippet for running a report using the ClassificationPreset, including how to incorporate test suites for better evaluation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_classification.mdx'},\n",
       " {'question': 'customizing classification report',\n",
       "  'summary_answer': 'The article explains how to customize the classification report by changing test conditions, adding metrics, and modifying report composition to better suit specific needs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_classification.mdx'},\n",
       " {'question': 'data drift preset example',\n",
       "  'summary_answer': 'The article provides a clear example of how to create a Report using the DataDriftPreset to evaluate shifts in data distributions between current and reference datasets.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'how to monitor data drift in ML models',\n",
       "  'summary_answer': 'The article explains methods to monitor data drift using feature and prediction drift metrics, helping ensure the ML model operates in a familiar environment.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'customizing drift detection methods',\n",
       "  'summary_answer': 'The article outlines how to change drift detection methods and parameters, allowing users to tailor the evaluation to specific dataset needs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'importance of data drift evaluation',\n",
       "  'summary_answer': 'The article discusses the significance of monitoring data drift to maintain ML model quality and make informed decisions about retraining and adjustments.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'implementing custom drift detection',\n",
       "  'summary_answer': 'The article details how to implement a custom drift detection method using a Python function, catering to advanced users looking to enhance their analysis.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'data summary preset overview',\n",
       "  'summary_answer': 'This article provides an overview of the Data Summary Preset, explaining how to visualize key descriptive statistics and conduct data comparisons.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_data_summary.mdx'},\n",
       " {'question': 'how to use DataSummaryPreset in Python',\n",
       "  'summary_answer': 'The article illustrates how to use the DataSummaryPreset in Python to generate reports and evaluate datasets, including a code example for implementation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_summary.mdx'},\n",
       " {'question': 'customize data summary report',\n",
       "  'summary_answer': 'Learn how to customize the Data Summary report, including selecting specific columns and modifying test conditions based on dataset requirements.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_summary.mdx'},\n",
       " {'question': 'evidently recommender systems example',\n",
       "  'summary_answer': 'The article provides a code snippet for using the RecSysPreset to generate top-k recommendations and evaluate them based on the current dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_recsys.mdx'},\n",
       " {'question': 'how to assess quality in recommender systems',\n",
       "  'summary_answer': \"The article explains how the RecsysPreset evaluates ranking and diversity through multiple metrics, emphasizing the importance of the 'k' parameter.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_recsys.mdx'},\n",
       " {'question': 'customizing recommender system reports',\n",
       "  'summary_answer': 'The document details ways to customize the report for recommender systems, including changing test conditions and modifying report composition to include additional metrics.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_recsys.mdx'},\n",
       " {'question': 'regression quality preset overview',\n",
       "  'summary_answer': 'The article provides a comprehensive overview of the Regression Quality Preset, explaining how it can be used to evaluate regression tasks using performance metrics and visualizations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_regression.mdx'},\n",
       " {'question': 'setting up regression preset in python',\n",
       "  'summary_answer': 'Instructions on how to implement the RegressionPreset in Python, including code snippets for running reports on current and reference datasets, are provided in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_regression.mdx'},\n",
       " {'question': 'customizing regression report tests',\n",
       "  'summary_answer': 'The article explains how to customize the test conditions within the regression report, allowing you to modify default settings and add additional metrics for analysis.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_regression.mdx'},\n",
       " {'question': 'text evals example code',\n",
       "  'summary_answer': 'The article provides code snippets for using the Text Evals Preset to run evaluations on one or two datasets, showcasing how to generate reports and include tests.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_text_evals.mdx'},\n",
       " {'question': 'how to customize text evals report',\n",
       "  'summary_answer': 'It explains various customization options for reports generated with Text Evals, including selecting specific descriptors and modifying test conditions.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_text_evals.mdx'},\n",
       " {'question': 'data quality tests in text evals',\n",
       "  'summary_answer': 'The article describes how to add automated data quality tests from a reference dataset when running evaluations with the Text Evals Preset, enhancing the analysis.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_text_evals.mdx'},\n",
       " {'question': 'evidently evaluate LLM outputs',\n",
       "  'summary_answer': 'This article explains how to automatically evaluate LLM outputs using Evidently, allowing for quick comparisons and improved decision-making.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'set up evidently cloud',\n",
       "  'summary_answer': 'It outlines the steps to set up your environment for using Evidently Cloud for LLM evaluations, including installation instructions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'python code examples for LLM evaluation',\n",
       "  'summary_answer': 'The article includes Python code snippets to demonstrate how to evaluate LLM responses and generate reports effectively.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'how to create dataset for LLM eval',\n",
       "  'summary_answer': 'It describes how to prepare a dataset for evaluation, using example data structures suitable for LLM input and output comparisons.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'add custom LLM judge in evidently',\n",
       "  'summary_answer': 'Instructions are given on how to implement a custom LLM judge using Evidently’s built-in templates to classify question appropriateness.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'report generation in evidently for LLM',\n",
       "  'summary_answer': 'The article provides guidance on how to create and run reports summarizing evaluation results of LLM responses.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'evidently tests for LLM evaluations',\n",
       "  'summary_answer': 'It explains how to include test conditions in evaluations, ensuring outputs meet specific criteria.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'configure LLM judges in evidently',\n",
       "  'summary_answer': 'The article discusses how to configure LLM judges for custom evaluation criteria and how to integrate various LLM models.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'track metrics in LLM evaluations',\n",
       "  'summary_answer': 'It highlights how to track evaluation metrics over time using dashboards in Evidently for better analysis of model performance.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'data drift evaluation python example',\n",
       "  'summary_answer': 'The article provides a step-by-step guide on how to evaluate data drift in Python using the Evidently library, including code examples for setting up and running the evaluation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'setup evidently cloud for data evaluation',\n",
       "  'summary_answer': 'Instructions for setting up Evidently Cloud for running evaluations as part of ML checks are outlined in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'install evidently python library',\n",
       "  'summary_answer': 'The article includes the command to install the Evidently Python library, essential for running data evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'how to create data summary report evidently',\n",
       "  'summary_answer': 'It explains how to create a data summary report using the DataSummaryPreset and suggests including tests for data quality.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'test for prediction quality in machine learning',\n",
       "  'summary_answer': 'The article discusses tests for evaluating prediction quality, including metrics for classification and regression accuracy.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'explore evidently dashboard features',\n",
       "  'summary_answer': 'Options for configuring and tracking results in an Evidently dashboard are elaborated upon, including code snippets for dashboard panels.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'setup tracing LLM application',\n",
       "  'summary_answer': 'The article details the steps to set up tracing for an LLM application, guiding users on what libraries to install and how to initialize tracing.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'Evidently Cloud tracing examples',\n",
       "  'summary_answer': 'The article provides examples of how to collect and view traces in Evidently Cloud after setup.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'initializing Tracely for LLM',\n",
       "  'summary_answer': 'Instructions on how to initialize Tracely for tracing LLM applications are outlined, including configuration options and import statements.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'evaluate LLM outputs Evidently',\n",
       "  'summary_answer': 'The article discusses how to run evaluations on captured LLM outputs using the Evidently library.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'tracing session in LLM app',\n",
       "  'summary_answer': 'Best practices for tracing sessions in an LLM application are explained, including session management and capturing attributes.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'adversarial testing examples',\n",
       "  'summary_answer': 'The article provides detailed steps for creating adversarial datasets, including examples of different test scenarios like harmful content and forbidden topics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/adversarial_data.mdx'},\n",
       " {'question': 'how to generate adversarial test data',\n",
       "  'summary_answer': 'You can generate adversarial test data by selecting predefined scenarios in the Evidently platform, automating the creation of edge cases to evaluate AI robustness.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/adversarial_data.mdx'},\n",
       " {'question': 'generate input test cases for AI',\n",
       "  'summary_answer': 'The article describes methods for generating synthetic input test cases to evaluate AI responses in various scenarios, enhancing test coverage.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/input_data.mdx'},\n",
       " {'question': 'how to create synthetic inputs in Evidently',\n",
       "  'summary_answer': 'It outlines the steps to create synthetic inputs using the Evidently UI, providing guidance on setting up a project and generating relevant inputs from examples.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/input_data.mdx'},\n",
       " {'question': 'benefits of synthetic input generation',\n",
       "  'summary_answer': 'The article highlights the advantages of synthetic input generation, including expanding test coverage and evaluating AI performance on diverse queries.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'synthetic-data/input_data.mdx'},\n",
       " {'question': 'review generated input datasets Evidently',\n",
       "  'summary_answer': 'The content explains how to review and refine generated input datasets within the Evidently platform, including options for editing and saving the dataset.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/input_data.mdx'},\n",
       " {'question': 'generate adversarial inputs example',\n",
       "  'summary_answer': 'It discusses generating adversarial inputs through specific descriptions to test AI systems against edge cases, a key aspect of synthetic input creation.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/input_data.mdx'},\n",
       " {'question': 'generate synthetic test data examples',\n",
       "  'summary_answer': 'The article explains how to generate synthetic test inputs and outputs using Evidently Cloud, providing examples for various testing scenarios.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/introduction.mdx'},\n",
       " {'question': 'using synthetic data for AI testing',\n",
       "  'summary_answer': 'It discusses the applications of synthetic data in evaluating AI systems, including experiments and regression testing.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'synthetic-data/introduction.mdx'},\n",
       " {'question': 'adversarial testing synthetic data',\n",
       "  'summary_answer': 'The article details how to create adversarial inputs to test AI system vulnerabilities using synthetic data generation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/introduction.mdx'},\n",
       " {'question': 'synthetic data regression testing',\n",
       "  'summary_answer': 'Learn how to use synthetic data for regression testing to validate changes before deployment in your AI models.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'synthetic-data/introduction.mdx'},\n",
       " {'question': 'Evidently Cloud pricing',\n",
       "  'summary_answer': 'The article mentions the availability of the synthetic data generation feature in Evidently Cloud and encourages checking pricing details.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'synthetic-data/introduction.mdx'},\n",
       " {'question': 'test AI with synthetic inputs',\n",
       "  'summary_answer': 'It provides insights on generating various types of synthetic inputs to effectively test and evaluate AI applications.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/introduction.mdx'},\n",
       " {'question': 'evaluate results synthetic data AI',\n",
       "  'summary_answer': 'The article shows how to run generated synthetic data through AI systems and evaluate results using the Evidently libraries.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/introduction.mdx'},\n",
       " {'question': 'create RAG evaluation dataset',\n",
       "  'summary_answer': 'The article provides a step-by-step guide on generating a RAG evaluation dataset from a knowledge base, including uploading files and refining test cases in the Evidently UI.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/rag_data.mdx'},\n",
       " {'question': 'when to use synthetic data for LLM',\n",
       "  'summary_answer': 'The article explains that synthetic data is crucial when real data is not available, especially for scaling datasets and testing edge cases in LLM evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'synthetic-data/why_synthetic.mdx'},\n",
       " {'question': 'generate synthetic test data for AI evaluation',\n",
       "  'summary_answer': 'It describes methods for generating synthetic test datasets to enhance evaluation, such as creating hundreds of structured test cases quickly and covering edge scenarios for AI systems.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/why_synthetic.mdx'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddda943b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26a253d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_questions = pd.DataFrame(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "593a8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.to_csv('ground_truth_evidently.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6cea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d173f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
