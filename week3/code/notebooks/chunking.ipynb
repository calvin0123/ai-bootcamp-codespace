{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3141053",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61511e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f45f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from minsearch import Index\n",
    "import docs\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ea2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ground_truth = pd.read_csv('../evals/ground_truth_evidently.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1afb2909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Evidently mapping roles and types',\n",
       " 'summary_answer': 'The article discusses how to accurately specify roles and types for each column in your dataset to avoid errors in evaluations.',\n",
       " 'difficulty': 'advanced',\n",
       " 'intent': 'text',\n",
       " 'filename': 'docs/library/data_definition.mdx'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d035c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86bf8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def calculate_num_tokens(search_results):\n",
    "    json_result = json.dumps(search_results)\n",
    "    num_tokens = len(encoding.encode(json_result))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a110e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "                break\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f995d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3eae99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        ground_truth,\n",
    "        search_function,\n",
    "        question_column='question',\n",
    "        id_column='filename'\n",
    "):\n",
    "    relevance_total = []\n",
    "    tokens = []\n",
    "\n",
    "    for q in ground_truth:\n",
    "        doc_id = q[id_column]\n",
    "        results = search_function(q[question_column])\n",
    "        num_tokens = calculate_num_tokens(results)\n",
    "        tokens.append(num_tokens)\n",
    "        relevance = [d[id_column] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    avg_tokens = sum(tokens) / len(tokens)\n",
    "    \n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "        'num_tokens': avg_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2000\n",
    "step = 1000\n",
    "top_k = 5\n",
    "\n",
    "def evaluate_params(size, step, top_k):\n",
    "    chunks = docs.chunk_documents(parsed_data, size=size, step=step)\n",
    "    \n",
    "    index = Index(\n",
    "        text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    "    )\n",
    "    \n",
    "    index.fit(chunks)\n",
    "    \n",
    "    def search(query: str):\n",
    "        return index.search(\n",
    "            query=query,\n",
    "            num_results=top_k,\n",
    "        )\n",
    "    \n",
    "    return evaluate(ground_truth, search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b5f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09cd1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 1000, 'step': 1000, 'top_k': 5, 'hit_rate': 0.4279918864097363, 'num_tokens': 1351.870182555781}\n",
      "{'size': 1000, 'step': 1000, 'top_k': 10, 'hit_rate': 0.5294117647058824, 'num_tokens': 2747.5943204868154}\n",
      "{'size': 1000, 'step': 1000, 'top_k': 15, 'hit_rate': 0.6105476673427992, 'num_tokens': 4089.4624746450304}\n",
      "{'size': 2000, 'step': 1000, 'top_k': 5, 'hit_rate': 0.43610547667342797, 'num_tokens': 2488.4543610547667}\n",
      "{'size': 2000, 'step': 1000, 'top_k': 10, 'hit_rate': 0.537525354969574, 'num_tokens': 5075.137931034483}\n",
      "{'size': 2000, 'step': 1000, 'top_k': 15, 'hit_rate': 0.6308316430020284, 'num_tokens': 7576.789046653144}\n",
      "{'size': 2000, 'step': 2000, 'top_k': 5, 'hit_rate': 0.4787018255578093, 'num_tokens': 2361.9269776876267}\n",
      "{'size': 2000, 'step': 2000, 'top_k': 10, 'hit_rate': 0.6064908722109533, 'num_tokens': 4684.156186612576}\n",
      "{'size': 2000, 'step': 2000, 'top_k': 15, 'hit_rate': 0.6754563894523327, 'num_tokens': 7019.182555780933}\n",
      "{'size': 3000, 'step': 1000, 'top_k': 5, 'hit_rate': 0.45030425963488846, 'num_tokens': 3540.087221095335}\n",
      "{'size': 3000, 'step': 1000, 'top_k': 10, 'hit_rate': 0.5456389452332657, 'num_tokens': 7197.210953346856}\n",
      "{'size': 3000, 'step': 1000, 'top_k': 15, 'hit_rate': 0.6328600405679513, 'num_tokens': 10740.245436105477}\n",
      "{'size': 3000, 'step': 2000, 'top_k': 5, 'hit_rate': 0.486815415821501, 'num_tokens': 3398.0993914807304}\n",
      "{'size': 3000, 'step': 2000, 'top_k': 10, 'hit_rate': 0.6105476673427992, 'num_tokens': 6723.529411764706}\n",
      "{'size': 3000, 'step': 2000, 'top_k': 15, 'hit_rate': 0.6896551724137931, 'num_tokens': 10143.369168356998}\n",
      "{'size': 3000, 'step': 3000, 'top_k': 5, 'hit_rate': 0.5010141987829615, 'num_tokens': 3157.162271805274}\n",
      "{'size': 3000, 'step': 3000, 'top_k': 10, 'hit_rate': 0.6267748478701826, 'num_tokens': 6242.11967545639}\n",
      "{'size': 3000, 'step': 3000, 'top_k': 15, 'hit_rate': 0.7038539553752535, 'num_tokens': 9399.275862068966}\n",
      "{'size': 5000, 'step': 1000, 'top_k': 5, 'hit_rate': 0.460446247464503, 'num_tokens': 5305.474645030426}\n",
      "{'size': 5000, 'step': 1000, 'top_k': 10, 'hit_rate': 0.5862068965517241, 'num_tokens': 10847.744421906693}\n",
      "{'size': 5000, 'step': 1000, 'top_k': 15, 'hit_rate': 0.6186612576064908, 'num_tokens': 16463.077079107505}\n",
      "{'size': 5000, 'step': 2000, 'top_k': 5, 'hit_rate': 0.4949290060851927, 'num_tokens': 5095.924949290061}\n",
      "{'size': 5000, 'step': 2000, 'top_k': 10, 'hit_rate': 0.6206896551724138, 'num_tokens': 10225.117647058823}\n",
      "{'size': 5000, 'step': 2000, 'top_k': 15, 'hit_rate': 0.6957403651115619, 'num_tokens': 15512.326572008114}\n",
      "{'size': 5000, 'step': 3000, 'top_k': 5, 'hit_rate': 0.5111561866125761, 'num_tokens': 4887.782961460446}\n",
      "{'size': 5000, 'step': 3000, 'top_k': 10, 'hit_rate': 0.640973630831643, 'num_tokens': 9866.983772819473}\n",
      "{'size': 5000, 'step': 3000, 'top_k': 15, 'hit_rate': 0.7139959432048681, 'num_tokens': 14923.632860040569}\n"
     ]
    }
   ],
   "source": [
    "sizes = [1000, 2000, 3000, 5000]\n",
    "steps = [1000, 2000, 3000]\n",
    "top_ks = [5, 10, 15]\n",
    "\n",
    "results = []\n",
    "\n",
    "for size in sizes:\n",
    "    for step in steps:\n",
    "        if step > size:\n",
    "            continue\n",
    "\n",
    "        for top_k in top_ks:\n",
    "            result = evaluate_params(size, step, top_k)\n",
    "            record = {\n",
    "                'size': size,\n",
    "                'step': step,\n",
    "                'top_k': top_k,\n",
    "                'hit_rate': result['hit_rate'],\n",
    "                'num_tokens': result['num_tokens']\n",
    "            }\n",
    "            print(record)\n",
    "            results.append(record)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b03720f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results, columns=['size', 'step', 'top_k', 'hit_rate', 'num_tokens'])\n",
    "\n",
    "alpha = 2\n",
    "beta = 0.5\n",
    "df['score'] = (df.hit_rate ** alpha) / ((df.num_tokens / 1000) ** beta)\n",
    "\n",
    "df = df.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e932ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>step</th>\n",
       "      <th>top_k</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>15</td>\n",
       "      <td>0.610548</td>\n",
       "      <td>4089.462475</td>\n",
       "      <td>0.184334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>15</td>\n",
       "      <td>0.675456</td>\n",
       "      <td>7019.182556</td>\n",
       "      <td>0.172207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.606491</td>\n",
       "      <td>4684.156187</td>\n",
       "      <td>0.169955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>2747.594320</td>\n",
       "      <td>0.169087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>15</td>\n",
       "      <td>0.703854</td>\n",
       "      <td>9399.275862</td>\n",
       "      <td>0.161591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    size  step  top_k  hit_rate   num_tokens     score\n",
       "2   1000  1000     15  0.610548  4089.462475  0.184334\n",
       "8   2000  2000     15  0.675456  7019.182556  0.172207\n",
       "7   2000  2000     10  0.606491  4684.156187  0.169955\n",
       "1   1000  1000     10  0.529412  2747.594320  0.169087\n",
       "17  3000  3000     15  0.703854  9399.275862  0.161591"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4878e40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n            hit_rate ** alpha\\nscore = ------------------------\\n        (num_tokens/1000) ** beta\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This represents \"retrieval quality adjusted for the cost of processing tokens\".\n",
    "\"\"\"\n",
    "\n",
    "            hit_rate ** alpha\n",
    "score = ------------------------\n",
    "        (num_tokens/1000) ** beta\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da141e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import search_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d334458",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = search_agent.AgentConfig(\n",
    "    chunk_size=1000,\n",
    "    chunk_step=1000,\n",
    "    top_k=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d790ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(model=OpenAIChatModel(), name='search', end_strategy='early', model_settings=None, output_type=<class 'search_agent.SearchResultArticle'>, instrument=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = search_agent.create_agent(config)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7b24c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.eval_orchestrator import run_full_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accc1268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION PIPELINE START ========================================\n",
      "Start time: 2025-10-30 22:05:35\n",
      "Configuration:\n",
      "  Ground truth: ../evals/ground_truth_sample_25_2025-10-30-21-03.csv\n",
      "  Agent model: gpt-4o-mini\n",
      "  Judge model: gpt-5-nano\n",
      "  Max concurrency: 10\n",
      "\n",
      "=== STEP 1: AGENT EVALUATION =========================================\n",
      "Loaded 25 ground truth questions\n",
      "Running agent evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1603580ce64985aebddc136d677d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forcing output\n",
      "forcing output\n",
      "forcing output\n",
      "forcing output\n",
      "Error processing {'question': 'custom text evaluator in Python', 'summary_answer': 'The article outlines how to create a custom text evaluator using the Evidently framework by implementing functions that evaluate data in specified columns.', 'difficulty': 'beginner', 'intent': 'code', 'filename': 'metrics/customize_descriptor.mdx'}: status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 200000, Requested 265. Please try again in 79ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 493, in _completions_create\n",
      "    return await self.client.chat.completions.create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 200000, Requested 265. Please try again in 79ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/notebooks/../evals/eval_agent_run.py\", line 48, in run_agent_on_question\n",
      "    result = await agent.run(question_record[\"question\"])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/agent/abstract.py\", line 235, in run\n",
      "    async for node in agent_run:\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/run.py\", line 150, in __anext__\n",
      "    next_node = await self._graph_run.__anext__()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 758, in __anext__\n",
      "    return await self.next(self._next_node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 731, in next\n",
      "    self._next_node = await node.run(ctx)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 416, in run\n",
      "    return await self._make_request(ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 458, in _make_request\n",
      "    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 411, in request\n",
      "    response = await self._completions_create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 523, in _completions_create\n",
      "    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n",
      "pydantic_ai.exceptions.ModelHTTPError: status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 200000, Requested 265. Please try again in 79ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing {'question': 'local workspace report saving', 'summary_answer': 'The article outlines how to save reports to both your local workspace and Evidently Cloud, including specific code examples for each.', 'difficulty': 'beginner', 'intent': 'code', 'filename': 'docs/library/output_formats.mdx'}: status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 194065, Requested 7851. Please try again in 574ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 493, in _completions_create\n",
      "    return await self.client.chat.completions.create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 194065, Requested 7851. Please try again in 574ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/notebooks/../evals/eval_agent_run.py\", line 48, in run_agent_on_question\n",
      "    result = await agent.run(question_record[\"question\"])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/agent/abstract.py\", line 235, in run\n",
      "    async for node in agent_run:\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/run.py\", line 150, in __anext__\n",
      "    next_node = await self._graph_run.__anext__()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 758, in __anext__\n",
      "    return await self.next(self._next_node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 731, in next\n",
      "    self._next_node = await node.run(ctx)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 416, in run\n",
      "    return await self._make_request(ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 458, in _make_request\n",
      "    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 411, in request\n",
      "    response = await self._completions_create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 523, in _completions_create\n",
      "    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n",
      "pydantic_ai.exceptions.ModelHTTPError: status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 194065, Requested 7851. Please try again in 574ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forcing output\n",
      "Error processing {'question': 'examples of data validation metrics', 'summary_answer': 'The article provides comprehensive examples of validation metrics that ensure data meets quality and integrity standards.', 'difficulty': 'beginner', 'intent': 'text', 'filename': 'metrics/all_metrics.mdx'}: status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 198784, Requested 15764. Please try again in 4.364s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 493, in _completions_create\n",
      "    return await self.client.chat.completions.create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 198784, Requested 15764. Please try again in 4.364s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/notebooks/../evals/eval_agent_run.py\", line 48, in run_agent_on_question\n",
      "    result = await agent.run(question_record[\"question\"])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/agent/abstract.py\", line 235, in run\n",
      "    async for node in agent_run:\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/run.py\", line 150, in __anext__\n",
      "    next_node = await self._graph_run.__anext__()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 758, in __anext__\n",
      "    return await self.next(self._next_node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 731, in next\n",
      "    self._next_node = await node.run(ctx)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 416, in run\n",
      "    return await self._make_request(ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 458, in _make_request\n",
      "    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 411, in request\n",
      "    response = await self._completions_create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 523, in _completions_create\n",
      "    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n",
      "pydantic_ai.exceptions.ModelHTTPError: status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 198784, Requested 15764. Please try again in 4.364s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.0563\n",
      "Results saved to reports/eval-run-2025-10-30-22-06.bin\n",
      "\n",
      "✓ Agent evaluation completed\n",
      "  Evaluated: 22 questions\n",
      "  Results saved: reports/eval-run-2025-10-30-22-06.bin\n",
      "  Agent Run Costs:\n",
      "    Input tokens cost:  $  0.0479\n",
      "    Output tokens cost: $  0.0083\n",
      "    Total cost:         $  0.0563\n",
      "\n",
      "=== STEP 2: JUDGE EVALUATION =========================================\n",
      "Loading evaluation results from reports/eval-run-2025-10-30-22-06.bin...\n",
      "Loaded 22 evaluation results\n",
      "Loading reference documents...\n",
      "Creating judge agent...\n",
      "Running judge evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d9642cf8bc4fff8fb17aa31f2ec780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.0276\n",
      "Judge results saved to: reports/eval-judge-2025-10-30-22-06.bin\n",
      "\n",
      "✓ Judge evaluation completed\n",
      "  Evaluated: 22 results\n",
      "  Judge results saved: reports/eval-judge-2025-10-30-22-06.bin\n",
      "  Judge Evaluation Costs:\n",
      "    Input tokens cost:  $  0.0050\n",
      "    Output tokens cost: $  0.0226\n",
      "    Total cost:         $  0.0276\n",
      "\n",
      "=== EVALUATION SUMMARY ===============================================\n",
      "\n",
      "Execution Time:\n",
      "  Duration: 102.2 seconds\n",
      "\n",
      "Dataset:\n",
      "  Questions evaluated: 22\n",
      "\n",
      "Evaluation Metrics:\n",
      "  ✗ CheckName.instructions_follow  50.0%\n",
      "  ✓ CheckName.instructions_avoid 100.0%\n",
      "  ✓ CheckName.answer_relevant 100.0%\n",
      "  ✓ CheckName.answer_clear    100.0%\n",
      "  ✓ CheckName.answer_match     95.0%\n",
      "  ✓ CheckName.answer_citations 100.0%\n",
      "  ✓ CheckName.completeness     85.0%\n",
      "  ⚠ CheckName.tool_call_search  63.6%\n",
      "\n",
      "Overall Score: 86.7%\n",
      "\n",
      "=== COST BREAKDOWN ===================================================\n",
      "Agent Run Costs:\n",
      "  Input tokens cost:  $  0.0479\n",
      "  Output tokens cost: $  0.0083\n",
      "  Total cost:         $  0.0563\n",
      "Judge Evaluation Costs:\n",
      "  Input tokens cost:  $  0.0050\n",
      "  Output tokens cost: $  0.0226\n",
      "  Total cost:         $  0.0276\n",
      "======================================================================\n",
      "TOTAL Costs:\n",
      "  Input tokens cost:  $  0.0529\n",
      "  Output tokens cost: $  0.0309\n",
      "  Total cost:         $  0.0838\n",
      "\n",
      "=== EVALUATION COMPLETE ==============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'run_results_path': 'reports/eval-run-2025-10-30-22-06.bin',\n",
       " 'judge_results_path': 'reports/eval-judge-2025-10-30-22-06.bin',\n",
       " 'run_cost': CostInfo(input_cost=0.0479394, output_cost=0.0083412, total_cost=0.0562806),\n",
       " 'judge_cost': CostInfo(input_cost=0.004963950000000001, output_cost=0.0225952, total_cost=0.02755915),\n",
       " 'total_cost': CostInfo(input_cost=0.05290335, output_cost=0.0309364, total_cost=0.08383975),\n",
       " 'df_run':                                        question  \\\n",
       " 0                                  what is NDCG   \n",
       " 1         different types of Tests in Evidently   \n",
       " 2       Evidently packages installation for LLM   \n",
       " 3         multi-class classification evaluation   \n",
       " 4             add pie charts to dashboard panel   \n",
       " 5                    evidently library features   \n",
       " 6      evaluate text emotions using huggingface   \n",
       " 7                        output formats options   \n",
       " 8      predicted vs actual values visualization   \n",
       " 9                      using OpenAI API for LLM   \n",
       " 10                  optimization of LLM prompts   \n",
       " 11             validating syntax in descriptors   \n",
       " 12                   LLM evaluation methods RAG   \n",
       " 13  examples of output formatting in evaluation   \n",
       " 14          evidently tests for LLM evaluations   \n",
       " 15            how to use classification presets   \n",
       " 16              context relevance metric in RAG   \n",
       " 17               add descriptors to LLM dataset   \n",
       " 18       integrating huggingface with evidently   \n",
       " 19             no code data evaluation tutorial   \n",
       " 20               Python code for LLM evaluation   \n",
       " 21       customizing recommender system reports   \n",
       " \n",
       "                                                answer  \\\n",
       " 0   # Understanding NDCG\\n\\n## What is NDCG?\\n\\nND...   \n",
       " 1   # Types of Tests in Evidently\\n\\n## Overview o...   \n",
       " 2   # Installation of Evidently for LLM\\n\\n## Inst...   \n",
       " 3   # Multi-Class Classification Evaluation with E...   \n",
       " 4   # Adding Pie Charts to the Evidently Dashboard...   \n",
       " 5   # Evidently Library Features Overview\\n\\n## Co...   \n",
       " 6   # Integrating Text Emotion Evaluation with Hug...   \n",
       " 7   # Evidently Output Formats\\n\\n## Output Format...   \n",
       " 8   # Visualization of Predicted vs Actual Values ...   \n",
       " 9   # Using OpenAI API with Evidently\\n\\n## Overvi...   \n",
       " 10  # Optimizing LLM Prompts with Evidently\\n\\n## ...   \n",
       " 11  # Validating Syntax in Descriptors\\n\\n## Overv...   \n",
       " 12  # LLM Evaluation Methods Using RAG\\n\\n## Retri...   \n",
       " 13  # Output Formatting in Evidently Evaluations\\n...   \n",
       " 14  # Evidently LLM Evaluations\\n\\n## Overview of ...   \n",
       " 15  # Using Classification Presets in Evidently\\n\\...   \n",
       " 16  # Context Relevance Metric in Evidently for RA...   \n",
       " 17  # Adding Descriptors to LLM Datasets in Eviden...   \n",
       " 18  # Integrating Hugging Face with Evidently\\n\\n#...   \n",
       " 19  # No-Code Data Evaluation Tutorial in Evidentl...   \n",
       " 20  # Python Code for LLM Evaluation with Evidentl...   \n",
       " 21  # Customizing Recommender System Reports in Ev...   \n",
       " \n",
       "                                              messages  tool_call_number  \\\n",
       " 0   [{'kind': 'user-prompt', 'content': 'what is N...                 4   \n",
       " 1   [{'kind': 'user-prompt', 'content': 'different...                 6   \n",
       " 2   [{'kind': 'user-prompt', 'content': 'Evidently...                 5   \n",
       " 3   [{'kind': 'user-prompt', 'content': 'multi-cla...                 4   \n",
       " 4   [{'kind': 'user-prompt', 'content': 'add pie c...                 5   \n",
       " 5   [{'kind': 'user-prompt', 'content': 'evidently...                 5   \n",
       " 6   [{'kind': 'user-prompt', 'content': 'evaluate ...                 5   \n",
       " 7   [{'kind': 'user-prompt', 'content': 'output fo...                 5   \n",
       " 8   [{'kind': 'user-prompt', 'content': 'predicted...                 8   \n",
       " 9   [{'kind': 'user-prompt', 'content': 'using Ope...                 6   \n",
       " 10  [{'kind': 'user-prompt', 'content': 'optimizat...                 5   \n",
       " 11  [{'kind': 'user-prompt', 'content': 'validatin...                 5   \n",
       " 12  [{'kind': 'user-prompt', 'content': 'LLM evalu...                 5   \n",
       " 13  [{'kind': 'user-prompt', 'content': 'examples ...                 5   \n",
       " 14  [{'kind': 'user-prompt', 'content': 'evidently...                 5   \n",
       " 15  [{'kind': 'user-prompt', 'content': 'how to us...                 6   \n",
       " 16  [{'kind': 'user-prompt', 'content': 'context r...                 5   \n",
       " 17  [{'kind': 'user-prompt', 'content': 'add descr...                 5   \n",
       " 18  [{'kind': 'user-prompt', 'content': 'integrati...                 5   \n",
       " 19  [{'kind': 'user-prompt', 'content': 'no code d...                 5   \n",
       " 20  [{'kind': 'user-prompt', 'content': 'Python co...                 5   \n",
       " 21  [{'kind': 'user-prompt', 'content': 'customizi...                 5   \n",
       " \n",
       "     requests                                  original_question  \\\n",
       " 0          2  {'question': 'what is NDCG', 'summary_answer':...   \n",
       " 1          2  {'question': 'different types of Tests in Evid...   \n",
       " 2          2  {'question': 'Evidently packages installation ...   \n",
       " 3          2  {'question': 'multi-class classification evalu...   \n",
       " 4          2  {'question': 'add pie charts to dashboard pane...   \n",
       " 5          2  {'question': 'evidently library features', 'su...   \n",
       " 6          2  {'question': 'evaluate text emotions using hug...   \n",
       " 7          2  {'question': 'output formats options', 'summar...   \n",
       " 8          3  {'question': 'predicted vs actual values visua...   \n",
       " 9          2  {'question': 'using OpenAI API for LLM', 'summ...   \n",
       " 10         2  {'question': 'optimization of LLM prompts', 's...   \n",
       " 11         2  {'question': 'validating syntax in descriptors...   \n",
       " 12         2  {'question': 'LLM evaluation methods RAG', 'su...   \n",
       " 13         2  {'question': 'examples of output formatting in...   \n",
       " 14         2  {'question': 'evidently tests for LLM evaluati...   \n",
       " 15         2  {'question': 'how to use classification preset...   \n",
       " 16         2  {'question': 'context relevance metric in RAG'...   \n",
       " 17         2  {'question': 'add descriptors to LLM dataset',...   \n",
       " 18         2  {'question': 'integrating huggingface with evi...   \n",
       " 19         2  {'question': 'no code data evaluation tutorial...   \n",
       " 20         2  {'question': 'Python code for LLM evaluation',...   \n",
       " 21         2  {'question': 'customizing recommender system r...   \n",
       " \n",
       "                                       original_result  \n",
       " 0   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 1   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 2   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 3   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 4   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 5   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 6   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 7   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 8   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 9   AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 10  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 11  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 12  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 13  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 14  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 15  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 16  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 17  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 18  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 19  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 20  AgentRunResult(output=SearchResultArticle(foun...  \n",
       " 21  AgentRunResult(output=SearchResultArticle(foun...  ,\n",
       " 'df_eval':                                        question CheckName.instructions_follow  \\\n",
       " 0           evidently tests for LLM evaluations                          True   \n",
       " 1      evaluate text emotions using huggingface                          True   \n",
       " 2                Python code for LLM evaluation                         False   \n",
       " 3                        output formats options                          True   \n",
       " 4             how to use classification presets                         False   \n",
       " 5              no code data evaluation tutorial                          True   \n",
       " 6               context relevance metric in RAG                         False   \n",
       " 7                                  what is NDCG                          True   \n",
       " 8        customizing recommender system reports                          True   \n",
       " 9   examples of output formatting in evaluation                         False   \n",
       " 10        multi-class classification evaluation                         False   \n",
       " 11       integrating huggingface with evidently                         False   \n",
       " 12                     using OpenAI API for LLM                          True   \n",
       " 13               add descriptors to LLM dataset                           NaN   \n",
       " 14                   evidently library features                         False   \n",
       " 15                  optimization of LLM prompts                         False   \n",
       " 16        different types of Tests in Evidently                         False   \n",
       " 17      Evidently packages installation for LLM                          True   \n",
       " 18     predicted vs actual values visualization                          True   \n",
       " 19             validating syntax in descriptors                          True   \n",
       " 20            add pie charts to dashboard panel                         False   \n",
       " 21                   LLM evaluation methods RAG                           NaN   \n",
       " \n",
       "    CheckName.instructions_avoid CheckName.answer_relevant  \\\n",
       " 0                          True                      True   \n",
       " 1                          True                      True   \n",
       " 2                          True                      True   \n",
       " 3                          True                      True   \n",
       " 4                          True                      True   \n",
       " 5                          True                      True   \n",
       " 6                          True                      True   \n",
       " 7                          True                      True   \n",
       " 8                          True                      True   \n",
       " 9                          True                      True   \n",
       " 10                         True                      True   \n",
       " 11                         True                      True   \n",
       " 12                         True                      True   \n",
       " 13                          NaN                       NaN   \n",
       " 14                         True                      True   \n",
       " 15                         True                      True   \n",
       " 16                         True                      True   \n",
       " 17                         True                      True   \n",
       " 18                         True                      True   \n",
       " 19                         True                      True   \n",
       " 20                         True                      True   \n",
       " 21                          NaN                       NaN   \n",
       " \n",
       "    CheckName.answer_clear CheckName.answer_match CheckName.answer_citations  \\\n",
       " 0                    True                   True                       True   \n",
       " 1                    True                   True                       True   \n",
       " 2                    True                   True                       True   \n",
       " 3                    True                   True                       True   \n",
       " 4                    True                   True                       True   \n",
       " 5                    True                   True                       True   \n",
       " 6                    True                   True                       True   \n",
       " 7                    True                   True                       True   \n",
       " 8                    True                   True                       True   \n",
       " 9                    True                  False                       True   \n",
       " 10                   True                   True                       True   \n",
       " 11                   True                   True                       True   \n",
       " 12                   True                   True                       True   \n",
       " 13                    NaN                    NaN                        NaN   \n",
       " 14                   True                   True                       True   \n",
       " 15                   True                   True                       True   \n",
       " 16                   True                   True                       True   \n",
       " 17                   True                   True                       True   \n",
       " 18                   True                   True                       True   \n",
       " 19                   True                   True                       True   \n",
       " 20                   True                   True                       True   \n",
       " 21                    NaN                    NaN                        NaN   \n",
       " \n",
       "    CheckName.completeness  CheckName.tool_call_search  \n",
       " 0                    True                        True  \n",
       " 1                    True                        True  \n",
       " 2                    True                       False  \n",
       " 3                    True                        True  \n",
       " 4                    True                        True  \n",
       " 5                    True                        True  \n",
       " 6                    True                       False  \n",
       " 7                    True                        True  \n",
       " 8                    True                        True  \n",
       " 9                    True                       False  \n",
       " 10                  False                       False  \n",
       " 11                  False                       False  \n",
       " 12                   True                        True  \n",
       " 13                    NaN                        True  \n",
       " 14                   True                       False  \n",
       " 15                  False                       False  \n",
       " 16                   True                       False  \n",
       " 17                   True                        True  \n",
       " 18                   True                        True  \n",
       " 19                   True                        True  \n",
       " 20                   True                        True  \n",
       " 21                    NaN                        True  ,\n",
       " 'metrics': CheckName.instructions_follow         0.5\n",
       " CheckName.instructions_avoid          1.0\n",
       " CheckName.answer_relevant             1.0\n",
       " CheckName.answer_clear                1.0\n",
       " CheckName.answer_match               0.95\n",
       " CheckName.answer_citations            1.0\n",
       " CheckName.completeness               0.85\n",
       " CheckName.tool_call_search       0.636364\n",
       " dtype: object,\n",
       " 'duration_seconds': 102.192953,\n",
       " 'timestamp': datetime.datetime(2025, 10, 30, 22, 5, 35, 788872)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_full_evaluation(agent, csv_path='../evals/ground_truth_sample_25_2025-10-30-21-03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6951f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
