{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2367f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07801345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ground_truth = pd.read_csv('./ground_truth_evidently.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ad2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_ground_truth.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8f32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_sample.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e453be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ground_truth[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f55b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'create Dataset object Evidently',\n",
       " 'summary_answer': 'To create a `Dataset` object in Evidently, you can use the `Dataset.from_pandas()` method with a data definition to specify the roles and types of your data columns.',\n",
       " 'difficulty': 'beginner',\n",
       " 'intent': 'code',\n",
       " 'filename': 'docs/library/data_definition.mdx'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e760578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main\n",
    "agent = main.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feeab0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "async def map_progress(seq, f, max_concurrency=6):\n",
    "    \"\"\"Asynchronously map async function f over seq with progress bar.\"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    async def run(el):\n",
    "        async with semaphore:\n",
    "            return await f(el)\n",
    "\n",
    "    # create one coroutine per element\n",
    "    coros = [run(el) for el in seq]\n",
    "\n",
    "    # turn them into tasks that complete as they finish\n",
    "    completed = asyncio.as_completed(coros)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for coro in tqdm(completed, total=len(seq)):\n",
    "        result = await coro\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c06f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "async def run_agent(q):\n",
    "    try:\n",
    "        result = await agent.run(q['question'])\n",
    "        return (q, result)\n",
    "    except:\n",
    "        print(f'error processing {q}')\n",
    "        traceback.print_exc()\n",
    "        return (None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62d897ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf2d565d4034201ba5fecd3060a3486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forcing output\n",
      "forcing output\n",
      "forcing output\n",
      "forcing output\n",
      "error processing {'question': 'Evidently mapping roles and types', 'summary_answer': 'The article discusses how to accurately specify roles and types for each column in your dataset to avoid errors in evaluations.', 'difficulty': 'advanced', 'intent': 'text', 'filename': 'docs/library/data_definition.mdx'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 493, in _completions_create\n",
      "    return await self.client.chat.completions.create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 189614, Requested 12860. Please try again in 742ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/t7/tf0dc46s4hdc5zgvcgk2vhd80000gn/T/ipykernel_92897/3916385333.py\", line 5, in run_agent\n",
      "    result = await agent.run(q['question'])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/agent/abstract.py\", line 235, in run\n",
      "    async for node in agent_run:\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/run.py\", line 150, in __anext__\n",
      "    next_node = await self._graph_run.__anext__()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 758, in __anext__\n",
      "    return await self.next(self._next_node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 731, in next\n",
      "    self._next_node = await node.run(ctx)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 416, in run\n",
      "    return await self._make_request(ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 458, in _make_request\n",
      "    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 411, in request\n",
      "    response = await self._completions_create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 523, in _completions_create\n",
      "    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n",
      "pydantic_ai.exceptions.ModelHTTPError: status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 189614, Requested 12860. Please try again in 742ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error processing {'question': 'create Dataset object Evidently', 'summary_answer': 'To create a `Dataset` object in Evidently, you can use the `Dataset.from_pandas()` method with a data definition to specify the roles and types of your data columns.', 'difficulty': 'beginner', 'intent': 'code', 'filename': 'docs/library/data_definition.mdx'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 493, in _completions_create\n",
      "    return await self.client.chat.completions.create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 192286, Requested 17769. Please try again in 3.016s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/t7/tf0dc46s4hdc5zgvcgk2vhd80000gn/T/ipykernel_92897/3916385333.py\", line 5, in run_agent\n",
      "    result = await agent.run(q['question'])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/agent/abstract.py\", line 235, in run\n",
      "    async for node in agent_run:\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/run.py\", line 150, in __anext__\n",
      "    next_node = await self._graph_run.__anext__()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 758, in __anext__\n",
      "    return await self.next(self._next_node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py\", line 731, in next\n",
      "    self._next_node = await node.run(ctx)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 416, in run\n",
      "    return await self._make_request(ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 458, in _make_request\n",
      "    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 411, in request\n",
      "    response = await self._completions_create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yenchunchen/Desktop/Project/ai-bootcamp/week3/code/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py\", line 523, in _completions_create\n",
      "    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n",
      "pydantic_ai.exceptions.ModelHTTPError: status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-Dtqe0FuXWhvEjasqV8q84gMy on tokens per min (TPM): Limit 200000, Used 192286, Requested 17769. Please try again in 3.016s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    }
   ],
   "source": [
    "all_results = await map_progress(ground_truth, run_agent, max_concurrency=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11251939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('all-results-v1.bin', 'wb') as f_out:\n",
    "    pickle.dump(all_results, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84478e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, r = all_results[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0954b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'install Evidently for LLM',\n",
       " 'summary_answer': 'Instructions for installing the Evidently library to use for creating and evaluating LLM judges are provided within the tutorial.',\n",
       " 'difficulty': 'beginner',\n",
       " 'intent': 'code',\n",
       " 'filename': 'examples/LLM_judge.mdx'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a11e7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunUsage(input_tokens=12459, output_tokens=465, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, requests=2, tool_calls=5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d71c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "pricing = PricingConfig()\n",
    "\n",
    "\n",
    "def calculate_cost(model, all_results):\n",
    "    total_input = 0\n",
    "    total_output = 0\n",
    "    \n",
    "    for q, result in all_results:\n",
    "        if result is None:\n",
    "            continue\n",
    "        usage = result.usage()\n",
    "        total_input = total_input + usage.input_tokens\n",
    "        total_output = total_output + usage.output_tokens\n",
    "    \n",
    "    return pricing.calculate_cost(model, total_input, total_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bb02146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CostInfo(input_cost=0.04618365, output_cost=0.0082908, total_cost=0.05447445)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cost('gpt-4o-mini', all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f34703fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def simplify_messages(messages):\n",
    "    messages_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "\n",
    "        for original_part in m.parts:\n",
    "            kind = original_part.part_kind\n",
    "            # print(original_part)\n",
    "            part = {\n",
    "                'kind': kind\n",
    "            }\n",
    "            if kind == 'user-prompt':\n",
    "                part['content'] = original_part.content\n",
    "            if kind == 'tool-call':\n",
    "                if original_part.tool_name == 'final_result':\n",
    "                    continue\n",
    "    \n",
    "                part['tool_name'] = original_part.tool_name\n",
    "                part['args'] = json.loads(original_part.args)\n",
    "            if kind == 'tool-return':\n",
    "                continue\n",
    "            if kind == 'text':\n",
    "                part['content'] = original_part.content\n",
    "\n",
    "            parts.append(part)\n",
    "\n",
    "        if len(parts) > 0:\n",
    "            messages_simplified.extend(parts)\n",
    "\n",
    "    return messages_simplified\n",
    "\n",
    "def count_tool_calls(messages):\n",
    "    cnt = 0 \n",
    "    for m in messages:\n",
    "        if m['kind'] == 'tool-call':\n",
    "            cnt = cnt + 1\n",
    "    return cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6609fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building and inspecting evaluation results\n",
    "# log all the data\n",
    "\n",
    "\n",
    "def process_result(q, result):\n",
    "    row = {}\n",
    "\n",
    "    row['question'] = q['question']\n",
    "    row['answer'] = result.output.format_article()\n",
    "    row['messages'] = simplify_messages(result.new_messages())\n",
    "    row['num_tool_calls'] = count_tool_calls(row['messages']) \n",
    "\n",
    "    row['original_question'] = q\n",
    "    row['original_result'] = result\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for q, r in all_results:\n",
    "    if r is None:\n",
    "        continue\n",
    "    \n",
    "    usage = r.usage()\n",
    "    row = process_result(q, r)\n",
    "    # row = {\n",
    "    #     'question': q['question'],\n",
    "    #     'answer': r.output.format_article(),\n",
    "    #     'tool_call_number': usage.tool_calls,\n",
    "    #     'requests': usage.requests\n",
    "    # }\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6356e9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'SemanticSimilarity descriptor explained',\n",
       " 'answer': \"# Understanding SemanticSimilarity Descriptor in Evidently\\n\\n## What is the SemanticSimilarity Descriptor?\\n\\nThe `SemanticSimilarity` descriptor in Evidently is designed to evaluate the similarity between pairs of text data. It leverages models from the HuggingFace library for tasks related to natural language understanding. This can include evaluating aspects such as semantic relationships, sentiment similarity, or contextual coherence depending on the model used. The `SemanticSimilarity` descriptor essentially helps in quantifying how close the meanings of two texts are, providing valuable insights especially in applications like chatbots, content recommendations, or any system where understanding text relationships is crucial.\\n\\n### References\\n- [Use HuggingFace models](https://github.com/evidentlyai/docs/blob/main/metrics/customize_hf_descriptor.mdx)\\n## How to Use SemanticSimilarity?\\n\\nTo use the `SemanticSimilarity` descriptor, you will generally start by importing it into your Python environment:  \\n```python\\nfrom evidently.descriptors import SemanticSimilarity\\n```\\n\\nYou'll also need to prepare your data in an appropriate format, typically as a pandas DataFrame where the descriptors can operate on the text columns. The parameters you set can specify the models and configurations for evaluating the text pairs. For example, you might indicate which HuggingFace transformer model to utilize for the similarity scoring.\\n\\n### References\\n- [Use HuggingFace models](https://github.com/evidentlyai/docs/blob/main/metrics/customize_hf_descriptor.mdx)\\n## References\\n- [Use HuggingFace models](https://github.com/evidentlyai/docs/blob/main/metrics/customize_hf_descriptor.mdx)\\n\",\n",
       " 'messages': [{'kind': 'user-prompt',\n",
       "   'content': 'SemanticSimilarity descriptor explained'},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'SemanticSimilarity descriptor in Evidently'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently library SemanticSimilarity function'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently SemanticSimilarity API details'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently descriptor types explanations'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'how to use SemanticSimilarity in Evidently'}}],\n",
       " 'num_tool_calls': 5,\n",
       " 'original_question': {'question': 'SemanticSimilarity descriptor explained',\n",
       "  'summary_answer': 'SemanticSimilarity() describes how to evaluate text similarity between columns using sentence embeddings as shown in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " 'original_result': AgentRunResult(output=SearchResultArticle(found_answer=True, title='Understanding SemanticSimilarity Descriptor in Evidently', sections=[Section(heading='What is the SemanticSimilarity Descriptor?', content='The `SemanticSimilarity` descriptor in Evidently is designed to evaluate the similarity between pairs of text data. It leverages models from the HuggingFace library for tasks related to natural language understanding. This can include evaluating aspects such as semantic relationships, sentiment similarity, or contextual coherence depending on the model used. The `SemanticSimilarity` descriptor essentially helps in quantifying how close the meanings of two texts are, providing valuable insights especially in applications like chatbots, content recommendations, or any system where understanding text relationships is crucial.', references=[Reference(title='Use HuggingFace models', filename='metrics/customize_hf_descriptor.mdx')]), Section(heading='How to Use SemanticSimilarity?', content=\"To use the `SemanticSimilarity` descriptor, you will generally start by importing it into your Python environment:  \\n```python\\nfrom evidently.descriptors import SemanticSimilarity\\n```\\n\\nYou'll also need to prepare your data in an appropriate format, typically as a pandas DataFrame where the descriptors can operate on the text columns. The parameters you set can specify the models and configurations for evaluating the text pairs. For example, you might indicate which HuggingFace transformer model to utilize for the similarity scoring.\", references=[Reference(title='Use HuggingFace models', filename='metrics/customize_hf_descriptor.mdx')])], references=[Reference(title='Use HuggingFace models', filename='metrics/customize_hf_descriptor.mdx')]))}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f99d6d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>messages</th>\n",
       "      <th>num_tool_calls</th>\n",
       "      <th>original_question</th>\n",
       "      <th>original_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SemanticSimilarity descriptor explained</td>\n",
       "      <td># Understanding SemanticSimilarity Descriptor ...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'SemanticS...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'question': 'SemanticSimilarity descriptor ex...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>using is_critical parameter in alerts</td>\n",
       "      <td># Using `is_critical` Parameter in Evidently A...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'using is_...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'question': 'using is_critical parameter in a...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quickstart Evidently setup</td>\n",
       "      <td># Quickstart Setup for Evidently\\n\\n## Creatin...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'Quickstar...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'question': 'Quickstart Evidently setup', 'su...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drift detection for numerical columns</td>\n",
       "      <td># Drift Detection for Numerical Columns in Evi...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'Drift det...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'question': 'Drift detection for numerical co...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-letter character percentage function</td>\n",
       "      <td># Non-letter Character Percentage Function in ...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'non-lette...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'question': 'non-letter character percentage ...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   question  \\\n",
       "0   SemanticSimilarity descriptor explained   \n",
       "1     using is_critical parameter in alerts   \n",
       "2                Quickstart Evidently setup   \n",
       "3     Drift detection for numerical columns   \n",
       "4  non-letter character percentage function   \n",
       "\n",
       "                                              answer  \\\n",
       "0  # Understanding SemanticSimilarity Descriptor ...   \n",
       "1  # Using `is_critical` Parameter in Evidently A...   \n",
       "2  # Quickstart Setup for Evidently\\n\\n## Creatin...   \n",
       "3  # Drift Detection for Numerical Columns in Evi...   \n",
       "4  # Non-letter Character Percentage Function in ...   \n",
       "\n",
       "                                            messages  num_tool_calls  \\\n",
       "0  [{'kind': 'user-prompt', 'content': 'SemanticS...               5   \n",
       "1  [{'kind': 'user-prompt', 'content': 'using is_...               4   \n",
       "2  [{'kind': 'user-prompt', 'content': 'Quickstar...               5   \n",
       "3  [{'kind': 'user-prompt', 'content': 'Drift det...               5   \n",
       "4  [{'kind': 'user-prompt', 'content': 'non-lette...               5   \n",
       "\n",
       "                                   original_question  \\\n",
       "0  {'question': 'SemanticSimilarity descriptor ex...   \n",
       "1  {'question': 'using is_critical parameter in a...   \n",
       "2  {'question': 'Quickstart Evidently setup', 'su...   \n",
       "3  {'question': 'Drift detection for numerical co...   \n",
       "4  {'question': 'non-letter character percentage ...   \n",
       "\n",
       "                                     original_result  \n",
       "0  AgentRunResult(output=SearchResultArticle(foun...  \n",
       "1  AgentRunResult(output=SearchResultArticle(foun...  \n",
       "2  AgentRunResult(output=SearchResultArticle(foun...  \n",
       "3  AgentRunResult(output=SearchResultArticle(foun...  \n",
       "4  AgentRunResult(output=SearchResultArticle(foun...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_run = pd.DataFrame(rows)\n",
    "\n",
    "df_run.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "829e0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d925d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval-run-v2-2025-10-29-20-15.bin', 'wb') as f_out:\n",
    "    pickle.dump(rows, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86de610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2408\n",
      "-rw-r--r--@ 1 yenchunchen  staff   386K Oct 28 21:48 all-results-v1.bin\n",
      "-rw-r--r--@ 1 yenchunchen  staff   433K Oct 29 20:15 eval-run-v2-2025-10-29-20-15.bin\n",
      "-rw-r--r--  1 yenchunchen  staff    26K Oct 29 08:12 evaluation-agent-run.ipynb\n",
      "-rw-r--r--  1 yenchunchen  staff   226K Oct 29 08:12 ground-truth.ipynb\n",
      "-rw-r--r--@ 1 yenchunchen  staff   104K Oct 24 18:43 ground_truth_evidently.csv\n",
      "-rw-r--r--  1 yenchunchen  staff    13K Oct 29 08:12 retrieval-evaluation.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f119ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ac726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
