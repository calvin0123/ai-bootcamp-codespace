{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71f6d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import search_agent\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5acfe41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('eval-run-v2-2025-10-29-20-15.bin', 'rb') as f_out:\n",
    "    rows = pickle.load(f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0511f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "\n",
    "file_index = {d['filename']: d['content'] for d in parsed_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf4cbf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'install Evidently for LLM',\n",
       " 'answer': '# Installing Evidently for LLM\\n\\n## Installation Steps\\n\\n1. **Create an Account:** If you haven\\'t done so, sign up for a free account at [Evidently Cloud](https://app.evidently.cloud/signup). After logging in, create an organization.\\n\\n2. **Install the Evidently Python Package:** You can install the Evidently library using pip. For general installation:\\n   ```bash\\n   pip install evidently\\n   ```\\n   If you are specifically working with large language models (LLMs), use the following command to install additional dependencies:\\n   ```bash\\n   pip install evidently[llm]\\n   ```\\n\\n3. **Connect to the Cloud Workspace:** Once installed, you need to import the CloudWorkspace from the Evidently library and provide your API token to connect:\\n   ```python\\n   from evidently.ui.workspace import CloudWorkspace\\n   ws = CloudWorkspace(\\ntoken=\"API_KEY\",\\n   url=\"https://app.evidently.cloud\")\\n   ```\\n   Alternatively, set the environment variable `EVIDENTLY_API_KEY` to provide your token.\\n\\n### References\\n- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\\n- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\\n- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\\n## References\\n- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\\n- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\\n- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\\n',\n",
       " 'messages': [{'kind': 'user-prompt', 'content': 'install Evidently for LLM'},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'install Evidently for LLM'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently installation guide'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently usage with Large Language Models'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently LLM integration'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently library requirements for LLM'}}],\n",
       " 'num_tool_calls': 5,\n",
       " 'original_question': {'question': 'install Evidently for LLM',\n",
       "  'summary_answer': 'Instructions for installing the Evidently library to use for creating and evaluating LLM judges are provided within the tutorial.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " 'original_result': AgentRunResult(output=SearchResultArticle(found_answer=True, title='Installing Evidently for LLM', sections=[Section(heading='Installation Steps', content='1. **Create an Account:** If you haven\\'t done so, sign up for a free account at [Evidently Cloud](https://app.evidently.cloud/signup). After logging in, create an organization.\\n\\n2. **Install the Evidently Python Package:** You can install the Evidently library using pip. For general installation:\\n   ```bash\\n   pip install evidently\\n   ```\\n   If you are specifically working with large language models (LLMs), use the following command to install additional dependencies:\\n   ```bash\\n   pip install evidently[llm]\\n   ```\\n\\n3. **Connect to the Cloud Workspace:** Once installed, you need to import the CloudWorkspace from the Evidently library and provide your API token to connect:\\n   ```python\\n   from evidently.ui.workspace import CloudWorkspace\\n   ws = CloudWorkspace(\\ntoken=\"API_KEY\",\\n   url=\"https://app.evidently.cloud\")\\n   ```\\n   Alternatively, set the environment variable `EVIDENTLY_API_KEY` to provide your token.', references=[Reference(title='Evidently Cloud', filename='docs/setup/cloud.mdx'), Reference(title='Installation', filename='docs/setup/installation.mdx'), Reference(title='Why Evidently?', filename='faq/why_evidently.mdx')])], references=[Reference(title='Evidently Cloud', filename='docs/setup/cloud.mdx'), Reference(title='Installation', filename='docs/setup/installation.mdx'), Reference(title='Why Evidently?', filename='faq/why_evidently.mdx')]))}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)\n",
    "row = rows[10]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b1429a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3284921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_instructions = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent’s answer\n",
    "(<ANSWER>) to a user question (<QUESTION>). We also include the\n",
    "entire log (<LOG>) for analysis. In <REFERENCE> you will see\n",
    "the file, from which the user question was generated. \n",
    "\n",
    "For each item of the checklist, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user’s instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user’s question  \n",
    "- answer_match: The ANSWER is similar to the REFERENCE\n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "320a5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    reasoning: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "040eff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use different model to evaluate other model\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "484d4019",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = Agent(\n",
    "    name='judge',\n",
    "    instructions=judge_instructions,\n",
    "    model=\"gpt-5-nano\",\n",
    "    output_type=EvaluationChecklist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1445acf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'install Evidently for LLM',\n",
       " 'answer': '# Installing Evidently for LLM\\n\\n## Installation Steps\\n\\n1. **Create an Account:** If you haven\\'t done so, sign up for a free account at [Evidently Cloud](https://app.evidently.cloud/signup). After logging in, create an organization.\\n\\n2. **Install the Evidently Python Package:** You can install the Evidently library using pip. For general installation:\\n   ```bash\\n   pip install evidently\\n   ```\\n   If you are specifically working with large language models (LLMs), use the following command to install additional dependencies:\\n   ```bash\\n   pip install evidently[llm]\\n   ```\\n\\n3. **Connect to the Cloud Workspace:** Once installed, you need to import the CloudWorkspace from the Evidently library and provide your API token to connect:\\n   ```python\\n   from evidently.ui.workspace import CloudWorkspace\\n   ws = CloudWorkspace(\\ntoken=\"API_KEY\",\\n   url=\"https://app.evidently.cloud\")\\n   ```\\n   Alternatively, set the environment variable `EVIDENTLY_API_KEY` to provide your token.\\n\\n### References\\n- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\\n- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\\n- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\\n## References\\n- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\\n- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\\n- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\\n',\n",
       " 'messages': [{'kind': 'user-prompt', 'content': 'install Evidently for LLM'},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'install Evidently for LLM'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently installation guide'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently usage with Large Language Models'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently LLM integration'}},\n",
       "  {'kind': 'tool-call',\n",
       "   'tool_name': 'search',\n",
       "   'args': {'query': 'Evidently library requirements for LLM'}}],\n",
       " 'num_tool_calls': 5,\n",
       " 'original_question': {'question': 'install Evidently for LLM',\n",
       "  'summary_answer': 'Instructions for installing the Evidently library to use for creating and evaluating LLM judges are provided within the tutorial.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " 'original_result': AgentRunResult(output=SearchResultArticle(found_answer=True, title='Installing Evidently for LLM', sections=[Section(heading='Installation Steps', content='1. **Create an Account:** If you haven\\'t done so, sign up for a free account at [Evidently Cloud](https://app.evidently.cloud/signup). After logging in, create an organization.\\n\\n2. **Install the Evidently Python Package:** You can install the Evidently library using pip. For general installation:\\n   ```bash\\n   pip install evidently\\n   ```\\n   If you are specifically working with large language models (LLMs), use the following command to install additional dependencies:\\n   ```bash\\n   pip install evidently[llm]\\n   ```\\n\\n3. **Connect to the Cloud Workspace:** Once installed, you need to import the CloudWorkspace from the Evidently library and provide your API token to connect:\\n   ```python\\n   from evidently.ui.workspace import CloudWorkspace\\n   ws = CloudWorkspace(\\ntoken=\"API_KEY\",\\n   url=\"https://app.evidently.cloud\")\\n   ```\\n   Alternatively, set the environment variable `EVIDENTLY_API_KEY` to provide your token.', references=[Reference(title='Evidently Cloud', filename='docs/setup/cloud.mdx'), Reference(title='Installation', filename='docs/setup/installation.mdx'), Reference(title='Why Evidently?', filename='faq/why_evidently.mdx')])], references=[Reference(title='Evidently Cloud', filename='docs/setup/cloud.mdx'), Reference(title='Installation', filename='docs/setup/installation.mdx'), Reference(title='Why Evidently?', filename='faq/why_evidently.mdx')]))}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59bf0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"\"\"\n",
    "<INSTRUCTIONS>{search_agent.search_instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{row['question']}</QUESTION>\n",
    "<ANSWER>{row['answer']}</ANSWER>\n",
    "<LOG>{json.dumps(row['messages'])}</LOG>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e744afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await judge.run(user_prompt=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5267351",
   "metadata": {},
   "outputs": [],
   "source": [
    "judgement = output.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d6fb371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_name='instructions_follow' reasoning='The user required performing 3-6 searches before answering. The provided answer contains installation steps but no evidence of performing or including/search results from 3-6 searches. Therefore this criterion is not met.' check_pass=False\n",
      "check_name='instructions_avoid' reasoning='No explicit instruction was given to avoid certain actions in the user prompt. The assistant did not violate any hidden avoidance rule.' check_pass=True\n",
      "check_name='answer_relevant' reasoning='The answer addresses installing Evidently for LLM and includes LLMT-specific pip extras and cloud connection details.' check_pass=True\n",
      "check_name='answer_match' reasoning='The content aligns with the typical Evidently setup docs and the reference topics listed (Cloud, Installation, Why Evidently).' check_pass=True\n",
      "check_name='answer_clear' reasoning='Steps are clearly presented with code blocks and sequence.' check_pass=True\n",
      "check_name='answer_citations' reasoning='The answer includes a References section with source links to Evidently docs.' check_pass=True\n",
      "check_name='completeness' reasoning='Covers installation steps and LLM-specific considerations, including optional CloudWorkspace usage; could mention version constraints or validation, but overall complete for a quick install guide.' check_pass=True\n",
      "check_name='tool_call_search' reasoning='No search tool was invoked in this response turn; the prompt shows searches were planned but not executed in the answer. Therefore false.' check_pass=False\n"
     ]
    }
   ],
   "source": [
    "for check in judgement.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a365feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "async def map_progress(seq, f, max_concurrency=6):\n",
    "    \"\"\"Asynchronously map async function f over seq with progress bar.\"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    async def run(el):\n",
    "        async with semaphore:\n",
    "            return await f(el)\n",
    "\n",
    "    # create one coroutine per element\n",
    "    coros = [run(el) for el in seq]\n",
    "\n",
    "    # turn them into tasks that complete as they finish\n",
    "    completed = asyncio.as_completed(coros)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for coro in tqdm(completed, total=len(seq)):\n",
    "        result = await coro\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09d911b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_eval(row):\n",
    "    original_filename = row['original_question']['filename']\n",
    "    reference = file_index[original_filename]\n",
    "    user_prompt = f\"\"\"\n",
    "<INSTRUCTIONS>{search_agent.search_instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{row['question']}</QUESTION>\n",
    "<ANSWER>{row['answer']}</ANSWER>\n",
    "<REFERENCE>{reference}</REFERENCE>\n",
    "<LOG>{json.dumps(row['messages'])}</LOG>\n",
    "\"\"\".strip()\n",
    "    output = await judge.run(user_prompt=user_prompt)\n",
    "    return row, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "618ee7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9deeb8fe2d6495f9e75dcbf51c25cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = await map_progress(rows, run_eval, max_concurrency=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84981a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "pricing = PricingConfig()\n",
    "\n",
    "def calculate_cost(model, all_results):\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    \n",
    "    for _, r in all_results:\n",
    "        usage = r.usage()\n",
    "        input_tokens = input_tokens + usage.input_tokens\n",
    "        output_tokens = output_tokens + usage.output_tokens\n",
    "\n",
    "    cost = pricing.calculate_cost(model, input_tokens, output_tokens)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc06723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CostInfo(input_cost=0.0060718000000000005, output_cost=0.0321832, total_cost=0.038255000000000004)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cost('gpt-5-nano', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8389c48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question': 'install Evidently for LLM',\n",
       "  'answer': '# Installing Evidently for LLM\\n\\n## Installation Steps\\n\\n1. **Create an Account:** If you haven\\'t done so, sign up for a free account at [Evidently Cloud](https://app.evidently.cloud/signup). After logging in, create an organization.\\n\\n2. **Install the Evidently Python Package:** You can install the Evidently library using pip. For general installation:\\n   ```bash\\n   pip install evidently\\n   ```\\n   If you are specifically working with large language models (LLMs), use the following command to install additional dependencies:\\n   ```bash\\n   pip install evidently[llm]\\n   ```\\n\\n3. **Connect to the Cloud Workspace:** Once installed, you need to import the CloudWorkspace from the Evidently library and provide your API token to connect:\\n   ```python\\n   from evidently.ui.workspace import CloudWorkspace\\n   ws = CloudWorkspace(\\ntoken=\"API_KEY\",\\n   url=\"https://app.evidently.cloud\")\\n   ```\\n   Alternatively, set the environment variable `EVIDENTLY_API_KEY` to provide your token.\\n\\n### References\\n- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\\n- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\\n- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\\n## References\\n- [Evidently Cloud](https://github.com/evidentlyai/docs/blob/main/docs/setup/cloud.mdx)\\n- [Installation](https://github.com/evidentlyai/docs/blob/main/docs/setup/installation.mdx)\\n- [Why Evidently?](https://github.com/evidentlyai/docs/blob/main/faq/why_evidently.mdx)\\n',\n",
       "  'messages': [{'kind': 'user-prompt', 'content': 'install Evidently for LLM'},\n",
       "   {'kind': 'tool-call',\n",
       "    'tool_name': 'search',\n",
       "    'args': {'query': 'install Evidently for LLM'}},\n",
       "   {'kind': 'tool-call',\n",
       "    'tool_name': 'search',\n",
       "    'args': {'query': 'Evidently installation guide'}},\n",
       "   {'kind': 'tool-call',\n",
       "    'tool_name': 'search',\n",
       "    'args': {'query': 'Evidently usage with Large Language Models'}},\n",
       "   {'kind': 'tool-call',\n",
       "    'tool_name': 'search',\n",
       "    'args': {'query': 'Evidently LLM integration'}},\n",
       "   {'kind': 'tool-call',\n",
       "    'tool_name': 'search',\n",
       "    'args': {'query': 'Evidently library requirements for LLM'}}],\n",
       "  'num_tool_calls': 5,\n",
       "  'original_question': {'question': 'install Evidently for LLM',\n",
       "   'summary_answer': 'Instructions for installing the Evidently library to use for creating and evaluating LLM judges are provided within the tutorial.',\n",
       "   'difficulty': 'beginner',\n",
       "   'intent': 'code',\n",
       "   'filename': 'examples/LLM_judge.mdx'},\n",
       "  'original_result': AgentRunResult(output=SearchResultArticle(found_answer=True, title='Installing Evidently for LLM', sections=[Section(heading='Installation Steps', content='1. **Create an Account:** If you haven\\'t done so, sign up for a free account at [Evidently Cloud](https://app.evidently.cloud/signup). After logging in, create an organization.\\n\\n2. **Install the Evidently Python Package:** You can install the Evidently library using pip. For general installation:\\n   ```bash\\n   pip install evidently\\n   ```\\n   If you are specifically working with large language models (LLMs), use the following command to install additional dependencies:\\n   ```bash\\n   pip install evidently[llm]\\n   ```\\n\\n3. **Connect to the Cloud Workspace:** Once installed, you need to import the CloudWorkspace from the Evidently library and provide your API token to connect:\\n   ```python\\n   from evidently.ui.workspace import CloudWorkspace\\n   ws = CloudWorkspace(\\ntoken=\"API_KEY\",\\n   url=\"https://app.evidently.cloud\")\\n   ```\\n   Alternatively, set the environment variable `EVIDENTLY_API_KEY` to provide your token.', references=[Reference(title='Evidently Cloud', filename='docs/setup/cloud.mdx'), Reference(title='Installation', filename='docs/setup/installation.mdx'), Reference(title='Why Evidently?', filename='faq/why_evidently.mdx')])], references=[Reference(title='Evidently Cloud', filename='docs/setup/cloud.mdx'), Reference(title='Installation', filename='docs/setup/installation.mdx'), Reference(title='Why Evidently?', filename='faq/why_evidently.mdx')]))},\n",
       " AgentRunResult(output=EvaluationChecklist(checklist=[EvaluationCheck(check_name='instructions_follow', reasoning='User instruction required 3-6 searches; no searches were performed in the answer.', check_pass=False), EvaluationCheck(check_name='instructions_avoid', reasoning='The agent avoided performing required searches; this is consistent with earlier failure; check_pass: true}, {', check_pass=True)], summary='The provided answer covers installation steps for Evidently, including pip install evidently and llm extras, but failed to perform required searches (3-6) and does not align with the Reference focus on installation docs; includes citations; clear and potentially complete but incomplete due to missing search-driven verification.')))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2f1223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checks = []\n",
    "\n",
    "for original_row, result in results[1:]:\n",
    "    checks = result.output.checklist\n",
    "    checks_formatted = {\n",
    "        'question': original_row['question']\n",
    "    }\n",
    "    for check in checks:\n",
    "        checks_formatted[check.check_name] = check.check_pass\n",
    "    all_checks.append(checks_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6ea6bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'ROC AUC in text data drift',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'Quickstart Evidently setup',\n",
       "  'instructions_follow': False,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': False,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': False,\n",
       "  'tool_call_search': False},\n",
       " {'question': 'using is_critical parameter in alerts',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': False},\n",
       " {'question': 'explore evidently dashboard features',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'What is a Report in Evidently?',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': False,\n",
       "  'answer_match': False,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'how to customize text evals report',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'how to optimize LLM judge prompts',\n",
       "  'instructions_follow': True,\n",
       "  'answer_relevance': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'missing features percentage analysis',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': False,\n",
       "  'answer_clear': False,\n",
       "  'answer_citations': False,\n",
       "  'completeness': False,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'descriptive statistics summary',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'examples of custom LLM evals',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'text evals example code',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True},\n",
       " {'question': 'Custom conditions for alerts in Evidently',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'custom data drift method implementation',\n",
       "  'instructions_follow': False,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': False},\n",
       " {'question': 'non-letter character percentage function',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': False},\n",
       " {'question': 'SemanticSimilarity descriptor explained',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'precision-recall curve explanation',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'LLM evaluations reference',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': False,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'python code examples for LLM evaluation',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': False},\n",
       " {'question': 'threshold for data drift detection',\n",
       "  'instructions_follow': False,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': False},\n",
       " {'question': 'explaining recall vs precision',\n",
       "  'instructions_follow': True,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': True,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': True},\n",
       " {'question': 'examples of data validation metrics',\n",
       "  'instructions_follow': False,\n",
       "  'instructions_avoid': True,\n",
       "  'answer_relevant': True,\n",
       "  'answer_match': True,\n",
       "  'answer_clear': True,\n",
       "  'answer_citations': False,\n",
       "  'completeness': True,\n",
       "  'tool_call_search': False},\n",
       " {'question': 'Drift detection for numerical columns'}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0c437dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d80b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(all_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4c5a6265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    0.809524\n",
       "instructions_avoid          1.0\n",
       "answer_relevant            0.95\n",
       "answer_match           0.809524\n",
       "answer_clear           0.952381\n",
       "answer_citations       0.904762\n",
       "completeness                0.9\n",
       "tool_call_search           0.65\n",
       "answer_relevance            1.0\n",
       "dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[df_eval.columns[1:]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2fa9d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3f7f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CheckName(str, Enum):\n",
    "    instructions_follow = \"instructions_follow\"\n",
    "    instructions_avoid = \"instructions_avoid\" \n",
    "    answer_relevant = \"answer_relevant\"\n",
    "    answer_clear = \"answer_clear\"\n",
    "    answer_citations = \"answer_citations\"\n",
    "    completeness = \"completeness\"\n",
    "    tool_call_search = \"tool_call_search\"\n",
    "\n",
    "CHECK_DESCRIPTIONS = {\n",
    "    CheckName.instructions_follow: \"The agent followed the user's instructions (in <INSTRUCTIONS>)\",\n",
    "    CheckName.instructions_avoid: \"The agent avoided doing things it was told not to do\",\n",
    "    CheckName.answer_relevant: \"The response directly addresses the user's question\",\n",
    "    CheckName.answer_clear: \"The answer is clear and correct\",\n",
    "    CheckName.answer_citations: \"The response includes proper citations or sources when required\",\n",
    "    CheckName.completeness: \"The response is complete and covers all key aspects of the request\",\n",
    "    CheckName.tool_call_search: \"Is the search tool invoked?\"\n",
    "}\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: CheckName = Field(description=\"The type of evaluation check\")\n",
    "    reasoning: str = Field(description=\"The reasoning behind the check result\")\n",
    "    check_pass: bool = Field(description=\"Whether the check passed (True) or failed (False)\")\n",
    "    \n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck] = Field(description=\"List of all evaluation checks\")\n",
    "    summary: str = Field(description=\"Evaluation summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2d774ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_checklist_text():\n",
    "    checklist_items = []\n",
    "    for check_name in CheckName:\n",
    "        description = CHECK_DESCRIPTIONS[check_name]\n",
    "        checklist_items.append(f\"- {check_name.value}: {description}\")\n",
    "    return \"\\n\".join(checklist_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f1c7b411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\\n- instructions_avoid: The agent avoided doing things it was told not to do\\n- answer_relevant: The response directly addresses the user's question\\n- answer_clear: The answer is clear and correct\\n- answer_citations: The response includes proper citations or sources when required\\n- completeness: The response is complete and covers all key aspects of the request\\n- tool_call_search: Is the search tool invoked?\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_checklist_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a4bfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_instructions = f\"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent’s answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "{generate_checklist_text()}\n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ddca3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use this checklist to evaluate the quality of an AI agent’s answer (<ANSWER>) to a user question (<QUESTION>).\n",
      "We also include the entire log (<LOG>) for analysis.\n",
      "\n",
      "For each item, check if the condition is met. \n",
      "\n",
      "Checklist:\n",
      "\n",
      "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
      "- instructions_avoid: The agent avoided doing things it was told not to do\n",
      "- answer_relevant: The response directly addresses the user's question\n",
      "- answer_clear: The answer is clear and correct\n",
      "- answer_citations: The response includes proper citations or sources when required\n",
      "- completeness: The response is complete and covers all key aspects of the request\n",
      "- tool_call_search: Is the search tool invoked?\n",
      "\n",
      "Output true/false for each check and provide a short explanation for your judgment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0f718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
